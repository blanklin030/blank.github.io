<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BlankLin</title>
  
  <subtitle>lazy and boring</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-12-10T16:43:31.920Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Blank Lin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深入浅出starrocks离线导入</title>
    <link href="http://yoursite.com/2023/12/11/starrocks-load/"/>
    <id>http://yoursite.com/2023/12/11/starrocks-load/</id>
    <published>2023-12-11T11:04:11.000Z</published>
    <updated>2023-12-10T16:43:31.920Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><ul><li>什么是StarRocks<br>StarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容MySQL协议和MySQL生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。StarRocks致力于在全场景OLAP业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</li><li>滴滴olap现状<br>当前在滴滴内部,主要大数据olap生态包括clickhouse/druid/starrocks等,而写入olap可以划分为两个方向,分布是实时和离线:<ul><li>2.1 实时方向,包括网约车/金融等大部分业务侧均是使用kafka/ddmq作为source端,将数据通过flink实时攒批写入到大数据olap存储侧</li><li>2.2 离线方向,包括网约车/金融等大部分业务侧则是将数据通过hive/mysql/hdfs这个source端,经过各项加工后流入到olap这个sink存储侧</li></ul></li><li>本文主题<br>本篇文章主要是介绍在starrocks这一侧,如果实现将hive数据进行加工后写入到starrock,涉及到starrokc的源码解析、组件介绍、及相关平台架构</li></ul><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>如下图,是当前滴滴内部hive2sr导入的实现架构图,用户主要是通过访问同步中心,配置hive表和sr表的字段映射及默认值,同步中心会将映射关系通过srm开放的http接口传给srm侧,srm进行相关处理后提交给sr的各个组件,组件之间通过thrift server进行rpc通信,而srm则通过http方式进行任务最终状态监听,当任务完成后返回给数梦的任务调度系统,最终完成整个全链路流程.<br><img src="/images/starrocks/hive2sr/structure.png" alt="架构图"></p><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ol><li>创建目标分区<br>同步中心会将hive2sr当前批次需要导入hive的分区列表传入srm,srm会校验当前sr表对应分区是否存在,不存在则按照hive分区值对应创建出sr表分区  </li><li>创建目标分区<br>上面这一步创建出目标分区后,srm会对应目标分区range范围,在sr目标表中创建出对应的临时分区,临时分区的作用是这样的,srm可以在一张已经定义分区规则的分区表上，创建临时分区，并为这些临时分区设定单独的数据分布策略。将hive数据写入指定的临时分区后,通过原子覆盖写操作(调整分区分桶策略)，srm可以将临时分区作为临时可用的数据载体覆盖到对应的目标分区,用以实现覆盖写操作</li><li>创建load任务<br>3.1 这个load首先需要配置涉及sr访问hadoop环境的相关参数,这个操作相当于在sr中配置hdfs-site.xml和core-site.xml<br>3.2 其实load任务需要组装出提交给spark的driver任务内容,通过cluster模式,以master on yarn的方式在fe节点提交出spark任务</li><li>fe进程调度etl阶段<br>在etl阶段,fe会开启spark driver等待yarn队列资源充足时,提交给yarn取执行spark任务,Spark集群执行ETL完成对导入数据的预处理。包括全局字典构建（BITMAP类型）、分区、排序、聚合等。预处理后的数据按parquet数据格式落盘HDFS存储侧。</li><li>etl完成后fe会调度load阶段<br>ETL 任务完成后，FE获取预处理过的每个分片的hdfs数据路径，并调度相关的 broker 执行 load 任务</li><li>broker加载hdfs文件后通知be进行push<br>BE 通过 Broker 进程读取 HDFS 数据，转化为 StarRocks 存储格式。此时临时分区完成对应hive分区数据的加载过程</li><li>将临时分区替换到目标分区<br>临时分区的数据将完成的替换到目标分区去,而目标分区数据将被删除,查询后将是新的hive分区数据</li></ol><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><h4 id="fe进程启动及相关调度处理"><a href="#fe进程启动及相关调度处理" class="headerlink" title="fe进程启动及相关调度处理"></a>fe进程启动及相关调度处理</h4><ul><li><p>入口文件-&gt;StarRocksFE.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">feServer.start();</span><br><span class="line">httpServer.start();</span><br><span class="line">qeService.start();</span><br></pre></td></tr></table></figure><p>主要是初始化配置和启动服务，分别是mysql server端口、thrift server端口、http端口</p></li><li><p>mysq服务启动-&gt;QeService.java<br>由于我们都是通过tcp协议来连sr,所以主要关注QeService</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void start() throws IOException &#123;</span><br><span class="line">        if (!mysqlServer.start()) &#123;</span><br><span class="line">            LOG.error(&quot;mysql server start failed&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(&quot;QE service start.&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>MysqlServer.java<br>这里主要是开启mysql协议的服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public boolean start() &#123;</span><br><span class="line">    ......</span><br><span class="line">    &#x2F;&#x2F; 打开fe的mysql协议的socket管道</span><br><span class="line">    &#x2F;&#x2F; 开启一个常驻线程用以监听mysql协议</span><br><span class="line">    listener &#x3D; ThreadPoolManager.newDaemonCacheThreadPool(1, &quot;MySQL-Protocol-Listener&quot;, true);</span><br><span class="line">    running &#x3D; true;</span><br><span class="line">    listenerFuture &#x3D; listener.submit(new Listener());</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>提交本次连接的上下文到连接调度器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">........</span><br><span class="line">clientChannel &#x3D; serverChannel.accept();</span><br><span class="line">if (clientChannel &#x3D;&#x3D; null) &#123;</span><br><span class="line">    continue;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 初始化本次session的上下文信息到连接调度器</span><br><span class="line">&#x2F;&#x2F; submit this context to scheduler</span><br><span class="line">ConnectContext context &#x3D; new ConnectContext(clientChannel, sslContext);</span><br><span class="line">&#x2F;&#x2F; Set globalStateMgr here.</span><br><span class="line">context.setGlobalStateMgr(GlobalStateMgr.getCurrentState());</span><br><span class="line">if (!scheduler.submit(context)) &#123;</span><br><span class="line">    LOG.warn(&quot;Submit one connect request failed. Client&#x3D;&quot; + clientChannel.toString());</span><br><span class="line">    &#x2F;&#x2F; clear up context</span><br><span class="line">    context.cleanup();</span><br><span class="line">&#125;</span><br><span class="line">............</span><br><span class="line">&#96;&#96;&#96;   </span><br><span class="line"></span><br><span class="line">+ 提交本次连接的上下文给线程池</span><br></pre></td></tr></table></figure><p>public boolean submit(ConnectContext context) {<br>  if (context == null) {</p><pre><code>  return false;</code></pre><p>  }</p><p>  context.setConnectionId(nextConnectionId.getAndAdd(1));<br>  // no necessary for nio.<br>  if (context instanceof NConnectContext) {</p><pre><code>  return true;</code></pre><p>  }<br>  // 这里是将Runnable提交到connect-scheduler-pool线程池<br>  if (executor.submit(new LoopHandler(context)) == null) {</p><pre><code>  LOG.warn(&quot;Submit one thread failed.&quot;);  return false;</code></pre><p>  }<br>  return true;<br>}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoopHandler.java (实现Runnable接口)</span><br></pre></td></tr></table></figure><p>public void run() {<br>  …..<br>  // 注册本次连接,sr会计算当前fe节点总的连接数,在每次连接超过1024后进行sleep连接的驱逐流程<br>  if (registerConnection(context)) {</p><pre><code>  MysqlProto.sendResponsePacket(context);</code></pre><p>  } else {</p><pre><code>  context.getState().setError(&quot;Reach limit of connections&quot;);  MysqlProto.sendResponsePacket(context);  return;</code></pre><p>  }<br>  ………<br>  // 常驻,进行核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan<br>  ConnectProcessor processor = new ConnectProcessor(context);<br>  processor.loop();<br>  ……….<br>}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; loop</span><br></pre></td></tr></table></figure><p>public void loop() {<br>  while (!ctx.isKilled()) {</p><pre><code>  try {      processOnce();  } catch (Exception e) {      // TODO(zhaochun): something wrong      LOG.warn(&quot;Exception happened in one seesion(&quot; + ctx + &quot;).&quot;, e);      ctx.setKilled();      break;  }</code></pre><p>  }<br>}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; processOnce</span><br></pre></td></tr></table></figure><p>// handle one process<br>public void processOnce() throws IOException {<br>  // 重置上下文的状态<br>  ctx.getState().reset();<br>  executor = null;</p><p>  // 重置mysql协议的顺序标识符<br>  final MysqlChannel channel = ctx.getMysqlChannel();<br>  channel.setSequenceId(0);<br>  // 从通道里获取数据包<br>  try {</p><pre><code>  packetBuf = channel.fetchOnePacket();  if (packetBuf == null) {      throw new IOException(&quot;Error happened when receiving packet.&quot;);  }</code></pre><p>  } catch (AsynchronousCloseException e) {</p><pre><code>  // when this happened, timeout checker close this channel  // killed flag in ctx has been already set, just return  return;</code></pre><p>  }</p><p>  // 调度,这里主要是上面介绍核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan过程<br>  dispatch();<br>  ………….<br>}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; dispatch</span><br></pre></td></tr></table></figure><p>…….<br>// 这里主要是实现mysql协议的几种状态<br>switch (command) {<br>  case COM_INIT_DB:</p><pre><code>  handleInitDb();  break;</code></pre><p>  case COM_QUIT:</p><pre><code>  handleQuit();  break;</code></pre><p>  case COM_QUERY:<br>  // 这里是完整的sql处理的总入口</p><pre><code>  handleQuery();  ctx.setStartTime();  break;</code></pre><p>  case COM_FIELD_LIST:</p><pre><code>  handleFieldList();  break;</code></pre><p>  case COM_CHANGE_USER:</p><pre><code>  handleChangeUser();  break;</code></pre><p>  case COM_RESET_CONNECTION:</p><pre><code>  handleResetConnnection();  break;</code></pre><p>  case COM_PING:</p><pre><code>  handlePing();  break;</code></pre><p>  default:</p><pre><code>  ctx.getState().setError(&quot;Unsupported command(&quot; + command + &quot;)&quot;);  LOG.warn(&quot;Unsupported command(&quot; + command + &quot;)&quot;);  break;</code></pre><p>}<br>……</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; handleQuery</span><br></pre></td></tr></table></figure><p>….<br>StatementBase parsedStmt = null;<br>try {<br>  ctx.setQueryId(UUIDUtil.genUUID());<br>  List<StatementBase> stmts;<br>  try {</p><pre><code>  //通过antlr4进行sql的解析,获取sql解析ast树列表  stmts = com.starrocks.sql.parser.SqlParser.parse(originStmt, ctx.getSessionVariable());</code></pre><p>  } catch (ParsingException parsingException) {</p><pre><code>  throw new AnalysisException(parsingException.getMessage());</code></pre><p>  }<br>  // 对ast语法树进行analyze分析过程<br>  for (int i = 0; i &lt; stmts.size(); ++i) {</p><pre><code>  ..........  // Only add the last running stmt for multi statement,  // because the audit log will only show the last stmt.  if (i == stmts.size() - 1) {      addRunningQueryDetail(parsedStmt);  }  executor = new StmtExecutor(ctx, parsedStmt);  ctx.setExecutor(executor);  ctx.setIsLastStmt(i == stmts.size() - 1);  executor.execute();  // 如果sql有一条执行失败,后续不再执行  if (ctx.getState().getStateType() == QueryState.MysqlStateType.ERR) {      break;  }  ........</code></pre><p>  }<br>}<br>….</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#### 对sql进行Parser语法解析</span><br><span class="line">+ SqlParser.java -&gt; parser</span><br></pre></td></tr></table></figure><p>// 首先，我们需要初始化 StarRocksLexer，即词法解析器。在这里，StarRocksLexer 是根据上文介绍的StarRocksLex.g4 词法文件，使用 Antlr4 自动生成的代码类。<br>StarRocksLexer lexer = new StarRocksLexer(new CaseInsensitiveStream(CharStreams.fromString(sql)));<br>lexer.setSqlMode(sessionVariable.getSqlMode());<br>// 然后，代码将词法解析器 StarRocksLexer 作为参数，传入语法解析器中。语法解析器类StarRocksParser，同样是根据上文介绍的 StarRocks.g4 语法文件自动生成的代码类。<br>CommonTokenStream tokenStream = new CommonTokenStream(lexer);<br>StarRocksParser parser = new StarRocksParser(tokenStream);<br>// 到这里，我们就完成了语法解析类的构建。之后再调用 parser.addErrorListener(new ErrorHandler())，将 Antlr4 的默认错误处理规则，替换为自定义的错误处理逻辑即可。<br>parser.removeErrorListeners();<br>parser.addErrorListener(new ErrorHandler());<br>parser.removeParseListeners();<br>parser.addParseListener(new TokenNumberListener(sessionVariable.getParseTokensLimit(),<br>        Math.max(Config.expr_children_limit, sessionVariable.getExprChildrenLimit())));<br>……<br>List<StatementBase> statements = Lists.newArrayList();<br>// 调用 parser.sqlStatements() 返回值 StarRocksParser.SqlStatementsContext，这是一套 antlr 自定义的抽象语法树，根据语法文件生成。<br>List<StarRocksParser.SingleStatementContext> singleStatementContexts = parser.sqlStatements().singleStatement();<br>for (int idx = 0; idx &lt; singleStatementContexts.size(); ++idx) {<br>    // 将 antlr 的语法树转换为 StarRocks 的抽象语法树<br>    StatementBase statement = (StatementBase) new AstBuilder(sessionVariable.getSqlMode())<br>            .visitSingleStatement(singleStatementContexts.get(idx));<br>    statement.setOrigStmt(new OriginStatement(sql, idx));<br>    statements.add(statement);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StarRocks.g4 -&gt; loadStatement</span><br><span class="line">因为本文主要介绍的导入流程,所以以LoadStatement来举例</span><br></pre></td></tr></table></figure><br>loadStatement<br>    : LOAD LABEL label=labelName<br>        data=dataDescList?<br>        broker=brokerDesc?<br>        (BY system=identifierOrString)?<br>        (PROPERTIES props=propertyList)?<br>    | LOAD LABEL label=labelName<br>        data=dataDescList?<br>        resource=resourceDesc<br>        (PROPERTIES props=propertyList)?<br>    ;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">antlr 的语法文件采用 BNF 范式，用&#39;|&#39;表示分支选项，&#39;?&#39;表达0次或一次，其他符号可类比正则表达式。</span><br><span class="line">+ AstBuilder.java -&gt; visitLoadStatement</span><br><span class="line">Antlr4 会根据语法文件生成一份 Visitor 模式的代码，这样就可以做到动作代码与文法产生式解耦，利于文法产生式的重用。而自定义的 AstBuilder 文件则继承了 StarRocksBaseVisitor，用于将 antlr 内部的 AST 翻译成 StarRocks 自定义的 AST。</span><br></pre></td></tr></table></figure><br>public ParseNode visitLoadStatement(StarRocksParser.LoadStatementContext context) {<br>    // 解析出load里写的label名称<br>    LabelName label = getLabelName(context.labelName());<br>    // 解析出load里的字段映射默认值配置等描述<br>    List<DataDescription> dataDescriptions = null;<br>    if (context.data != null) {<br>        dataDescriptions = context.data.dataDesc().stream().map(this::getDataDescription)<br>                .collect(toList());<br>    }<br>    // 解析出load里关于spark任务配置的property属性<br>    Map<String, String> properties = null;<br>    if (context.props != null) {<br>        properties = Maps.newHashMap();<br>        List<Property> propertyList = visit(context.props.property(), Property.class);<br>        for (Property property : propertyList) {<br>            properties.put(property.getKey(), property.getValue());<br>        }<br>    }<br>    // 解析出load里引用的spark外部资源名称<br>    if (context.resource != null) {<br>        ResourceDesc resourceDesc = getResourceDesc(context.resource);<br>        return new LoadStmt(label, dataDescriptions, resourceDesc, properties);<br>    }<br>    // 解析出broker的配置<br>    BrokerDesc brokerDesc = getBrokerDesc(context.broker);<br>    String cluster = null;<br>    if (context.system != null) {<br>        cluster = ((Identifier) visit(context.system)).getValue();<br>    }<br>    //  visit 的返回值返回一个 AST 的基类，在StarRocks 中称为 ParseNode<br>    return new LoadStmt(label, dataDescriptions, brokerDesc, cluster, properties);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们对 Parser 源码进行了重点分析，包括 ANTLR4、SqlParser 和 ASTBuilder。与此同时，我们还通过一个例子，介绍了如何将一条文本类型的 SQL 语句，一步步解析成 StarRocks 内部使用的 AST。</span><br><span class="line">可以看到，Parser 能判断出用户的 SQL 中是否存在明显的语法错误，如 SQL 语句select * from;会在Parser 阶段报错。但如果 SQL 语句select * from foo;没有语法错误，StarRocks 中也没有 foo 这张表，那么 StarRocks 该如何做到错误处理呢？这就需要依赖下一节的 Analyzer 模块去判断了。</span><br><span class="line"></span><br><span class="line">#### 对sql进行analyzer语法解析</span><br><span class="line">+ StmtExecutor.java-&gt; executor</span><br></pre></td></tr></table></figure><br>……..<br>// 本文主要介绍load流程,所以这里值关注ddl的词法分析过程<br>} else if (parsedStmt instanceof DdlStmt) {<br>    handleDdlStmt();<br>}<br>……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br> if (parsedStmt instanceof ShowStmt) {<br>    com.starrocks.sql.analyzer.Analyzer.analyze(parsedStmt, context);<br>    PrivilegeChecker.check(parsedStmt, context);</p><pre><code>QueryStatement selectStmt = ((ShowStmt) parsedStmt).toSelectStmt();if (selectStmt != null) {    parsedStmt = selectStmt;    execPlan = StatementPlanner.plan(parsedStmt, context);}</code></pre><p> } else {<br>    // 这里是analyze解析的入口函数<br>        execPlan = StatementPlanner.plan(parsedStmt, context);<br> }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StatementPlanner.java -&gt; plan</span><br></pre></td></tr></table></figure><br>// 注意在analyze阶段,也是会进行锁库操作(db.readLock();)<br>lock(dbs);<br>try (PlannerProfile.ScopedTimer ignored = PlannerProfile.getScopedTimer(“Analyzer”)) {<br>    Analyzer.analyze(stmt, session);<br>}</p><p>PrivilegeChecker.check(stmt, session);<br>if (stmt instanceof QueryStatement) {<br>    OptimizerTraceUtil.logQueryStatement(session, “after analyze:\n%s”, (QueryStatement) stmt);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Analyzer 类是所有语句的语义解析的主入口，采用了 Visitor 设计模式，会根据处理的语句类型的不同，调用不同语句的 Analyzer。不同语句的处理逻辑会包含在单独的语句 Analyzer 中，交由不同的 Analyzer 处理</span><br><span class="line"></span><br><span class="line">+ LoadExecutor.java -&gt; accept</span><br></pre></td></tr></table></figure><br>public <R, C> R accept(AstVisitor<R, C> visitor, C context) {<br>    return visitor.visitLoadStatement(this, context);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;real_analyze.png)</span><br><span class="line"></span><br><span class="line">+ LoadStmtAnalyzer.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public Void visitLoadStatement(LoadStmt statement, ConnectContext context) {<br>    analyzeLabel(statement, context);<br>    analyzeDataDescriptions(statement);<br>    analyzeProperties(statement);<br>    return null;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如上所示,LoadStatement交由LoadStmtAnalyzer来专门处理解析,这样做的优点是，可以将 Statement 的定义文件和 Analyze 的处理逻辑分开，并且同一类型的语句也交由特定的 Analyzer 处理，做到不同功能之间代码的解耦合。</span><br><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br>  ……..<br>// 这里是ddl 执行分析的总入口<br>ShowResultSet resultSet = DDLStmtExecutor.execute(parsedStmt, context);<br>  ……..<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; execute</span><br></pre></td></tr></table></figure><br>……..<br>// ddl statement analyze 处理逻辑的基类<br>return stmt.accept(StmtExecutorVisitor.getInstance(), context);<br>……..<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;analyze.png)</span><br><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public ShowResultSet visitLoadStatement(LoadStmt stmt, ConnectContext context) {<br>    ErrorReport.wrapWithRuntimeException(() -&gt; {<br>        EtlJobType jobType = stmt.getEtlJobType();<br>        if (jobType == EtlJobType.UNKNOWN) {<br>            throw new DdlException(“Unknown load job type”);<br>        }<br>        if (jobType == EtlJobType.HADOOP &amp;&amp; Config.disable_hadoop_load) {<br>            throw new DdlException(“Load job by hadoop cluster is disabled.”</p><pre><code>                + &quot; Try using broker load. See &#39;help broker load;&#39;&quot;);    }    // 这里进行真正的创建load任务的分析过程    context.getGlobalStateMgr().getLoadManager().createLoadJobFromStmt(stmt, context);});return null;</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### 创建load处理pending阶段</span><br><span class="line"></span><br><span class="line">+ LoadManager.java -&gt; createLoadJobFromStmt</span><br></pre></td></tr></table></figure><br>public void createLoadJobFromStmt(LoadStmt stmt) throws DdlException {<br>    // 校验库元数据<br>  Database database = checkDb(stmt.getLabel().getDbName());<br>  long dbId = database.getId();<br>  LoadJob loadJob = null;<br>  // 加写锁,防止并发<br>  writeLock();<br>  try {<br>    // 校验lable是否已使用<br>      checkLabelUsed(dbId, stmt.getLabel().getLabelName());<br>      if (stmt.getBrokerDesc() == null &amp;&amp; stmt.getResourceDesc() == null) {<br>          throw new DdlException(“LoadManager only support the broker and spark load.”);<br>      }<br>      // 判断queue队列长度是否超过1024,超过就报错<br>      if (loadJobScheduler.isQueueFull()) {<br>          throw new DdlException(<br>                  “There are more than “ + Config.desired_max_waiting_jobs + “ load jobs in waiting queue, “</p><pre><code>                      + &quot;please retry later.&quot;);  }  // 按load类型初始化出LoadJob  loadJob = BulkLoadJob.fromLoadStmt(stmt);  // 在内存中记录该load元数据  createLoadJob(loadJob);</code></pre><p>  } finally {<br>      writeUnlock();<br>  }<br>  // 通知bdbje加入load元数据同步<br>  Catalog.getCurrentCatalog().getEditLog().logCreateLoadJob(loadJob);</p><p>  // loadJob守护进程从load job schedule queue取出任务去执行<br>  loadJobScheduler.submitJob(loadJob);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过 &#96;createLoadJobFromStmt&#96; 创建load任务</span><br><span class="line">+ LoadJobScheduler.java -&gt; process</span><br><span class="line">注意：LoadJobScheduler 继承自 MasterDaemon，MasterDaemon 继承自 Daemon，</span><br><span class="line">Daemon继承自Thread，重载了run方法，里面有一个loop，主要执行runOneCycle</span><br><span class="line">MasterDaemon 又重写了 runOneCycle，执行 runAfterCatalogReady 函数</span><br><span class="line">LoadJobScheduler 又重写了 runAfterCatalogReady 主要就是干process处理，里面是一个死循环，不断从LinkedBlockingQueue类型的needScheduleJobs里出栈取要执行的job</span><br></pre></td></tr></table></figure><br>while (true) {<br>  // take one load job from queue<br>  LoadJob loadJob = needScheduleJobs.poll();<br>  if (loadJob == null) {<br>      return;<br>  }</p><p>  // schedule job<br>  try {<br>      loadJob.execute();<br>  }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadJob.java -&gt; execute</span><br></pre></td></tr></table></figure><br>public void execute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>        DuplicatedRequestException, LoadException {<br>            // 加写锁,这里防止多并发提交<br>    writeLock();<br>    try {<br>        unprotectedExecute();<br>    } finally {<br>        writeUnlock();<br>    }<br>}<br>// 这里主要关注beginTxn,做了事务处理,目的是防止fe节点突然挂掉再拉起后其他follower节点切换到leader节点时会重新执行这个任务,而这个时候旧leader节点已经执行过一段这个任务,导致fe节点之间执行无法同步,所以直接抛出label已经存在不再继续执行<br>public void unprotectedExecute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>            DuplicatedRequestException, LoadException {<br>    // check if job state is pending<br>    if (state != JobState.PENDING) {<br>        return;<br>    }<br>    // the limit of job will be restrict when begin txn<br>    beginTxn();<br>    unprotectedExecuteJob();<br>    // update spark load job state from PENDING to ETL when pending task is finished<br>    if (jobType != EtlJobType.SPARK) {<br>        unprotectedUpdateState(JobState.LOADING);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadJob.java -&gt; unprotectedExecuteJob</span><br></pre></td></tr></table></figure><br>protected void unprotectedExecuteJob() throws LoadException {<br>    // create pending task<br>    LoadTask task = new SparkLoadPendingTask(this, fileGroupAggInfo.getAggKeyToFileGroups(),<br>            sparkResource, brokerDesc);<br>    task.init();<br>    idToTasks.put(task.getSignature(), task);<br>    // 注意这里的线程池,初始化时只有5个核心线程(最大线程数),而队列长度只有1024,这里会影响任务长期处于pending状态,如果当前5个任务在下面的spark driver阶段一直等不到yarn资源,则一直处于这个线程执行状态,其他提交进来的任务执行在queue队列中等待<br>    submitTask(Catalog.getCurrentCatalog().getPendingLoadTaskScheduler(), task);<br>    ……….<br>    // this.pendingLoadTaskScheduler = new LeaderTaskExecutor(“pending_load_task_scheduler”, Config.max_broker_load_job_concurrency,Config.desired_max_waiting_jobs, !isCheckpointCatalog);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; init</span><br><span class="line">&#x2F;&#x2F; 初始化任务的配置参数</span><br></pre></td></tr></table></figure><br>public void init() throws LoadException {<br>    createEtlJobConf();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadTask -&gt; exec</span><br><span class="line">&#x2F;&#x2F; 由于SparkLoadTask继承自LoadTask,而LoadTask实现了PriorityRunnable接口,所以上面的submitTask实际上就是自动执行这里的exec方法</span><br></pre></td></tr></table></figure><br>@Override<br>protected void exec() {<br>    boolean isFinished = false;<br>    try {<br>        // execute pending task<br>        executeTask();<br>        // callback on pending task finished<br>        callback.onTaskFinished(attachment);<br>        isFinished = true;<br>    ………<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### load任务处理etl阶段</span><br><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; executeTask</span><br></pre></td></tr></table></figure><br>void executeTask() throws LoadException {<br>    LOG.info(“begin to execute spark pending task. load job id: {}”, loadJobId);<br>    submitEtlJob();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>private void submitEtlJob() throws LoadException {<br>    SparkPendingTaskAttachment sparkAttachment = (SparkPendingTaskAttachment) attachment;<br>    // 配置etl阶段清洗出的文件在hdfs上的存放路径<br>    etlJobConfig.outputPath = EtlJobConfig.getOutputPath(resource.getWorkingDir(), dbId, loadLabel, signature);<br>    sparkAttachment.setOutputPath(etlJobConfig.outputPath);</p><pre><code>// 提交etl任务SparkEtlJobHandler handler = new SparkEtlJobHandler();handler.submitEtlJob(loadJobId, loadLabel, etlJobConfig, resource, brokerDesc, sparkLoadAppHandle,        sparkAttachment);LOG.info(&quot;submit spark etl job success. load job id: {}, attachment: {}&quot;, loadJobId, sparkAttachment);</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkEtlJobHandler.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,<br>                             BrokerDesc brokerDesc, SparkLoadAppHandle handle, SparkPendingTaskAttachment attachment)<br>      throws LoadException {<br>  // delete outputPath<br>  // init local dir<br>  // prepare dpp archive<br>  SparkLauncher launcher = new SparkLauncher(envs);<br>  // master      |  deployMode<br>  // ——————|——————-<br>  // yarn        |  cluster<br>  // spark://xx  |  client<br>  launcher.setMaster(resource.getMaster())<br>          .setDeployMode(resource.getDeployMode().name().toLowerCase())<br>          .setAppResource(appResourceHdfsPath)<br>          .setMainClass(SparkEtlJob.class.getCanonicalName())<br>          .setAppName(String.format(ETL_JOB_NAME, loadLabel))<br>          .setSparkHome(sparkHome)<br>          .addAppArgs(jobConfigHdfsPath)<br>          .redirectError();</p><p>  // spark configs</p><p>  // start app<br>  State state = null;<br>  String appId = null;<br>  String logPath = null;<br>  String errMsg = “start spark app failed. error: “;<br>  try {<br>     // 提交spark任务给yarn<br>      Process process = launcher.launch();<br>      handle.setProcess(process);<br>      if (!FeConstants.runningUnitTest) {<br>        // 监听spark driver输出的日志,直到任务被yarn接收后,退出driver进程<br>          SparkLauncherMonitor.LogMonitor logMonitor = SparkLauncherMonitor.createLogMonitor(handle);<br>          logMonitor.setSubmitTimeoutMs(GET_APPID_TIMEOUT_MS);<br>          logMonitor.setRedirectLogPath(logFilePath);<br>          logMonitor.start();<br>          try {<br>              logMonitor.join();<br>          } catch (InterruptedException e) {<br>              logMonitor.interrupt();<br>              throw new LoadException(errMsg + e.getMessage());<br>          }<br>      }<br>      appId = handle.getAppId();<br>      state = handle.getState();<br>      logPath = handle.getLogPath();<br>  } catch (IOException e) {<br>      LOG.warn(errMsg, e);<br>      throw new LoadException(errMsg + e.getMessage());<br>  }<br>……….<br>  // success<br>  attachment.setAppId(appId);<br>  attachment.setHandle(handle);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### load任务处理loading阶段</span><br><span class="line">+ 1、spark load 有一个 LoadEtlChecker定时调度任务,每5s执行一次</span><br></pre></td></tr></table></figure><br>protected void runAfterCatalogReady() {<br>    try {<br>        loadManager.processEtlStateJobs();<br>    } catch (Throwable e) {<br>        LOG.warn(“Failed to process one round of LoadEtlChecker with error message {}”, e.getMessage(), e);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 2、这个定时调度就是定时检查load任务状态,有变化就去更新,简单说就是load状态扭转控制器</span><br></pre></td></tr></table></figure><br>// only for those jobs which have etl state, like SparkLoadJob<br>public void processEtlStateJobs() {<br>    idToLoadJob.values().stream().filter(job -&gt; (job.jobType == EtlJobType.SPARK &amp;&amp; job.state == JobState.ETL))<br>            .forEach(job -&gt; {<br>                try {<br>                    ((SparkLoadJob) job).updateEtlStatus();<br>                } catch (DataQualityException e) {<br>                    LOG.info(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(FailMsg.CancelType.ETL_QUALITY_UNSATISFIED,<br>                                    DataQualityException.QUALITY_FAIL_MSG),<br>                            true, true);<br>                } catch (TimeoutException e) {<br>                    // timeout, retry next time<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                } catch (UserException e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(CancelType.ETL_RUN_FAIL, e.getMessage()), true, true);<br>                } catch (Exception e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                }<br>            });<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 3、当任务状态走到loading时,会提交load任务,函数是submitPushTasks(),这里简单说就是把表的排序健&#x2F;分区&#x2F;be副本等查询出来,后面要把这些元数据通过rpc调用be来处理</span><br></pre></td></tr></table></figure><br>if (tablet instanceof LocalTablet) {<br>    for (Replica replica : ((LocalTablet) tablet).getImmutableReplicas()) {<br>        long replicaId = replica.getId();<br>        tabletAllReplicas.add(replicaId);<br>        long backendId = replica.getBackendId();<br>        Backend backend = GlobalStateMgr.getCurrentState().getCurrentSystemInfo()<br>                .getBackend(backendId);</p><pre><code>    pushTask(backendId, tableId, partitionId, indexId, tabletId,            replicaId, schemaHash, params, batchTask, tabletMetaStr,            backend, replica, tabletFinishedReplicas, TTabletType.TABLET_TYPE_DISK);}if (tabletAllReplicas.size() == 0) {    LOG.error(&quot;invalid situation. tablet is empty. id: {}&quot;, tabletId);}// check tablet push statesif (tabletFinishedReplicas.size() &gt;= quorumReplicaNum) {    quorumTablets.add(tabletId);    if (tabletFinishedReplicas.size() == tabletAllReplicas.size()) {        fullTablets.add(tabletId);    }}</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、遍历生成出的每个tablet,再去遍历每个副本replica,都会给每个replica提交一个load任务,那这里要注意的是会构造</span><br></pre></td></tr></table></figure><br>PushTask pushTask = new PushTask(backendId, dbId, tableId, partitionId,<br>        indexId, tabletId, replicaId, schemaHash,<br>        0, id, TPushType.LOAD_V2,<br>        TPriority.NORMAL, transactionId, taskSignature,<br>        tBrokerScanRange, params.tDescriptorTable,<br>        params.useVectorized, timezone, tabletType);<br>if (AgentTaskQueue.addTask(pushTask)) {<br>    batchTask.addTask(pushTask);<br>    if (!tabletToSentReplicaPushTask.containsKey(tabletId)) {<br>        tabletToSentReplicaPushTask.put(tabletId, Maps.newHashMap());<br>    }<br>    tabletToSentReplicaPushTask.get(tabletId).put(replicaId, pushTask);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、会通过thrift rpc 通知这个副本的be,主要是这个地方client.submit_tasks(agentTaskRequests);</span><br></pre></td></tr></table></figure><br> // create AgentClient<br>address = new TNetworkAddress(backend.getHost(), backend.getBePort());<br>client = ClientPool.backendPool.borrowObject(address);<br>List<TAgentTaskRequest> agentTaskRequests = new LinkedList<TAgentTaskRequest>();<br>for (AgentTask task : tasks) {<br>    agentTaskRequests.add(toAgentTaskRequest(task));<br>}<br>client.submit_tasks(agentTaskRequests);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、这个submit_tasks方法主要是调用BackendService类,这个类实现在be这一侧</span><br></pre></td></tr></table></figure><br>public com.starrocks.thrift.TAgentResult submit_tasks(java.util.List<com.starrocks.thrift.TAgentTaskRequest> tasks) throws org.apache.thrift.TException<br>{<br>    send_submit_tasks(tasks);<br>    return recv_submit_tasks();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个submit_tasks方法是 Thrift 生成的 Java 客户端代码。它和 C++ 服务端的对应关系是:</span><br><span class="line">1) fe发送 Thrift RPC 请求,调用这个 Java submit_tasks方法。</span><br><span class="line">2) Thrift框架会将参数 tasks 序列化成数据包,发送给服务端be。</span><br><span class="line">3) be服务端 Thrift 接收到数据包,反序列化出 tasks 参数。</span><br><span class="line">4) 根据方法名submit_tasks映射到 C++ 的 BackendService::submit_tasks 方法,将 tasks 作为参数调用它。</span><br><span class="line">5) C++ 方法执行完成,将返回值通过 Thrift 序列化后发送给客户端。</span><br><span class="line">6) Java 端 recv_submit_tasks 反序列化出返回结果,赋值给 submit_tasks 方法的返回值。</span><br><span class="line">7) submit_tasks 返回结果,Java 前端获得调用结果。</span><br><span class="line"></span><br><span class="line">#### be部分如何处理loading的代码逻辑</span><br><span class="line">+ 1、BackendService的rpc服务在启动be时已开启</span><br></pre></td></tr></table></figure><br>// Begin to start services<br>// 1. Start thrift server with ‘be_port’.<br>auto thrift_server = BackendService::create<BackendService>(exec_env, starrocks::config::be_port);<br>if (auto status = thrift_server-&gt;start(); !status.ok()) {<br>    LOG(ERROR) &lt;&lt; “Fail to start BackendService thrift server on port “ &lt;&lt; starrocks::config::be_port &lt;&lt; “: “<br>                &lt;&lt; status;<br>    starrocks::shutdown_logging();<br>    exit(1);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 2、客户端BackendServiceClient::submit_tasks()会序列化tasks参数,然后发送RPC请求到服务端,服务端的RPC框架收到请求,根据方法名映射到BackendService::submit_tasks()</span><br></pre></td></tr></table></figure><br>void BackendService::submit_tasks(TAgentResult&amp; return_value, const std::vector<TAgentTaskRequest>&amp; tasks) {<br>    _agent_server-&gt;submit_tasks(return_value, tasks);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 3、而agent_server里处理submit_tasks的核心逻辑是再次调用PushTaskWorkerPool下的submit_tasks</span><br></pre></td></tr></table></figure><br>// batch submit push tasks<br>if (!push_divider.empty()) {<br>    LOG(INFO) &lt;&lt; “begin batch submit task: “ &lt;&lt; tasks[0].task_type;<br>    for (const auto&amp; push_item : push_divider) {<br>        const auto&amp; push_type = push_item.first;<br>        auto all_push_tasks = push_item.second;<br>        switch (push_type) {<br>        case TPushType::LOAD_V2:<br>            _push_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        case TPushType::DELETE:<br>        case TPushType::CANCEL_DELETE:<br>            _delete_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        default:<br>            ret_st = Status::InvalidArgument(strings::Substitute(“tasks(type=$0, push_type=$1) has wrong task type”,<br>                                                                    TTaskType::PUSH, push_type));<br>            LOG(WARNING) &lt;&lt; “fail to batch submit push task. reason: “ &lt;&lt; ret_st.get_error_msg();<br>        }<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、在TaskWorkPool里处理submit_tasks的,主要是实现批量注册task信息,做去重和幂等校验,成功后进行遍历,核心函数是_convert_task</span><br></pre></td></tr></table></figure><br>for (size_t i = 0; i &lt; task_count; i++) {<br>    if (failed_task[i] == 0) {<br>        auto new_task = _convert_task(*tasks[i], recv_time);<br>        _tasks.emplace_back(std::move(new_task));<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、_convert_task它会将TAgentTaskRequest转换成PushReqAgentTaskRequest类型的任务请求,这个是通用线程池,构造函数里回调了_worker_thread_callback,构造出的EngineBatchLoadTask对象后取执行task</span><br></pre></td></tr></table></figure><br>EngineBatchLoadTask engine_task(push_req, &amp;tablet_infos, agent_task_req-&gt;signature, &amp;status,<br>                                ExecEnv::GetInstance()-&gt;load_mem_tracker());<br>StorageEngine::instance()-&gt;execute_task(&amp;engine_task);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、在EngineBatchLoadTask::execute()里追踪了内存使用后,会检查tablet是否存在,目录是否达到存储限制等,继续执行load_v2(sparkload&#x2F;brokerload)的_process,而_process也是做了下校验后提交到_push里</span><br></pre></td></tr></table></figure><br>if (status == STARROCKS_SUCCESS) {<br>    uint32_t retry_time = 0;<br>    while (retry_time &lt; PUSH_MAX_RETRY) {<br>        status = _process();</p><pre><code>    if (status == STARROCKS_PUSH_HAD_LOADED) {        LOG(WARNING) &lt;&lt; &quot;transaction exists when realtime push, &quot;                        &quot;but unfinished, do not report to fe, signature: &quot;                        &lt;&lt; _signature;        break; // not retry anymore    }    // Internal error, need retry    if (status == STARROCKS_ERROR) {        LOG(WARNING) &lt;&lt; &quot;push internal error, need retry.signature: &quot; &lt;&lt; _signature;        retry_time += 1;    } else {        break;    }}</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 7、在_push()里初始化了PushHandler类,现在才开始处理数据的摄入过程</span><br></pre></td></tr></table></figure><br>vectorized::PushHandler push_handler;<br>res = push_handler.process_streaming_ingestion(tablet, request, type, tablet_info_vec);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 8、在核心逻辑_do_streaming_ingestion函数里,开始检查这个tablet是否能迁移,能的话,拿回tablet的写锁后做load转换过程</span><br></pre></td></tr></table></figure><br>Status st = Status::OK();<br>if (push_type == PUSH_NORMAL_V2) {<br>    st = _load_convert(tablet_vars-&gt;at(0).tablet, &amp;(tablet_vars-&gt;at(0).rowset_to_add));<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 9、而在_load_convert里就是初始化出RowsetWriterContext这个上下文对象和RowsetWriter,RowsetWriter对象主要是需要去读出tablet的每一行,用这个wirter写回</span><br></pre></td></tr></table></figure><br>st = reader-&gt;init(t_scan_range, _request);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “fail to init reader. res=” &lt;&lt; st.to_string() &lt;&lt; “, tablet=” &lt;&lt; cur_tablet-&gt;full_name();<br>    return st;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 10、而reader的初始化就是构造PushBrokerReader对象,在PushBrokerReader里会按照文件类型构造出扫描对象,们目前通过sparkload处理的数据,都是使用parquet格式,所以这个初始化就是初始化出ParquetScanner</span><br></pre></td></tr></table></figure><br>// init scanner<br>FileScanner* scanner = nullptr;<br>switch (t_scan_range.ranges[0].format_type) {<br>case TFileFormatType::FORMAT_PARQUET: {<br>    scanner = new ParquetScanner(_runtime_state.get(), _runtime_profile, t_scan_range, _counter.get());<br>    if (scanner == nullptr) {<br>        return Status::InternalError(“Failed to create scanner”);<br>    }<br>    break;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 11、再回到PushHandler里的_load_conver函数的读取tablet过程,也是按行来去读一个个chunk,而这个next_chunk在PushBrokerReader里又是如何处理的呢?</span><br></pre></td></tr></table></figure><br>ChunkPtr chunk = ChunkHelper::new_chunk(schema, 0);<br>while (!reader-&gt;eof()) {<br>    st = reader-&gt;next_chunk(&amp;chunk);<br>    if (!st.ok()) {<br>        LOG(WARNING) &lt;&lt; “fail to read next chunk. err=” &lt;&lt; st.to_string() &lt;&lt; “, read_rows=” &lt;&lt; num_rows;<br>        return st;<br>    } else {<br>        if (reader-&gt;eof()) {<br>            break;<br>        }</p><pre><code>    st = rowset_writer-&gt;add_chunk(*chunk);    if (!st.ok()) {        LOG(WARNING) &lt;&lt; &quot;fail to add chunk to rowset writer&quot;                        &lt;&lt; &quot;. res=&quot; &lt;&lt; st &lt;&lt; &quot;, tablet=&quot; &lt;&lt; cur_tablet-&gt;full_name()                        &lt;&lt; &quot;, read_rows=&quot; &lt;&lt; num_rows;        return st;    }    num_rows += chunk-&gt;num_rows();    chunk-&gt;reset();}</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 12、next_chunk函数会通过scanner对象,去读每行,直到读完为止,再把读出来的结果转换成chunk</span><br></pre></td></tr></table></figure><br>auto res = _scanner-&gt;get_next();<br>if (res.status().is_end_of_file()) {<br>    _eof = true;<br>    return Status::OK();<br>} else if (!res.ok()) {<br>    return res.status();<br>}</p><p>return _convert_chunk(res.value(), chunk);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 13、在parquet_scanner里是如何读每个行的呢?当然是按批来读,把读出的批追加到chunk去,chunk满了就返回,这种方式就是通用的流式数据处理,避免了一次新把整个parquet读到内存里,在这里主要关注到next_batch函数,就是Parquet文件的读取流程</span><br></pre></td></tr></table></figure><br>const TBrokerRangeDesc&amp; range_desc = _scan_range.ranges[_next_file];<br>Status st = create_random_access_file(range_desc, _scan_range.broker_addresses[0], _scan_range.params,<br>                                        CompressionTypePB::NO_COMPRESSION, &amp;file);</p><p>14、而在create_random_access_file函数由于parquet_scanner是继承自file_scanner,统一回来按文件类型去创建RandomAccessFile的逻辑<br>int64_t timeout_ms = _state-&gt;query_options().query_timeout * 1000 / 4;<br>timeout_ms = std::max(timeout_ms, static_cast<int64_t>(DEFAULT_TIMEOUT_MS));<br>BrokerFileSystem fs_broker(address, params.properties, timeout_ms);<br>if (config::use_local_filecache_for_broker_random_access_file) {<br>    ASSIGN_OR_RETURN(auto broker_file, fs_broker.new_sequential_file(range_desc.path));</p><pre><code>std::string dest_path = create_tmp_file_path();LOG(INFO) &lt;&lt; &quot;broker load cache file: &quot; &lt;&lt; dest_path;ASSIGN_OR_RETURN(auto dest_file, FileSystem::Default()-&gt;new_writable_file(dest_path));auto res = fs::copy(broker_file.get(), dest_file.get(), 10 * 1024 * 1024);std::shared_ptr&lt;RandomAccessFile&gt; local_file;ASSIGN_OR_RETURN(local_file, FileSystem::Default()-&gt;new_random_access_file(dest_path));src_file = std::make_shared&lt;TempRandomAccessFile&gt;(dest_path, local_file);</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 15、使用BrokerFileSystem通过broker来打开文件(这里的address就是我们在fe节点里创建tBrokerScanRange传入的),用broker把hdfs上的文件下载到本地临时目录下,再打开本地临时目录</span><br></pre></td></tr></table></figure><br>Status st = call_method(_broker_addr, &amp;BrokerServiceClient::openReader, request, &amp;response);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; st;<br>    return st;<br>}<br>if (response.opStatus.statusCode != TBrokerOperationStatusCode::OK) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; response.opStatus.message;<br>    return to_status(response.opStatus);<br>}</p><p>// Get file size.<br>ASSIGN_OR_RETURN(const uint64_t file_size, get_file_size(path));<br>auto stream = std::make_shared<BrokerInputStream>(_broker_addr, response.fd, file_size);<br>return std::make_unique<SequentialFile>(std::move(stream), path);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">注意,这里又是通过rpc来调用broker了</span><br><span class="line"></span><br><span class="line">+ 16、再回来看parquet_scanner里的get_next()函数,就是整个parquet文件的核心逻辑,而核心里的调用就是初始化chunk的逻辑</span><br></pre></td></tr></table></figure><br>Status ParquetScanner::initialize_src_chunk(ChunkPtr<em> chunk) {<br>    SCOPED_RAW_TIMER(&amp;_counter-&gt;init_chunk_ns);<br>    _pool.clear();<br>    (</em>chunk) = std::make_shared<Chunk>();<br>    size_t column_pos = 0;<br>    _chunk_filter.clear();<br>    for (auto i = 0; i &lt; _num_of_columns_from_file; ++i) {<br>        SlotDescriptor<em> slot_desc = _src_slot_descriptors[i];<br>        if (slot_desc == nullptr) {<br>            continue;<br>        }<br>        auto</em> array = _batch-&gt;column(column_pos++).get();<br>        ColumnPtr column;<br>        RETURN_IF_ERROR(new_column(array-&gt;type().get(), slot_desc, &amp;column, &amp;_conv_funcs[i], &amp;_cast_exprs[i]));<br>        column-&gt;reserve(_max_chunk_size);<br>        (*chunk)-&gt;append_column(column, slot_desc-&gt;id());<br>    }<br>    return Status::OK();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1) 清空内存池</span><br><span class="line">2) 创建新的空chunk</span><br><span class="line">3) 遍历所有columns:</span><br><span class="line">  3.1、如果column在schema中,获取对应的SlotDescriptor</span><br><span class="line">  3.2、根据数组类型和SlotDescriptor创建新的Column</span><br><span class="line">  3.3、预留chunk大小内存</span><br><span class="line">  3.4、将Column添加到chunk</span><br><span class="line">4) 返回OK状态</span><br><span class="line">这样就构造了一个空的、符合schema的chunk。</span><br><span class="line">主要逻辑是:</span><br><span class="line">- 清理内存池重用</span><br><span class="line">- 创建空chunk</span><br><span class="line">- 为每个column创建对应的Column对象</span><br><span class="line">- 预留内存</span><br><span class="line">- 添加到chunk</span><br><span class="line">这是非常标准的按schema构建chunk的过程。</span><br><span class="line">这种实现可以高效构建chunk,同时预留内存减少后续内存分配,并可以重用内存池避免频繁new&#x2F;delete。</span><br><span class="line">构建一个符合schema并预分配内存的空chunk是vectorized执行的基础,这个初始化实现是非常重要的一步</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 演进之路</span><br><span class="line">&gt; 上面主要解读了整个离线导入过程的核心代码,在离线导入模块集成到同步中心后,也遇到了一些问题,进行了3次升级来支持不同业务侧对离线导入的需求,下面主要介绍下几个大feature的升级</span><br><span class="line"></span><br><span class="line">#### 国际化业务需要字符串分区</span><br><span class="line">滴滴内部的国际化业务,由于不同国家有时区问题,所以业务侧在hive建表时总会创建出dt&#x3D;yyyy-mm-dd&#x2F;country_code&#x3D;xx的表结构,但如果要将这种hive表结构导入到sr,在当前官方版本都是不支持的,一直到3.0+版本我们和社区一起共建了string类型分区这个feature,并且在离线导入这一块进行了深度迭代升级,用以支撑国际化业务的快速落地sr,这个feature我们已经和入到官方版本中,当前社区版本需要在3.0+才可以支持string类型分区,而滴滴内部在2.3和2.5都已经支持了string类型分区功能,如下表结构</span><br></pre></td></tr></table></figure><br>CREATE TABLE <code>db</code>.<code>tb</code> (<br>  <code>is_passenger</code> varchar(65533),<br>  <code>country_code</code> varchar(65533),<br>  <code>dt</code> varchar(65533),<br>  <code>pid</code> varchar(65533),<br>  <code>resource_stage_1_show_cnt</code> bigint(20)<br>) ENGINE=OLAP<br>DUPLICATE KEY(<code>is_passenger</code>, <code>country_code</code>, <code>dt</code>)<br>COMMENT “xxxx”<br>PARTITION BY LIST(<code>country_code</code>,<code>dt</code>)(<br>  PARTITION pMX_20231208 VALUES IN ((‘MX’, ‘2023-12-08’))<br>)<br>DISTRIBUTED BY HASH(<code>pid</code>) BUCKETS 48<br>PROPERTIES (<br>“replication_num” = “3”,<br>“in_memory” = “false”,<br>“storage_format” = “DEFAULT”,<br>“enable_persistent_index” = “false”<br>);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### spark任务提交由于yarn资源缺乏只能等待5分钟便终止等待load退出</span><br><span class="line">滴滴内部yarn资源队列core和memory均做了一定限制,在降本增效的前提下,很多业务侧无法再去扩容yarn队列,而sr官方社区版本在spark任务提交给yarn之后,默认只给等待5分钟,如果始终等不到yarn接收任务,则自动退出,针对这种特定场景,我们优化了load任务的执行流程,引入了&#96;&quot;spark_load_submit_timeout&quot; &#x3D; &quot;7200&quot;&#96;参数,在每个load任务提交后都可以自动化配置这个任务需要等待yarn多少时间,并且如果spark任务确实等待超时后无法提交,也加入了通知yarn去杀死这个等待的spark任务,防止后续等待到yarn资源后又再次提交执行,而sr这一侧load已经退出,白白浪费资源,如下load结构</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.lable_name_xxx (<br>  DATA INFILE(<br>    “hdfs://DClusterUS1/xxxxx”<br>  )<br>  INTO TABLE tb<br>  TEMPORARY PARTITION(temp<strong>partition)<br>  FORMAT AS “ORC”<br>  (<code>column_1</code>, <code>column_2</code>)<br>  SET (<br>    <code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br>    <code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>    <code>dt</code> = “2023-12-09”,<br>    <code>country_code</code> = “MX”<br>)<br>) WITH RESOURCE ‘external_spark_resource’ (<br>  “spark.yarn.tags” = “xxxxxxx”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 由于spark driver进程在fe节点中占用过多内存,会导致cgroup自动杀死fe进程</span><br><span class="line">目前我们的k8s集群对fe节点的配置只有12g,所以大批量spark driver进程在fe节点中,按照之前统计每个dirver进程大约会占用200-300mb,而fe进程的元数据占用内存大约在5g-10g之间,所以留给spark driverfe进程的可用内存其实非常少,但是目前大量业务侧使用离线导入功能,这样就会造成fe进程不定时被kill的困局,针对这个场景,我们开发了load限流功能,将并发提交的任务转入限流队列中,由队列资源来控制当前任务的并行度,在fe节点内存资源达到高峰期时,进行相应的限流策略,用以保障fe节点的稳定性,如下所示</span><br><span class="line">![架构图](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;structure.png)</span><br><span class="line"></span><br><span class="line">#### 由于hivesql写入的hive表产出的hdsf文件自动生成的格式是_col1..._coln,在通过字段导入时会造成hive表字段和sr表字段映射失败</span><br><span class="line">在离线导入功能演进过程中,这个场景在大量业务侧被迫切提出需求,业务侧由于业务口径的变更,需要对hive表进行修改,而一修改hive表结构,历史数据回溯时就会出现分区映射的hdfs文件无法正确映射成功,这个场景和通过hive sql导入hive表时一样的,hive和sr之间字段无法映射成功,就造成导入失败,针对这一场景我们升级了离线导入模块,引入了spark sql模式来读取hive表,而不是之前的spark 文件方式读取,这种方式成功解决了用户变更表字段回溯场景和hive sql导入场景,如下load结构:</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.label_name (<br>DATA FROM TABLE hive_tb<br>INTO TABLE sr_tb<br>TEMPORARY PARTITION(temp</strong>partition)<br>SET (<br><code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br><code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>…….<br>)<br>WHERE (<code>dt</code> = ‘2023-12-09’)<br>)WITH RESOURCE ‘spark_external_resource’ (<br>  “spark.yarn.tags” = “h2s_foit_150748320231209914ff11b87c94c85947ab13f84ff4622”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br>```<br>注意:目前spark sql导入只在明细模型(duplicate key)和聚合模型(aggregate key)上支持</p><h3 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h3><p>目前starrocks离线导入功能已经集成在滴滴的同步中心,用户可以通过同步中心按hive表结构自动创建sr表结构,并且配置出hive表字段和sr表字段的映射关系后,构造出导入配置相应参数,调用sr的离线导入模块进行数据导入,而这个导入功能我们在2024年也会有以下规划:</p><h4 id="spark-sql支持主键模型-primary-key"><a href="#spark-sql支持主键模型-primary-key" class="headerlink" title="spark sql支持主键模型(primary key)"></a>spark sql支持主键模型(primary key)</h4><p>目前有一部分业务侧会通过主键模型将hive分区数据定时导回给sr,当前还不支持spark sql方式导入,2024年我们会进行这个feature开发</p><h4 id="独立sr构建工具"><a href="#独立sr构建工具" class="headerlink" title="独立sr构建工具"></a>独立sr构建工具</h4><p>当前的etl构建过程仍然需要在fe进程中进行交互处理,但fe节点我们在k8s初始化时只有12g,虽然在演进之路里我们加入了限流功能来保障fe稳定性,但是这个方式也会将hive导入耗时加大,对于一些需要高优保障的任务仍然需要按时产出,所以我们规划在2024年可以将这个构建工具从sr的fe进程中独立出来,单独部署在构建集群中,在完成etl阶段后,通过rpc方式通知给sr的fe进程来拉取</p><h4 id="hive外表联邦查询"><a href="#hive外表联邦查询" class="headerlink" title="hive外表联邦查询"></a>hive外表联邦查询</h4><p>当前很多业务侧的hive表分区压缩后也会在100g+,这种case导入到sr后查询性能也不一定会很好,但是导入过程却很浪费资源,我们计划在2024年完成hive外表的集成功能,让用户可以直接通过sr来查询hive表,而不需要再进行分区导入这一部分操作</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;什么是StarRocks&lt;br&gt;StarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="starrocks" scheme="http://yoursite.com/tags/starrocks/"/>
    
  </entry>
  
  <entry>
    <title>常用的linux排查命令</title>
    <link href="http://yoursite.com/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/</id>
    <published>2023-01-04T15:04:11.000Z</published>
    <updated>2023-01-04T15:05:15.857Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何看查占用cpu最多的进程？"><a href="#如何看查占用cpu最多的进程？" class="headerlink" title="如何看查占用cpu最多的进程？"></a>如何看查占用cpu最多的进程？</h2><ul><li><p>方法一<br>核心指令：ps<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps H -eo pid,pcpu | sort -nk2 | tail</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps H -eo pid,pcpu | sort -nk2 | tail</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">30904  1.0</span><br><span class="line">30914  1.0</span><br></pre></td></tr></table></figure><p>结果：<br>瞧见了吧，最耗cpu的pid=30914。<br>画外音：实际上是31396。</p></li><li><p>方法二<br>核心指令：top<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">top</span><br><span class="line">Shift + t</span><br></pre></td></tr></table></figure><h2 id="找到了最耗CPU的进程ID，对应的服务名是什么呢？"><a href="#找到了最耗CPU的进程ID，对应的服务名是什么呢？" class="headerlink" title="找到了最耗CPU的进程ID，对应的服务名是什么呢？"></a>找到了最耗CPU的进程ID，对应的服务名是什么呢？</h2></li><li><p>方法一<br>核心指令：ps<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | fgrep pid</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps aux | fgrep 30914</span><br><span class="line">work 30914  1.0  0.8 309568 71668 ?  Sl   Feb02 124:44 .&#x2F;router2 –conf&#x3D;rs.conf</span><br></pre></td></tr></table></figure><p>结果：<br>瞧见了吧，进程是./router2</p></li><li><p>方法二<br>直接查proc即可。<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll &#x2F;proc&#x2F;pid</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ll &#x2F;proc&#x2F;30914</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 cwd -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 exe -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2&#x2F;router2</span><br></pre></td></tr></table></figure><p>画外音：这个好，全路径都出来了。</p></li></ul><h2 id="如何查看某个端口的连接情况？"><a href="#如何查看某个端口的连接情况？" class="headerlink" title="如何查看某个端口的连接情况？"></a>如何查看某个端口的连接情况？</h2><ul><li><p>方法一<br>核心指令：netstat<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lap | fgrep port</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ netstat -lap | fgrep 22022</span><br><span class="line">tcp        0      0 1.2.3.4:22022          *:*                         LISTEN      31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46642          ESTABLISHED 31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46640          ESTABLISHED 31396&#x2F;imui</span><br></pre></td></tr></table></figure></li><li><p>方法二<br>核心指令：lsof<br>实际命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i :port</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ &#x2F;usr&#x2F;sbin&#x2F;lsof -i :22022</span><br><span class="line">COMMAND   PID USER   FD   TYPE   DEVICE SIZE NODE NAME</span><br><span class="line">router  30904 work   50u  IPv4 69065770       TCP 1.2.3.4:46638-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   51u  IPv4 69065772       TCP 1.2.3.4:46639-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   52u  IPv4 69065774       TCP 1.2.3.4:46640-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br></pre></td></tr></table></figure><p>学废了吗？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;如何看查占用cpu最多的进程？&quot;&gt;&lt;a href=&quot;#如何看查占用cpu最多的进程？&quot; class=&quot;headerlink&quot; title=&quot;如何看查占用cpu最多的进程？&quot;&gt;&lt;/a&gt;如何看查占用cpu最多的进程？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;方法一&lt;br&gt;核心
      
    
    </summary>
    
    
      <category term="tool" scheme="http://yoursite.com/categories/tool/"/>
    
    
      <category term="sre" scheme="http://yoursite.com/tags/sre/"/>
    
  </entry>
  
  <entry>
    <title>2022年终总结</title>
    <link href="http://yoursite.com/2022/12/30/2022/"/>
    <id>http://yoursite.com/2022/12/30/2022/</id>
    <published>2022-12-30T08:04:11.000Z</published>
    <updated>2022-12-30T07:46:15.037Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>2022年仓皇而逃，在恍惚间我仍然有一丝怀疑，此时此刻是2019年的冬天，我们在准备着即将到来到2020年，订好了机票，订好了酒店，在跨年夜的云层之上，陌生人在沉睡，而我即将见到相隔数月思念日日夜夜的人儿。</p></li><li><p>这种怀疑在这3年间时不时的击碎我，重建我，沉溺我，到最后清醒我，时间过得太快了啊，快到迅雷烈风，快到掩耳而逝，我不是还在17/8岁的高中课堂上汗流浃背准备高考吗？怎么这一眨眼之间，我竟已是前额白发丝丝黑眼圈蜡黄脸的中年人了呢？</p></li><li><p>人世间的痛不知道何时能结束，人世间的离愁不知道何时能消散，虚度30余载光阴，到如今即将到来的第二个本命年，我有何成就吗？我有实现过理想吗？我活着的这些时刻，有给这个世界带来什么美好吗？我不自知只觉惭愧。</p></li><li><p>麻木的躯壳麻木的灵魂，肤浅的知识肤浅的见识，2022年在核酸、隔离、阳性中结束，这浅浅6个字结束了我的又一个365天，我是谁？我在2022年都做了什么事？有什么可以拿说来细说一二吗？有什么不足和遗憾吗？希望2023年可以改变吗？希望改变什么呢？</p></li><li><p>没有答案，我甚至连流水账都写不出来，哦，我这个可笑的中年人。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;2022年仓皇而逃，在恍惚间我仍然有一丝怀疑，此时此刻是2019年的冬天，我们在准备着即将到来到2020年，订好了机票，订好了酒店，在跨年夜的云层之上，陌生人在沉睡，而我即将见到相隔数月思念日日夜夜的人儿。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;这种怀疑在这3
      
    
    </summary>
    
    
      <category term="blank" scheme="http://yoursite.com/categories/blank/"/>
    
    
      <category term="流水记" scheme="http://yoursite.com/tags/%E6%B5%81%E6%B0%B4%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse集群cpu飙高问题排查</title>
    <link href="http://yoursite.com/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/</id>
    <published>2022-12-08T11:35:11.000Z</published>
    <updated>2022-12-15T14:00:44.169Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse是分布式系统，一条查询sql会经过pipeline处理后通过后台的查询线程池开启多线程查询，而经常会收到cpu飙高告警，则是因为这条sql开启了系统所有的cpu资源（多个线程）进行计算</p><h3 id="告警图"><a href="#告警图" class="headerlink" title="告警图"></a>告警图</h3><p>如下图所示，提示cpu飙高<br><img src="/images/clickhouse/cpu_high/1.png" alt="clickhouse"><br>如下图所示，ganglia上展示了当时的cpu顶着线在跑<br><img src="/images/clickhouse/cpu_high/2.png" alt="clickhouse"></p><h3 id="system-query-log"><a href="#system-query-log" class="headerlink" title="system.query_log"></a>system.query_log</h3><blockquote><p><a href="https://clickhouse.com/docs/en/operations/system-tables/query_log" target="_blank" rel="noopener">地址</a>，该表包含了所有进入到ck的sql语句。<br>该表包含字段如下：</p><ul><li>type (Enum8) — 执行查询时的事件类型. 值:<ul><li>‘QueryStart’ = 1 — 查询成功启动.</li><li>‘QueryFinish’ = 2 — 查询成功完成.</li></ul></li><li>‘ExceptionBeforeStart’ = 3 — 查询执行前有异常.</li><li>‘ExceptionWhileProcessing’ = 4 — 查询执行期间有异常.</li><li>event_date (Date) — 查询开始日期.</li><li>event_time (DateTime) — 查询开始时间.</li><li>event_time_microseconds (DateTime64) — 查询开始时间（毫秒精度）.</li><li>query_start_time (DateTime) — 查询执行的开始时间.</li><li>query_start_time_microseconds (DateTime64) — 查询执行的开始时间（毫秒精度）.</li><li>query_duration_ms (UInt64) — 查询消耗的时间（毫秒）.</li><li>read_rows (UInt64) — 从参与了查询的所有表和表函数读取的总行数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_rows 包括在所有副本上读取的行总数。 每个副本发送它的 read_rows 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li><li>read_bytes (UInt64) — 从参与了查询的所有表和表函数读取的总字节数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_bytes 包括在所有副本上读取的字节总数。 每个副本发送它的 read_bytes 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li><li>written_rows (UInt64) — 对于 INSERT 查询，为写入的行数。 对于其他查询，值为0。</li><li>written_bytes (UInt64) — 对于 INSERT 查询时，为写入的字节数。 对于其他查询，值为0。</li><li>result_rows (UInt64) — SELECT 查询结果的行数，或INSERT 的行数。</li><li>result_bytes (UInt64) — 存储查询结果的RAM量.</li><li>memory_usage (UInt64) — 查询使用的内存.</li><li>query (String) — 查询语句.</li><li>exception (String) — 异常信息.</li><li>exception_code (Int32) — 异常码.</li><li>stack_trace (String) — Stack Trace. 如果查询成功完成，则为空字符串。</li><li>is_initial_query (UInt8) — 查询类型. 可能的值:<ul><li>1 — 客户端发起的查询.</li><li>0 — 由另一个查询发起的，作为分布式查询的一部分.</li></ul></li><li>user (String) — 发起查询的用户.</li><li>query_id (String) — 查询ID.</li><li>address (IPv6) — 发起查询的客户端IP地址.</li><li>port (UInt16) — 发起查询的客户端端口.</li><li>initial_user (String) — 初始查询的用户名（用于分布式查询执行）.</li><li>initial_query_id (String) — 运行初始查询的ID（用于分布式查询执行）.</li><li>initial_address (IPv6) — 运行父查询的IP地址.</li><li>initial_port (UInt16) — 发起父查询的客户端端口.</li><li>interface (UInt8) — 发起查询的接口. 可能的值:<ul><li>1 — TCP.</li><li>2 — HTTP.</li></ul></li><li>os_user (String) — 运行 clickhouse-client的操作系统用户名.</li><li>client_hostname (String) — 运行clickhouse-client 或其他TCP客户端的机器的主机名。</li><li>client_name (String) — clickhouse-client 或其他TCP客户端的名称。</li><li>client_revision (UInt32) — clickhouse-client 或其他TCP客户端的Revision。</li><li>client_version_major (UInt32) — clickhouse-client 或其他TCP客户端的Major version。</li><li>client_version_minor (UInt32) — clickhouse-client 或其他TCP客户端的Minor version。</li><li>client_version_patch (UInt32) — clickhouse-client 或其他TCP客户端的Patch component。</li><li>http_method (UInt8) — 发起查询的HTTP方法. 可能值:<ul><li>0 — TCP接口的查询.</li><li>1 — GET</li><li>2 — POST</li></ul></li><li>http_user_agent (String) — http请求的客户端参数</li><li>quota_key (String) — 在quotas 配置里设置的“quota key” （见 keyed).</li><li>revision (UInt32) — ClickHouse revision.</li><li>ProfileEvents (Map(String, UInt64))) — 不同指标的计数器 </li><li>Settings (Map(String, String)) — 当前请求里的setting部分参数</li><li>thread_ids (Array(UInt64)) — 参与查询的线程数.</li><li>Settings.Names (Array（String)) — 客户端运行查询时更改的设置的名称。 要启用对设置的日志记录更改，请将log_query_settings参数设置为1。</li><li>Settings.Values (Array（String)) — Settings.Names 列中列出的设置的值。</li></ul></blockquote><h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><ul><li>查询当前时间内耗cpu最高的sql前10条<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select initial_user, event_time, query,</span><br><span class="line">read_rows, read_bytes,</span><br><span class="line">written_rows, written_bytes,</span><br><span class="line">result_rows, result_bytes,</span><br><span class="line">memory_usage, length(thread_ids) as thread_count</span><br><span class="line">from system.query_log</span><br><span class="line">WHERE event_time &gt; &#39;2022-10-27 18:30:00&#39; AND event_time &lt; &#39;2022-10-27 18:35:00&#39; </span><br><span class="line">and initial_user&lt;&gt;&#39;default&#39;</span><br><span class="line">order by thread_count desc</span><br><span class="line">limit 10;</span><br></pre></td></tr></table></figure></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;clickhouse是分布式系统，一条查询sql会经过pipeline处理后通过后台的查询线程池开启多线程查询，而经常会收到
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse 表写入报readonly排查</title>
    <link href="http://yoursite.com/2022/12/08/clickhouse%20%E8%A1%A8readonly%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2022/12/08/clickhouse%20%E8%A1%A8readonly%E9%97%AE%E9%A2%98/</id>
    <published>2022-12-08T08:04:11.000Z</published>
    <updated>2023-01-15T05:32:36.894Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse的Replicated<em>MergeTree表是通过zookeeper来完成同一个shard之间的副本数据的同步，当<em>*Table is in readonly mode</em></em>的原因是zookeeper当下压力过大，这个可以从system.replication_queue看到同步队列的数量，或者通过system.part_log看当前part的处理数量</p><h3 id="system-replicas"><a href="#system-replicas" class="headerlink" title="system.replicas"></a>system.replicas</h3><blockquote><p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/replicas" target="_blank" rel="noopener">官方地址</a>所解释,该表记录了驻留在本地服务器上的复制表的信息和状态，如下图所示，我们看到每个表的每个副本在zookeeper上的状态</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select table,zookeeper_path,replica_path from &#96;system&#96;.replicas limit 10</span><br></pre></td></tr></table></figure><p><img src="/images/clickhouse/readonly/1.png" alt="clickhouse"><br>如上图所示，<strong>zookeeper_path</strong>字段可以查看到在哪个shard上的哪个副本处于readonly状态。</p><h3 id="system-part-log"><a href="#system-part-log" class="headerlink" title="system.part_log"></a>system.part_log</h3><blockquote><p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/part_log" target="_blank" rel="noopener">官方地址</a>，该表包含part操作的所有记录，包括drop/merge/download/new等。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT *    </span><br><span class="line">FROM system.part_log</span><br><span class="line">WHERE  concat(database, &#39;.&#39;, table) &#x3D; &#39;i9xiaoapp_stream.dwd_pope_core_action_diff_di_local&#39;</span><br><span class="line">and event_time &gt;&#x3D; &#39;2023-01-15 12:00:00&#39;</span><br><span class="line">order by event_time desc</span><br></pre></td></tr></table></figure><br><img src="/images/clickhouse/readonly/2.png" alt="clickhouse"></p></blockquote><h3 id="解决副本不同步问题"><a href="#解决副本不同步问题" class="headerlink" title="解决副本不同步问题"></a>解决副本不同步问题</h3><blockquote><p>注意，此时先让用户把写入任务停掉，已方便观察！</p><ul><li>可以通过system.part_log查询mergepart状态的写入是否都已经完成</li><li>根据system.replicas表给出的readonly状态的zookeeper_path，去到当前所指向的副本机器上，删除这个副本的表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table i9xiaoapp_stream.dwd_pope_core_action_diff_di_local</span><br></pre></td></tr></table></figure></li><li>操作之后，可以通过查询system.replicas里看是否还有处于readonly，发现已经消失<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select table,zookeeper_path,replica_path from &#96;system&#96;.replicas where concat(database, &#39;.&#39;, table) &#x3D; &#39;i9xiaoapp_stream.dwd_pope_core_action_diff_di_local&#39; limit 10</span><br></pre></td></tr></table></figure></li><li>再在出问题的副本上，重新创建该本，之后可通过system.replicas找到该副本再次出现，并且已恢复<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table</span><br></pre></td></tr></table></figure></li><li>如果还没恢复，则去对应出错的副本节点，去检查一下zookeeper上的对应路径是否也已经删除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmr zookeeper_path</span><br></pre></td></tr></table></figure></li><li>此时再查询system.replicas表readonly的队列应该已经被清理掉了<br>可以继续操作元数据修改</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;clickhouse的Replicated&lt;em&gt;MergeTree表是通过zookeeper来完成同一个shard之间的副
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse副本不同步问题排查</title>
    <link href="http://yoursite.com/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/</id>
    <published>2022-12-08T08:04:11.000Z</published>
    <updated>2022-12-22T09:32:56.127Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse的Replicated**MergeTree表是通过zookeeper来完成同一个shard之间的副本数据的同步，因为clickhouse本身不是master/slave的架构，我们通过proxy的方式，设定了同一个shard里某个副本是读/写节点角色，另一个节点是读角色，当用户写入时，proxy会路由到读/写节点去完成写入操作，然后通过zk发起同步任务，把日志进入到system.replcation_queue。<br>今天早上用户和我们反馈说数据查询时每一次结果都不一样，用户查询也是通过连我们的proxy进行读节点的路由，所以用户反馈的结果不一致，其实是第一次路由到读/写节点查询结果是a，第二次路由到读节点，查询结果是b，由于a和b两个副本数据没有同步，导致了查询结果不一致，下面是排查的过程。</p><h3 id="system-replication-queue"><a href="#system-replication-queue" class="headerlink" title="system.replication_queue"></a>system.replication_queue</h3><blockquote><p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/replication_queue/" target="_blank" rel="noopener">官方地址</a>所解释,该表记录了当前的副本任务队列的所有信息，如下图所示，我们看到当前副本同步出现大量异常错误  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where data_files &gt; 0</span><br></pre></td></tr></table></figure><p><img src="/images/clickhouse/replicas_data_diff/1.png" alt="clickhouse"><br>如下图所示，<strong>type</strong>字段可以查看到当前是什么类型的操作导致的，发现是<strong>MUTATE_PART</strong>操作，<strong>last_exception</strong>字段显示当前我们在操作ttl时创建的元数据目录下columns.txt无法打开，这个是已知bug。<br><img src="/images/clickhouse/replicas_data_diff/2.png" alt="clickhouse"></p><h3 id="system-mutations"><a href="#system-mutations" class="headerlink" title="system.mutations"></a>system.mutations</h3><blockquote><p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/mutations" target="_blank" rel="noopener">官方地址</a>，该表包含了所有mutation操作的日志信息，<a href="https://clickhouse.com/docs/en/sql-reference/statements/alter/#mutations" target="_blank" rel="noopener">mutation</a>操作包括修改字段类型/修改表ttl操作/按条件删表的数据/按条件更新表的数据等，这些操作都是异步后台线程去处理，都会去回溯该表的所有parts，需要rewrite每个part的信息并且这个操作还不是原子性的，所以如果某个节点操作失败，可能引发该表无法使用。  </p></blockquote><p>该表包含字段如下：</p><ul><li><p>database (String) — 数据库名称.</p></li><li><p>table (String) — 表名称.</p></li><li><p>data_path (String) — 本地文件的路径.</p></li><li><p>mutation_id (String) — mutation的唯一标识，可通过该标识直接kill mutation.</p></li><li><p>command (String) — mutation命令.</p></li><li><p>create_time (DateTime) —  mutation创建时间.</p></li><li><p>block_numbers (Map) — partition_id：需要进行mutation操作的分区id，number：需要进行mutation的分区对应的block序号.</p></li><li><p>parts_to_do_names (Array) — 即将完成的需要进行mutation的数组.</p></li><li><p>parts_to_do (Int64) — 准备进行mutation操作的part序号.</p></li><li><p>is_done (UInt8) — 该操作是否已完成.</p></li><li><p>latest_failed_part (String) — mutation操作最后失败的part名称.</p></li><li><p>latest_fail_time (DateTime) — 最后失败的时间.</p></li><li><p>latest_fail_reason (String) — 最后失败的原因.</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.mutations where latest_failed_part !&#x3D; &#39;&#39;</span><br></pre></td></tr></table></figure><p><img src="/images/clickhouse/replicas_data_diff/3.png" alt="clickhouse"></p><blockquote><p>根据异常错误的<strong>command</strong>字段，我们看到错误是通过<code>MATERIALIZE TTL FAST 16070400</code>引起的，mutation操作在写节点处理完成后，也会通过zookeeper进行副本数据同步</p></blockquote><h3 id="解决副本不同步问题"><a href="#解决副本不同步问题" class="headerlink" title="解决副本不同步问题"></a>解决副本不同步问题</h3><ul><li>终止该失败的mutation<br>具体操作语句可以查看<a href="https://clickhouse.com/docs/zh/sql-reference/statements/kill/#:~:text=KILL%20MUTATION%E2%80%8B&amp;text=Tries%20to%20cancel%20and%20remove,list%20of%20mutations%20to%20stop." target="_blank" rel="noopener">官方文档</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill mutation where database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39; and mutation_id &#x3D; &#39;zz&#39;;</span><br></pre></td></tr></table></figure></li><li>操作之后，可以通过查询shard里每个副本的count是否一致来判断数据是否已经进行同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(dt), dt from xx.yy group by dt;</span><br></pre></td></tr></table></figure></li><li>操作之后，可以通过上面的<code>system.replication_queue</code>表来观察是否开始进行副本数据同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where type&#x3D;&#39;GET_PART&#39; and database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39;</span><br></pre></td></tr></table></figure></li><li>如果还没恢复，则去对应出错的副本节点，将本地表删除后重建（出错节点可以从上一步里看出来）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists xx.yy</span><br><span class="line">create table xx.yy</span><br></pre></td></tr></table></figure></li><li>此时再查询replication_queue表出错的队列应该已经被清理掉了<br>可以继续操作元数据修改</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;clickhouse的Replicated**MergeTree表是通过zookeeper来完成同一个shard之间的副本数
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse解析器大揭秘</title>
    <link href="http://yoursite.com/2022/11/04/clickhouse-parser/"/>
    <id>http://yoursite.com/2022/11/04/clickhouse-parser/</id>
    <published>2022-11-04T06:09:55.000Z</published>
    <updated>2022-11-04T09:01:49.663Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>我们知道语法分析器的作用是根据给定的<a href="https://baike.baidu.com/item/%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95/2447403" target="_blank" rel="noopener">形式文法</a>对由词法单元（Token）序列构成的输入进行语法检查、并构建由输入的词法单元（Token）组成的数据结构（一般是<a href="https://baike.baidu.com/item/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E6%A0%91/20452541" target="_blank" rel="noopener">语法分析树</a>、<a href="https://baike.baidu.com/item/%E6%8A%BD%E8%B1%A1%E8%AF%AD%E6%B3%95%E6%A0%91/6129952" target="_blank" rel="noopener">抽象语法树</a>等层次化的数据结构）。而一提到语法解析目前市面上有很多语法解析器，其中解析sql更是数不胜数，例如最为人所知的antlr和jflex，而本文的主人公ClickHouse却自己去纯手工打造实现了一套sql解析器，本篇文章就来聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制。</p><h2 id="简单入门"><a href="#简单入门" class="headerlink" title="简单入门"></a>简单入门</h2><blockquote><p>首先来简单入门解决个小问题，那就是我们如何去连接ck，如何将query传递ck呢，如何设置传递给ck的query长度呢？   </p></blockquote><h3 id="通过TCP方式请求"><a href="#通过TCP方式请求" class="headerlink" title="通过TCP方式请求"></a>通过TCP方式请求</h3><blockquote><p>通过tcp方式使用clickhouse自己的客户端，连接clickhouse，在会话session里先使用<strong>set max_query_size=xx</strong>的方式让当前这个会话修改query的长度，如下图：  </p></blockquote><p><img src="/images/clickhouse/maxquerysize/1.png" alt="clickhouse"></p><h3 id="通过HTTP方式请求"><a href="#通过HTTP方式请求" class="headerlink" title="通过HTTP方式请求"></a>通过HTTP方式请求</h3><blockquote><p>通过http方式请求，<a href="http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写">http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写</a><br>注意chproxy只允许最大max_query_size为512mb，超过此长度会直接报错  </p></blockquote><h3 id="通过sql创建setting授权给登陆用户"><a href="#通过sql创建setting授权给登陆用户" class="headerlink" title="通过sql创建setting授权给登陆用户"></a>通过sql创建setting授权给登陆用户</h3><ol><li>创建setting profile<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create settings profile if not exists role_max_query_size SETTINGS max_query_size &#x3D; 100000000000;</span><br></pre></td></tr></table></figure></li><li>将profile赋值给某个用户<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant role_max_query_size to prod_voyager_stats_events;</span><br></pre></td></tr></table></figure></li></ol><h3 id="源码解析ck是如何处理max-query-size的"><a href="#源码解析ck是如何处理max-query-size的" class="headerlink" title="源码解析ck是如何处理max_query_size的"></a>源码解析ck是如何处理max_query_size的</h3><blockquote><p>由于源码较多，只抽出具体实现函数进行源码讲解，本次讲解基于clickhouse v20.6.3.28-stable（该版本与最新版出入较大）。  </p></blockquote><p><img src="/images/clickhouse/maxquerysize/2.png" alt="clickhouse"></p><ol><li>如上图所示，在<code>HTTPHandler.cpp</code>下进行各种http的协议处理时，有一个变量叫<strong>HTMLForm</strong>类型的<code>params</code>，承载的是http请求里的<code>uri</code>，并且在代码的484行进行了此变量的处理，如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">for (const auto &amp; [key, value] : params)</span><br><span class="line">&#123;</span><br><span class="line">    if (key &#x3D;&#x3D; &quot;database&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (database.empty())</span><br><span class="line">            database &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (key &#x3D;&#x3D; &quot;default_format&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (default_format.empty())</span><br><span class="line">            default_format &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (param_could_be_skipped(key))</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        &#x2F;&#x2F;&#x2F; Other than query parameters are treated as settings.</span><br><span class="line">        if (!customizeQueryParam(context, key, value))</span><br><span class="line">            settings_changes.push_back(&#123;key, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>而<code>customizeQueryParam</code>会判断该参数param是否等于query，如果是则不会进入setting的设置，再判断是否是param_开头的如果是则会传入context（理解为这次session会话中需要设置的各种上下文内容）则也不会进行setting处理，不是前面2个case则进行setting处理，重载系统默认的setting里的参数，如下图<br><img src="/images/clickhouse/maxquerysize/3.png" alt="clickhouse"></li><li>虽然第二步已经设置了setting，但注意代码的512行，这行代码会走向<strong>SettingsConstraints.cpp</strong>类的<strong>checkImpl</strong>校验逻辑里，有一些配置是不允许修改的，例如<strong>profile</strong>，例如配置就是如果已经通过grant授权配置了<strong>setting profile</strong>了，会去看这个用户的相关权限，如果不符合则会直接抛出exception，不再进行处理，注意这里还有一个问题，抛了异常后，不再将此次请求写入到system.query_log中，之后我们会修复此问题<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; For external data we also want settings</span><br><span class="line">context.checkSettingsConstraints(settings_changes);</span><br><span class="line">context.applySettingsChanges(settings_changes);</span><br></pre></td></tr></table></figure></li><li>做完setting的约束校验后，都符合条件，则我们已经重载了setting里的<strong>max_query_size</strong>，之后就走入了<strong>executeQuery.cpp</strong>执行query逻辑，在它的构造函数里我们就可以看到query是根据<strong>max_query_size</strong>来读取的，如下图<br><img src="/images/clickhouse/maxquerysize/4.png" alt="clickhouse"></li></ol><h2 id="深入探知"><a href="#深入探知" class="headerlink" title="深入探知"></a>深入探知</h2><p>介绍完了<strong>max_query_size</strong>的处理逻辑后，其实我们已经大致明白ck在query的处理流程是如何流转的，那么现在问题来了，我们知道可以通过<code>select xx from tb SETTINGS max_query_size=12112</code>这种方式传入自定义的setting参数，但是有些参数有生效，而select语法却对max_query_size不生效，原因是什么呢？好了，别着急现在我们就来解答ck是如何处理setting这层逻辑的。</p><h3 id="为什么max-query-size的select中不生效？"><a href="#为什么max-query-size的select中不生效？" class="headerlink" title="为什么max_query_size的select中不生效？"></a>为什么max_query_size的select中不生效？</h3><p><img src="/images/clickhouse/maxquerysize/5.png" alt="clickhouse"><br>原因很简单，只要我们读过了上面的流转过程，就知道max_query_size这个参数的处理系统默认是256kb，那么如果未通过uri方式传入<strong>max_query_size</strong>，则在截取query长度前，默认都是256kb，注意截取query时是还未进行ck的parser逻辑处理的，我们可以看到query里的setting是需要经过ck的parser解析后，才会重载进去(如下图6)，所以呢如果你的select query在256kb范围内，则截取完整query后，经过ck的parser解析出ast树，是会带上新的setting，但此时已经没有意义了，而相反的如果你的query超过了256kb，则只截取到256kb前的query，此时setting也不会走到<strong>ParserSelectQuery</strong>里，同时因为你的query被不完整截取后，会直接报ast语法错误<br><img src="/images/clickhouse/maxquerysize/6.png" alt="图6"></p><h2 id="源码看解析器"><a href="#源码看解析器" class="headerlink" title="源码看解析器"></a>源码看解析器</h2><h3 id="1-HTTPHandler-cpp-gt-processQuery"><a href="#1-HTTPHandler-cpp-gt-processQuery" class="headerlink" title="1. HTTPHandler.cpp =&gt; processQuery"></a>1. HTTPHandler.cpp =&gt; processQuery</h3><blockquote><p>每一个http请求都在clickhouse都会起一个叫<strong>HTTPHandler</strong>的线程去处理，根据http请求header和body，初始化请求上下文环境：包括session、用户信息、当前database、响应信息等，另外还处理限流，用户权限，根据配置取到setting信息进行设置，本文重点是调用<code>executeQuery</code>方法处理<code>query</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">executeQuery(*in, *used_output.out_maybe_delayed_and_compressed, &#x2F;* allow_into_outfile &#x3D; *&#x2F; false, context,</span><br><span class="line">        [&amp;response] (const String &amp; current_query_id, const String &amp; content_type, const String &amp; format, const String &amp; timezone)</span><br><span class="line">        &#123;</span><br><span class="line">            response.setContentType(content_type);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Query-Id&quot;, current_query_id);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Format&quot;, format);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Timezone&quot;, timezone);</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="2-executeQuery-cpp-gt-executeQuery"><a href="#2-executeQuery-cpp-gt-executeQuery" class="headerlink" title="2. executeQuery.cpp =&gt; executeQuery"></a>2. executeQuery.cpp =&gt; executeQuery</h3><p>从流中读出字节到buffer里，根据设置的<code>max_query_size</code>判断buffer是否已满，复制到LimitReadBuffer里，重点是执行<strong>executeQueryImpl</strong>，返回tuple类型的(ast, stream)，从stream里提取出<strong>pipeline(流水线)</strong>，根据ast构造出<code>IBlockInputStream</code>或者<code>IBlockOutputStream</code>，传给pipeline后执行pipeline的execute方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::tie(ast, streams) &#x3D; executeQueryImpl(begin, end, context, false, QueryProcessingStage::Complete, may_have_tail, &amp;istr);</span><br></pre></td></tr></table></figure></p><h3 id="2-executeQuery-cpp-gt-executeQueryImpl"><a href="#2-executeQuery-cpp-gt-executeQueryImpl" class="headerlink" title="2. executeQuery.cpp =&gt; executeQueryImpl"></a>2. executeQuery.cpp =&gt; executeQueryImpl</h3><p>按照解析出的ast，构造出Interpreter，调用Interpreter的exec方法去执行后返回pipeline，执行结果记录到query_log里，最后把构造出对应的ast和pipeline返回<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 这里是实现了ParserQuery对象，继承了IParserBase，IParserBase继承自IParser，等下走到6时，才知道虚函数parseImpl会通过ParserQuery对象实现</span><br><span class="line">ParserQuery parser(end, settings.enable_debug_queries);</span><br><span class="line">.........</span><br><span class="line">ast &#x3D; parseQuery(parser, begin, end, &quot;&quot;, max_query_size, settings.max_parser_depth);</span><br><span class="line">.........</span><br><span class="line">auto interpreter &#x3D; InterpreterFactory::get(ast, context, stage);</span><br><span class="line">.........</span><br><span class="line">res &#x3D; interpreter-&gt;execute();</span><br><span class="line">QueryPipeline &amp; pipeline &#x3D; res.pipeline;</span><br><span class="line">.........</span><br><span class="line">return std::make_tuple(ast, std::move(res));</span><br></pre></td></tr></table></figure></p><h3 id="3-parseQuery-cpp-gt-parseQueryAndMovePosition"><a href="#3-parseQuery-cpp-gt-parseQueryAndMovePosition" class="headerlink" title="3. parseQuery.cpp =&gt; parseQueryAndMovePosition"></a>3. parseQuery.cpp =&gt; parseQueryAndMovePosition</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr res &#x3D; tryParseQuery(parser, pos, end, error_message, false, query_description, allow_multi_statements, max_query_size, max_parser_depth);</span><br></pre></td></tr></table></figure><h3 id="4-parseQuery-cpp-gt-tryParseQuery"><a href="#4-parseQuery-cpp-gt-tryParseQuery" class="headerlink" title="4. parseQuery.cpp =&gt; tryParseQuery"></a>4. parseQuery.cpp =&gt; tryParseQuery</h3><blockquote><p>尝试解析SQL，将sql通过语法树规则装入TokenIterator，返回ASTPtr  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ClickHouse词法分析器是由Tokens和Lexer类来实现，token是最基础的元祖，之间是没有任何关联的，只是一堆词组和符号，通过lexer语法进行解析后，把元祖里的token建立起关系。</span><br><span class="line">Tokens tokens(pos, end, max_query_size);</span><br><span class="line">IParser::Pos token_iterator(tokens, max_parser_depth);</span><br><span class="line">&#x2F;&#x2F; 注意这里，TokenIterator对-&gt;使用了重载，在重载函数里去初始化TOKEN，主要是从第一个字符开始使用pos++的方式进行判断，可以进入Token Lexer::nextTokenImpl()进行查看</span><br><span class="line">if (token_iterator-&gt;isEnd() || token_iterator-&gt;type &#x3D;&#x3D; TokenType::Semicolon) &#123;</span><br><span class="line">    out_error_message &#x3D; &quot;Empty query&quot;;</span><br><span class="line">    pos &#x3D; token_iterator-&gt;begin;</span><br><span class="line">    return nullptr;</span><br><span class="line">&#125;</span><br><span class="line">.....</span><br><span class="line">Expected expected;</span><br><span class="line">......</span><br><span class="line">ASTPtr res;</span><br><span class="line">bool parse_res &#x3D; parser.parse(token_iterator, res, expected);</span><br></pre></td></tr></table></figure><blockquote><p>注意：<code>IParser</code>的<code>parse</code>方法是<code>virtual</code>虚函数，<code>IParser</code>作为接口角色，被<code>IParserBase</code>继承，在<code>IParserBase</code>里实现了<code>parse</code>方法。</p></blockquote><h3 id="5-IParserBase-cpp-gt-parse"><a href="#5-IParserBase-cpp-gt-parse" class="headerlink" title="5. IParserBase.cpp =&gt; parse"></a>5. IParserBase.cpp =&gt; parse</h3><blockquote><p>在解每个token时都会根据当前的token进行预判（parseImpl返回的结果），返回true才会进入下一个子token  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bool IParserBase::parse(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    expected.add(pos, getName());</span><br><span class="line"></span><br><span class="line">    return wrapParseImpl(pos, IncreaseDepthTag&#123;&#125;, [&amp;]</span><br><span class="line">    &#123;</span><br><span class="line">        bool res &#x3D; parseImpl(pos, node, expected);</span><br><span class="line">        if (!res)</span><br><span class="line">            node &#x3D; nullptr;</span><br><span class="line">        return res;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意到parseImpl在IParserBase中是一个虚函数，将被继承自IParserBase类的子类实现，而在 <strong><em> 第2步 </em></strong>中我们定义的子类是ParserQuery，所以此时是直接调用到ParserQuery子类的parseImpl方法</p></blockquote><h3 id="6-ParserQuery-cpp-gt-parseImpl"><a href="#6-ParserQuery-cpp-gt-parseImpl" class="headerlink" title="6. ParserQuery.cpp =&gt; parseImpl"></a>6. ParserQuery.cpp =&gt; parseImpl</h3><blockquote><p>Parser的主要类（也都是继承自IParserBase）分别定义出来后，每个去尝试解析，如果都不在这几个主要Parser里，则返回false，否则返回true，clickhouse把query分类成以下14类，但本质上可以归纳为2类，第一类是有结果输出可对应show/select/desc/create等，第二类是无结果输出可对应insert/use/set等  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">bool ParserQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ParserQueryWithOutput query_with_output_p(enable_explain);</span><br><span class="line">    ParserInsertQuery insert_p(end);</span><br><span class="line">    ParserUseQuery use_p;</span><br><span class="line">    ParserSetQuery set_p;</span><br><span class="line">    ParserSystemQuery system_p;</span><br><span class="line">    ParserCreateUserQuery create_user_p;</span><br><span class="line">    ParserCreateRoleQuery create_role_p;</span><br><span class="line">    ParserCreateQuotaQuery create_quota_p;</span><br><span class="line">    ParserCreateRowPolicyQuery create_row_policy_p;</span><br><span class="line">    ParserCreateSettingsProfileQuery create_settings_profile_p;</span><br><span class="line">    ParserDropAccessEntityQuery drop_access_entity_p;</span><br><span class="line">    ParserGrantQuery grant_p;</span><br><span class="line">    ParserSetRoleQuery set_role_p;</span><br><span class="line">    ParserExternalDDLQuery external_ddl_p;</span><br><span class="line"></span><br><span class="line">    bool res &#x3D; query_with_output_p.parse(pos, node, expected)</span><br><span class="line">        || insert_p.parse(pos, node, expected)</span><br><span class="line">        || use_p.parse(pos, node, expected)</span><br><span class="line">        || set_role_p.parse(pos, node, expected)</span><br><span class="line">        || set_p.parse(pos, node, expected)</span><br><span class="line">        || system_p.parse(pos, node, expected)</span><br><span class="line">        || create_user_p.parse(pos, node, expected)</span><br><span class="line">        || create_role_p.parse(pos, node, expected)</span><br><span class="line">        || create_quota_p.parse(pos, node, expected)</span><br><span class="line">        || create_row_policy_p.parse(pos, node, expected)</span><br><span class="line">        || create_settings_profile_p.parse(pos, node, expected)</span><br><span class="line">        || drop_access_entity_p.parse(pos, node, expected)</span><br><span class="line">        || grant_p.parse(pos, node, expected)</span><br><span class="line">        || external_ddl_p.parse(pos, node, expected);</span><br><span class="line"></span><br><span class="line">    return res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意看这个parseImpl方法，进来后都会先去接<code>ParserQueryWithOutput</code>类解析相关ast，这里类涉及到了<code>explain</code>、<code>select</code>、<code>show</code>，<code>create</code>、<code>alter</code>等相关语法的解析，如果解析不过，则直接报错，解析成功后会处理我们这篇文章中提到的<strong>SETTING</strong>，如下图7定义的，将setting传入的变量存入到<code>s_settings</code>指针中。<br><img src="/images/clickhouse/maxquerysize/7.png" alt="图7"></p><h3 id="clcikhouse的parser总结："><a href="#clcikhouse的parser总结：" class="headerlink" title="clcikhouse的parser总结："></a>clcikhouse的parser总结：</h3><ul><li><ol><li>ClickHouse词法分析器<br>词法解析的主要任务是读入源程序的输入字符、将它们组成词素，生成并输出一个词法单元（Token）序列，每个词法单元对应于一个词素。<code>ClickHouse</code>中的每个词法单元（<code>Token</code>）使用一个<code>struct Tocken</code>结构体对象来进行存储，结构体中存储了词法单元的<code>type</code>和<code>value</code>。<br>ClickHouse词法分析器是由<code>Tokens</code>和<code>Lexer</code>类来实现， <strong><em>DB::Lexer::nextTokenImpl()</em></strong>函数用来对<code>SQL</code>语句进行词法分析的具体实现</li></ol></li><li><ol><li>ClickHouse语法解析器<br>ClickHouse中定义了不同的Parser用来对不同类型的SQL语句进行语法分析，例如：ParserInsertQuery（Insert语法分析器）、ParserCreateQuery（Create语法分析器）、ParserAlterQuery（Alter语法分析器）等等。<br>Parser首先判断输入的Token序列是否是该类型的SQL，若是该类型的SQL，则继续检查语法的正确性，正确则生成AST返回，语法错误的则抛出语法错误异常，否则直接返回空AST语法树</li></ol></li></ul><p><img src="/images/clickhouse/parser/1.png" alt="clickhouse"></p><h3 id="解答setting生效问题"><a href="#解答setting生效问题" class="headerlink" title="解答setting生效问题"></a>解答setting生效问题</h3><p><img src="/images/clickhouse/maxquerysize/8.png" alt="图7"><br>如上图所示，当前原生ck只支持InterpreterSelectQuery和InterpreterInsertQuery对query传入setting进行了重载处理。<br>InterpreterSelectQuery是在自己的构造函数里初始化了setting到context里<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">void InterpreterSelectQuery::initSettings()</span><br><span class="line">&#123;</span><br><span class="line">    auto &amp; query &#x3D; getSelectQuery();</span><br><span class="line">    if (query.settings())</span><br><span class="line">        InterpreterSetQuery(query.settings(), *context).executeForCurrentContext();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>InterpreterInsertQuery是在parser解析出ast后，在<code>executeQueryImpl</code>进行的setting重载context。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto * insert_query &#x3D; ast-&gt;as&lt;ASTInsertQuery&gt;();</span><br><span class="line"></span><br><span class="line">if (insert_query &amp;&amp; insert_query-&gt;settings_ast)</span><br><span class="line">    InterpreterSetQuery(insert_query-&gt;settings_ast, context).executeForCurrentContext();</span><br></pre></td></tr></table></figure><br>剩下的setting都已经是通过Interpreter执行结束后再处理的，对于我们需要在前置传入时没有效果了。</p><h3 id="解决业务困扰"><a href="#解决业务困扰" class="headerlink" title="解决业务困扰"></a>解决业务困扰</h3><p>当前我们的离线导入hive2ck，其实是通过将数据写入到临时表，这张临时表是按照目标表的表结构重新创建了一个MergeTree表，通过spark任务将hive数据以流式方式写入到临时表分区，生产出分区对应的多个part，生产过程中我们会将part拉回到临时表对应的detach目录，这个过程叫<a href="http://way.xiaojukeji.com/article/29557" target="_blank" rel="noopener">离线构建</a>，再将再将part通过ck的attach命令激活，这时候临时表就对该分区可见了，然后再通过replace partition的方式，将临时表的分区替换到我们的目标表去，这整个过程，就是我们的hive2ck处理流程，如下图所示：<br><img src="/images/clickhouse/maxquerysize/9.png" alt="图10"></p><blockquote><p>我们将此离线构建继承到数梦的同步中心后，陆续遇到了业务方来咨询相关问题，下面是问题汇总及如何解决的  </p></blockquote><h4 id="如何在离线导入中将明细数据写入到关联的物化视图"><a href="#如何在离线导入中将明细数据写入到关联的物化视图" class="headerlink" title="如何在离线导入中将明细数据写入到关联的物化视图"></a>如何在离线导入中将明细数据写入到关联的物化视图</h4><p>熟悉clickhouse的同学们都知道，原生ck对于物化视图的写入，唯一的方式是在明细表通过insert写入时，才会将数据经过物化视图的触发器写入关联的物化视图，而在离线构建过程中，ck是不支持的，但很多业务方跟我们提出这个需求，希望离线构建可以支持将分区数据写入到关联物化视图去，于是我们对ck的replace partition 进行了改造。<br>改造前的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221102&#39; FROM test.visits_basic_tmp;</span><br></pre></td></tr></table></figure><br>改造后的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221101&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW;</span><br></pre></td></tr></table></figure><br>在离线构建走到替换分区这一步时，我们改造了AstAlterQuery，让<code>ParserAlterQuery</code>增加了对<code>and trigger view</code>的语法解析，解析之后进入到<code>InterpreterAlterQuery</code>时，如果ast返回的trigger view是true，则程序流程会流转到取出明细表元数据，查询是否有关联物化视图，重新构造出类似下面的sql，交过pipeline进行执行，由此将该分区数据写入到物化视图去。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into 物化视图 select from 明细表 where 分区&#x3D;xx</span><br></pre></td></tr></table></figure></p><h4 id="分区过大导入失败如何解决"><a href="#分区过大导入失败如何解决" class="headerlink" title="分区过大导入失败如何解决"></a>分区过大导入失败如何解决</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx has timed out! (120000ms) Possible deadlock avoided. Client should retry</span><br></pre></td></tr></table></figure><p><img src="/images/clickhouse/maxquerysize/10.png" alt="图10"><br>如上图所示，替换分区前，会给该明细表加一把锁，并设定锁时间（lock_acquire_timeout），系统默认时120s，如果该分区过大，替换过程超过120s，则会爆上面错误，而本文最开始已经讲解过如何处理setting，考虑到ck原生只支持insert和select时interpreter对setting重载，由此进行改造让InterpreterAlterQuery也支持通过sql传入锁时间，如下面语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221108&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW SETTINGS lock_acquire_timeout&#x3D;86400000;</span><br></pre></td></tr></table></figure><br>因为ParserQueryWithOutput已经对setting进行了解析，而AstAlterQuery其实是继承自ASTQueryWithOutput，所以我们已经获得了setting这一块的ast，无需再自己初始化新的ast，只要在InterpreterAlterQuery里把setting重载就行了，如图11<br><img src="/images/clickhouse/maxquerysize/11.png" alt="图10"></p><h4 id="替换分区成功，物化视图数据写入报错如何解决"><a href="#替换分区成功，物化视图数据写入报错如何解决" class="headerlink" title="替换分区成功，物化视图数据写入报错如何解决"></a>替换分区成功，物化视图数据写入报错如何解决</h4><ol><li>首先我们在数梦平台上控制了相关的ddl语句修改，如果用户要删明细表字段，则必须先去处理关联的物化视图字段，如果用户要删明细表，则必须先删物化视图</li><li>遇到替换分区成功，而物化视图写入失败，报错都是锁明细表超时，对于这种case可直接解锁明细表的锁，让物化视图自己去写，不再锁明细表，所以只需要做简单的锁释放便可</li></ol><p>以上是对ck进行改造过程中遇到的3个问题，此改造过程主要是满足离线导入可写入物化视图，未来我们还将对ck进行更多改造，以满足不同业务需求，各个 业务线大佬们如果在使用ck过程中有遇到任何问题，欢迎加入ck用户群，和我们一起沟通解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h2&gt;&lt;p&gt;我们知道语法分析器的作用是根据给定的&lt;a href=&quot;https://baike.baidu.com/item/%E5%BD
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>starrocks在滴滴的落地</title>
    <link href="http://yoursite.com/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/"/>
    <id>http://yoursite.com/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/</id>
    <published>2022-10-17T06:48:24.803Z</published>
    <updated>2023-01-07T15:53:33.781Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><h4 id="ClickHouse介绍"><a href="#ClickHouse介绍" class="headerlink" title="ClickHouse介绍"></a>ClickHouse介绍</h4><blockquote><p><code>ClickHouse</code>是由俄罗斯的第一大搜索引擎<code>Yandex</code>公司开源的列存数据库。令人惊喜的是，<code>ClickHouse</code>相较于很多商业<code>MPP</code>数据库，比如<code>Vertica</code>，<code>InfiniDB</code>有着极大的性能提升。除了<code>Yandex</code>以外，越来越多的公司开始尝试使用<code>ClickHouse</code>等列存数据库。对于一般的分析业务，结构性较强且数据变更不频繁，可以考虑将需要进行关联的表打平成宽表，放入<code>ClickHouse</code>中。</p><ul><li>配置丰富，只依赖与<code>Zookeeper</code></li><li>线性可扩展性，可以通过添加服务器扩展集群</li><li>容错性高，不同分片间采用异步多主复制</li><li>单表性能极佳，采用向量计算，支持采样和近似计算等优化手段</li><li>功能强大支持多种表引擎</li></ul></blockquote><h4 id="StarRocks介绍"><a href="#StarRocks介绍" class="headerlink" title="StarRocks介绍"></a>StarRocks介绍</h4><blockquote><p><code>StarRocks</code>是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容<code>MySQL</code>协议和<code>MySQL</code>生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。<code>StarRocks</code>致力于在全场景<code>OLAP</code>业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</p><ul><li>不依赖于大数据生态，同时外表的联邦查询可以兼容大数据生态</li><li>提供多种不同的模型，支持不同维度的数据建模</li><li>支持在线弹性扩缩容，可以自动负载均衡</li><li>支持高并发分析查询</li><li>实时性好，支持数据秒级写入</li><li>兼容MySQL 5.7协议和MySQL生态</li></ul></blockquote><h4 id="二者的对比"><a href="#二者的对比" class="headerlink" title="二者的对比"></a>二者的对比</h4><p><strong>相似之处</strong></p><ul><li>都可以提供极致的性能</li><li>都不依赖于<code>Hadoop</code>生态</li><li>底层存储分片都提供了主主的复制高可用机制。</li><li>都是<code>MPP</code>架构</li><li>都是列式存储</li><li>都支持表述SQL语法</li><li>都提供了MOLAP库的预聚合能力</li></ul><p><strong>差异性</strong></p><ul><li><code>ClickHouse</code>在更适用于大宽表的场景，<code>TP</code>的数据通过<code>CDC</code>工具的，可以考虑在<code>Flink</code>中将需要关联的表打平，以大宽表的形式写入<code>ClickHouse</code></li><li><code>StarRocks</code>对于<code>join</code>的能力更强，<code>ClickHouse虽</code>然提供了<code>join</code>的语义，但使用上对大表关联的能力支撑较弱，复杂的关联查询经常会引起<code>OOM</code></li><li><code>ClickHouse</code>对高并发的业务并不友好，即使一个查询，也会用服务器一半的<code>CPU</code>去查询</li><li><code>StarRocks</code>可以支撑数千用户同时进行分析查询，在部分场景下，高并发能力能够达到万级。<code>StarRocks</code>在数据存储层，采用先分区再分桶的策略，增加了数据的指向性，利用前缀索引可以快读对数据进行过滤和查找，减少磁盘的<code>I/O</code>操作，提升查询性能</li><li>对于用户的原有的查询基表的 <code>SQL</code> 语句保持不变，<code>StarRocks</code> 会自动选择一个最优的物化视图，从物化视图中读取数据并计算。用户可以通过 <code>EXPLAIN</code> 命令来检查当前查询是否使用了物化视图。而<code>ClickHouse</code>则需要用户自行指定<code>SQL</code>中所需要使用的物化视图。</li></ul><h4 id="为什么推荐StarRocks"><a href="#为什么推荐StarRocks" class="headerlink" title="为什么推荐StarRocks"></a>为什么推荐StarRocks</h4><ol><li>滴滴大数据<code>OLAP</code>团队目前在维护的引擎有<code>StarRocks</code>、<code>ClickHouse</code>、<code>Druid</code>，3个引擎各有各的特点，现有的OLAP引擎(Kylin、Druid、ClickHouse)多表Join时的性能都比较差，甚至不支持多表Join，现有的引擎<code>Druid</code>虽然有<code>lookup</code>表的能力，但经过实际测试后性能不佳。<code>Apache Kylin</code>实际上也不支持<code>Join</code>，多表的<code>Join</code>需要通过在<code>cube</code>构建的时候底层打成宽表来实现。<code>ClickHouse</code>只支持本地<code>Hash join</code>的模式，不支持分布式<code>Shuffle join</code>，多数情况下灵活性受限，性能表现不佳。</li><li><code>OLAP</code>引擎需要同时具备明细数据查询和数据聚合的能力。由于<code>Apache Kylin</code>、<code>Druid</code>不能较好支持明细数据查询，我们引入了<code>ClickHouse</code>，通过在明细表基础上创建相应聚合物化视图来处理，但是不够灵活，对于上层应用来说，查明细和查聚合需要切到不同的表去处理。</li><li>目前我们团队在有限人员情况下需要维护这3个引擎的稳定性，导致我们对每一个引擎的理解深度都不够，特别像<code>ClickHouse</code>，运维成本非常高，ClickHouse集群的分片、副本信息，都是通过静态的配置文件的方式进行配置。当整个集群需要扩缩容的时候，就必须通过修改配置文件的方式进行刷新，数据的均衡都需要运维人员介入。此外ClickHouse通过zookeeper来做副本管理，当集群规模变大时，副本数过多会导致zookeeper的压力变大，集群的稳定性也就会相应变差。<br>为解决以上问题，滴滴大数据<code>OLAP</code>团队在2022年初开始调研<code>StarRocks</code>，在全面测试过从上面对<code>StarRocks</code>和<code>ClickHouse</code>的对比，我们也可以明显感受到<code>StarRocks</code>在多数场景下都是优于<code>ClickHouse</code>的，我们希望通过<code>StarRocks</code>来实现<code>OLAP</code>平台的多业务场景的查询引擎统一化。<br><img src="/images/starrocks/helloworld/20.png" alt="1"><br>注：这是我们针对<code>Druid</code>、<code>ClickHouse</code>、<code>StarRocks</code>进行的测试对比，<a href="http://wiki.intra.xiaojukeji.com/pages/viewpage.action?pageId=799050659" target="_blank" rel="noopener">链接</a>。</li></ol><h3 id="StarRocks特性"><a href="#StarRocks特性" class="headerlink" title="StarRocks特性"></a>StarRocks特性</h3><blockquote><p><code>StarRocks</code>的架构设计融合了<code>MPP</code>数据库，以及分布式系统的设计思想，其特性如下所示。</p><h4 id="架构精简"><a href="#架构精简" class="headerlink" title="架构精简"></a>架构精简</h4><ul><li><code>StarRocks</code>内部通过<code>MPP</code>计算框架完成<code>SQL</code>的具体执行工作。<code>MPP</code>框架能够充分的利用多节点的计算能力，整个查询可以并行执行，从而实现良好的交互式分析体验。</li><li><code>StarRocks</code>集群不需要依赖任何其他组件，易部署、易维护和极简的架构设计，降低了<code>StarRocks</code>系统的复杂度和维护成本，同时也提升了系统的可靠性和扩展性。管理员只需要专注于<code>StarRocks</code>系统，无需学习和管理任何其他外部系统。</li></ul></blockquote><h4 id="全面向量化引擎"><a href="#全面向量化引擎" class="headerlink" title="全面向量化引擎"></a>全面向量化引擎</h4><p><code>StarRocks</code>的计算层全面采用了向量化技术，将所有算子、函数、扫描过滤和导入导出模块进行了系统性优化。通过列式的内存布局、适配<code>CPU</code>的<code>SIMD</code>指令集等手段，充分发挥了现代<code>CPU</code>的并行计算能力，从而实现亚秒级别的多维分析能力。</p><h4 id="智能查询优化"><a href="#智能查询优化" class="headerlink" title="智能查询优化"></a>智能查询优化</h4><p><code>StarRocks</code>通过<code>CBO</code>优化器 <strong>（Cost Based Optimizer）</strong> 可以对复杂查询自动优化。无需人工干预，就可以通过统计信息合理估算执行成本，生成更优的执行计划，大大提高了<code>AdHoc</code>和<code>ETL</code>场景的数据分析效率。</p><h4 id="联邦查询"><a href="#联邦查询" class="headerlink" title="联邦查询"></a>联邦查询</h4><p><code>StarRocks</code>支持使用外表的方式进行联邦查询，当前可以支持<code>Hive</code>、<code>MySQL</code>、<code>Elasticsearch</code>、<code>Iceberg</code>和<code>Hudi</code>类型的外表，您无需通过数据导入，可以直接进行数据查询加速。</p><h4 id="高效更新"><a href="#高效更新" class="headerlink" title="高效更新"></a>高效更新</h4><p><code>StarRocks</code>支持明细模型(DUPLICATE KEY)、聚合模型(AGGREGATE KEY)、主键模型(PRIMARY KEY)和更新模型(UNIQUE KEY)，其中主键模型可以按照主键进行<code>Upsert</code>或<code>Delete</code>操作，通过存储和索引的优化可以在并发更新的同时实现高效的查询优化，更好的服务实时数仓的场景。</p><h4 id="智能物化视图"><a href="#智能物化视图" class="headerlink" title="智能物化视图"></a>智能物化视图</h4><ul><li><code>StarRocks</code>支持智能的物化视图。您可以通过创建物化视图，预先计算生成预聚合表用于加速聚合类查询请求。</li><li><code>StarRocks</code>的物化视图能够在数据导入时自动完成汇聚，与原始表数据保持一致。</li><li>查询的时候，您无需指定物化视图，<code>StarRocks</code>能够自动选择最优的物化视图来满足查询请求。</li></ul><h4 id="标准SQL"><a href="#标准SQL" class="headerlink" title="标准SQL"></a>标准SQL</h4><ul><li><code>StarRocks</code>支持标准的<code>SQL</code>语法，包括聚合、<code>JOIN</code>、排序、窗口函数和自定义函数等功能。</li><li><code>StarRocks</code>可以完整支持<code>TPC-H</code>的22个<code>SQL</code>和<code>TPC-DS</code>的99个<code>SQL</code>。</li><li><code>StarRocks</code>兼容<code>MySQL</code>协议语法，可以使用现有的各种客户端工具、<code>BI</code>软件访问<code>StarRocks</code>，对<code>StarRocks</code>中的数据进行拖拽式分析。</li></ul><h4 id="流批一体"><a href="#流批一体" class="headerlink" title="流批一体"></a>流批一体</h4><ul><li><code>StarRocks</code>支持实时和批量两种数据导入方式。</li><li><code>StarRocks</code>支持的数据源有<code>Kafka</code>、<code>HDFS</code>和本地文件。</li><li><code>StarRocks</code>支持的数据格式有<code>ORC</code>、<code>Parquet</code>和<code>CSV</code>等。</li><li><code>StarRocks</code>可以实时消费<code>Kafka</code>数据来完成数据导入，保证数据不丢不重 <strong>（exactly once）</strong>。</li><li><code>StarRocks</code>也可以从本地或者远程（HDFS）批量导入数据。</li></ul><h4 id="高可用易扩展"><a href="#高可用易扩展" class="headerlink" title="高可用易扩展"></a>高可用易扩展</h4><ul><li><code>StarRocks</code>的元数据和数据都是多副本存储，并且集群中服务有热备，多实例部署，避免了单点故障。</li><li>集群具有自愈能力，可弹性恢复，节点的宕机、下线和异常都不会影响<code>StarRocks</code>集群服务的整体稳定性。</li><li><code>StarRocks</code>采用分布式架构，存储容量和计算能力可近乎线性水平扩展。<code>StarRocks</code>单集群的节点规模可扩展到数百节点，数据规模可达到10 PB级别。</li><li>扩缩容期间无需停服，可以正常提供查询服务。</li><li><code>StarRocks</code>中表模式热变更，可通过一条简单<code>SQL</code>命令动态地修改表的定义，例如增加列、减少列和新建物化视图等。同时，处于模式变更中的表也可以正常导入和查询数据。</li></ul><h3 id="StarRocks应用场景"><a href="#StarRocks应用场景" class="headerlink" title="StarRocks应用场景"></a>StarRocks应用场景</h3><blockquote><p>StarRocks可以满足企业级用户的多种分析需求，具体的业务场景如下所示：</p><h4 id="OLAP多维分析"><a href="#OLAP多维分析" class="headerlink" title="OLAP多维分析"></a>OLAP多维分析</h4><ul><li>用户行为分析</li><li>用户画像、标签分析、圈人</li><li>高维业务指标报表</li><li>自助式报表平台</li><li>业务问题探查分析</li><li>跨主题业务分析</li><li>财务报表</li><li>系统监控分析</li></ul></blockquote><h4 id="实时数仓"><a href="#实时数仓" class="headerlink" title="实时数仓"></a>实时数仓</h4><ul><li>电商大促数据分析</li><li>教育行业的直播质量分析</li><li>物流行业的运单分析</li><li>金融行业绩效分析、指标计算</li><li>广告投放分析</li><li>管理驾驶舱</li><li>探针分析APM（Application Performance Management）</li></ul><h4 id="高并发查询"><a href="#高并发查询" class="headerlink" title="高并发查询"></a>高并发查询</h4><ul><li>广告主报表分析</li><li>零售行业渠道人员分析</li><li>saas行业面向用户分析报表</li><li>dashboard多页面分析</li></ul><h4 id="统一分析"><a href="#统一分析" class="headerlink" title="统一分析"></a>统一分析</h4><p>通过使用一套系统解决多维分析、高并发查询、实时分析和Ad-Hoc查询等场景，降低系统复杂度和多技术栈开发与维护成本。</p><h3 id="如何使用StarRocks"><a href="#如何使用StarRocks" class="headerlink" title="如何使用StarRocks"></a>如何使用StarRocks</h3><blockquote><p>我们olap团队已经将<code>StarRocks</code>接入到滴滴各个平台，下面会介绍如何使用滴滴内部的平台和工具方便快捷的使用<code>StarRocks</code>引擎。<br><img src="/images/starrocks/helloworld/19.png" alt="1"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;h4 id=&quot;ClickHouse介绍&quot;&gt;&lt;a href=&quot;#ClickHouse介绍&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>clickhouse离线导入大揭秘</title>
    <link href="http://yoursite.com/2022/09/26/replace-trigger/"/>
    <id>http://yoursite.com/2022/09/26/replace-trigger/</id>
    <published>2022-09-26T14:57:11.000Z</published>
    <updated>2022-09-28T06:01:34.018Z</updated>
    
    <content type="html"><![CDATA[<h3 id="如何按照hive表的表结构创建clickhouse表"><a href="#如何按照hive表的表结构创建clickhouse表" class="headerlink" title="如何按照hive表的表结构创建clickhouse表"></a>如何按照hive表的表结构创建clickhouse表</h3><p>点击<a href="http://studio.data.didichuxing.com/stream/create-table?tableType=clickHouse" target="_blank" rel="noopener">链接</a>通过数梦的实时平台可以快捷创建对应hive表结构的ck表，如下图所示：<br><img src="/images/clickhouse/hive2clickhouse/1.png" alt="clickhouse"></p><h3 id="hive表的数据如何导入clickhouse表"><a href="#hive表的数据如何导入clickhouse表" class="headerlink" title="hive表的数据如何导入clickhouse表"></a>hive表的数据如何导入clickhouse表</h3><h4 id="创建同步任务"><a href="#创建同步任务" class="headerlink" title="创建同步任务"></a>创建同步任务</h4><p>点击<a href="http://sync.data-pre.didichuxing.com/job/offline/edit/818177?step=1" target="_blank" rel="noopener">链接</a>通过数梦的同步中心可以快捷创建hive表映射到ck表的同步任务，如下图所示<br><img src="/images/clickhouse/hive2clickhouse/2.png" alt="clickhouse"></p><h4 id="同步任务流程"><a href="#同步任务流程" class="headerlink" title="同步任务流程"></a>同步任务流程</h4><p>如下图所示，我们按照这个流程处理hive数据导入到clickhouse<br><img src="/images/clickhouse/hive2clickhouse/3.png" alt="clickhouse"></p><ul><li><ol><li>创建临时表<br>按照ck表的表结构，我们会在集群的所有写节点创建同样表结构的单机表（MergeTree）引擎</li></ol></li><li><ol><li>起spark任务，利用clickhouse-local工具将hive表导入到临时表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat data.orc | clickhouse-local \</span><br><span class="line">--format&#x3D;Native \</span><br><span class="line">--query&#x3D;&#39;CREATE TABLE input (col1 String, col2 String, col3 String) ENGINE &#x3D; File(ORC, stdin);CREATE TABLE target_table (col1 String, col2 String, col3 String) ENGINE &#x3D; MergeTree() partition by tuple() order by col1;insert into target_table select *,&quot;$year&quot; as year,&quot;$month&quot; as month, &quot;$day&quot; as day from input;optimize table target_table final&#39; \</span><br><span class="line">--config-file&#x3D;config.xmls</span><br></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;如何按照hive表的表结构创建clickhouse表&quot;&gt;&lt;a href=&quot;#如何按照hive表的表结构创建clickhouse表&quot; class=&quot;headerlink&quot; title=&quot;如何按照hive表的表结构创建clickhouse表&quot;&gt;&lt;/a&gt;如何按照hive
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse的写入流程</title>
    <link href="http://yoursite.com/2022/04/05/clickhouse-insert/"/>
    <id>http://yoursite.com/2022/04/05/clickhouse-insert/</id>
    <published>2022-04-05T07:09:55.000Z</published>
    <updated>2022-08-22T10:09:00.256Z</updated>
    
    <content type="html"><![CDATA[<h3 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h3><p>ck请求处理过程<br><img src="/images/clickhouse/insert/1.png" alt="clickhouse"></p><h4 id="客户端请求发给ck，ck接收到请求，放入请求队列"><a href="#客户端请求发给ck，ck接收到请求，放入请求队列" class="headerlink" title="客户端请求发给ck，ck接收到请求，放入请求队列"></a>客户端请求发给ck，ck接收到请求，放入请求队列</h4><h4 id="ck将请求队列的请求分发给Request-handler线程处理"><a href="#ck将请求队列的请求分发给Request-handler线程处理" class="headerlink" title="ck将请求队列的请求分发给Request handler线程处理"></a>ck将请求队列的请求分发给Request handler线程处理</h4><h4 id="Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。"><a href="#Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。" class="headerlink" title="Request handler线程处理请求，解析sql语句，执行sql语句逻辑。"></a>Request handler线程处理请求，解析sql语句，执行sql语句逻辑。</h4><h4 id="简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）"><a href="#简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）" class="headerlink" title="简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）"></a>简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）</h4><h4 id="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"><a href="#请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求" class="headerlink" title="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"></a>请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求</h4><h3 id="insert语句解析执行"><a href="#insert语句解析执行" class="headerlink" title="insert语句解析执行"></a>insert语句解析执行</h3><h4 id="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"><a href="#初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息" class="headerlink" title="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"></a>初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息</h4><h4 id="解析sql语句"><a href="#解析sql语句" class="headerlink" title="解析sql语句"></a>解析sql语句</h4><h4 id="检查被写入的表是否存在，是否有写入权限，是否被限流"><a href="#检查被写入的表是否存在，是否有写入权限，是否被限流" class="headerlink" title="检查被写入的表是否存在，是否有写入权限，是否被限流"></a>检查被写入的表是否存在，是否有写入权限，是否被限流</h4><h4 id="对insert数据校验，字段是否存在，是否满足约束"><a href="#对insert数据校验，字段是否存在，是否满足约束" class="headerlink" title="对insert数据校验，字段是否存在，是否满足约束"></a>对insert数据校验，字段是否存在，是否满足约束</h4><h4 id="根据默认值填充空字段和物化字段"><a href="#根据默认值填充空字段和物化字段" class="headerlink" title="根据默认值填充空字段和物化字段"></a>根据默认值填充空字段和物化字段</h4><h4 id="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"><a href="#缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。" class="headerlink" title="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"></a>缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。</h4><h4 id="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"><a href="#将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree" class="headerlink" title="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"></a>将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree</h4><h4 id="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"><a href="#检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表" class="headerlink" title="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"></a>检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表</h4><h3 id="分布式表写入"><a href="#分布式表写入" class="headerlink" title="分布式表写入"></a>分布式表写入</h3><blockquote><p>分布式表数据写入一般情况下是异步写入，只有对使用 remote(‘addresses_expr’, db, table[, ‘user’[, ‘password’], sharding_key]) 定义的表的写入是同步的。如果分布式表中含有local表或replical的表local副本，直接写本地表。<br><img src="/images/clickhouse/insert/2.png" alt="clickhouse"></p></blockquote><h4 id="将写入block数据按sharding逻辑分成多个block"><a href="#将写入block数据按sharding逻辑分成多个block" class="headerlink" title="将写入block数据按sharding逻辑分成多个block"></a>将写入block数据按sharding逻辑分成多个block</h4><h4 id="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"><a href="#将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据" class="headerlink" title="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"></a>将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据</h4><h4 id="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"><a href="#检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎" class="headerlink" title="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"></a>检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎</h4><h4 id="shard在远程，将新insert语句写入远程shard本地缓存文件。"><a href="#shard在远程，将新insert语句写入远程shard本地缓存文件。" class="headerlink" title="shard在远程，将新insert语句写入远程shard本地缓存文件。"></a>shard在远程，将新insert语句写入远程shard本地缓存文件。</h4><h4 id="通知后台线程发送本地缓存中的数据"><a href="#通知后台线程发送本地缓存中的数据" class="headerlink" title="通知后台线程发送本地缓存中的数据"></a>通知后台线程发送本地缓存中的数据</h4><h4 id="后台执行过程"><a href="#后台执行过程" class="headerlink" title="后台执行过程"></a>后台执行过程</h4><h4 id="读远程shard本地缓存目录"><a href="#读远程shard本地缓存目录" class="headerlink" title="读远程shard本地缓存目录"></a>读远程shard本地缓存目录</h4><h4 id="逐个处理每个文件"><a href="#逐个处理每个文件" class="headerlink" title="逐个处理每个文件"></a>逐个处理每个文件</h4><h4 id="根据配置的loadbalance策略，选择合适的机器连接"><a href="#根据配置的loadbalance策略，选择合适的机器连接" class="headerlink" title="根据配置的loadbalance策略，选择合适的机器连接"></a>根据配置的loadbalance策略，选择合适的机器连接</h4><h4 id="将文件的insert语句通过上步连接发送给远程机器执行"><a href="#将文件的insert语句通过上步连接发送给远程机器执行" class="headerlink" title="将文件的insert语句通过上步连接发送给远程机器执行"></a>将文件的insert语句通过上步连接发送给远程机器执行</h4><h4 id="执行成功后删除对应文件"><a href="#执行成功后删除对应文件" class="headerlink" title="执行成功后删除对应文件"></a>执行成功后删除对应文件</h4><h3 id="本地表写入"><a href="#本地表写入" class="headerlink" title="本地表写入"></a>本地表写入</h3><blockquote><p>本地表一般是StorageReplicatedXXMergeTree，其写入过程如下：<br><img src="/images/clickhouse/insert/3.png" alt="clickhouse"><br>本地表是以block为最小单元单次写入，一个block中的数据可能是一次insert的全部数据，也可以是部分数据。</p></blockquote><h4 id="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"><a href="#检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。" class="headerlink" title="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"></a>检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。</h4><h4 id="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"><a href="#检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。" class="headerlink" title="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"></a>检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。</h4><h4 id="将写入block数据按partition-by-逻辑分成多个block"><a href="#将写入block数据按partition-by-逻辑分成多个block" class="headerlink" title="将写入block数据按partition by 逻辑分成多个block"></a>将写入block数据按partition by 逻辑分成多个block</h4><h4 id="依次将每个分区的block数据写入表中"><a href="#依次将每个分区的block数据写入表中" class="headerlink" title="依次将每个分区的block数据写入表中"></a>依次将每个分区的block数据写入表中</h4><h4 id="创建分区的临时part"><a href="#创建分区的临时part" class="headerlink" title="创建分区的临时part"></a>创建分区的临时part</h4><h4 id="计算part数据的sha1生成part对应的blockid"><a href="#计算part数据的sha1生成part对应的blockid" class="headerlink" title="计算part数据的sha1生成part对应的blockid"></a>计算part数据的sha1生成part对应的blockid</h4><h4 id="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"><a href="#检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。" class="headerlink" title="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"></a>检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。</h4><h4 id="将part信息发布到ck，通知其他副本拉去新添加part"><a href="#将part信息发布到ck，通知其他副本拉去新添加part" class="headerlink" title="将part信息发布到ck，通知其他副本拉去新添加part"></a>将part信息发布到ck，通知其他副本拉去新添加part</h4><h4 id="将临时part加入到commit到mergetree表"><a href="#将临时part加入到commit到mergetree表" class="headerlink" title="将临时part加入到commit到mergetree表"></a>将临时part加入到commit到mergetree表</h4><h4 id="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"><a href="#如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件" class="headerlink" title="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"></a>如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件</h4><h4 id="临时part创建过程"><a href="#临时part创建过程" class="headerlink" title="临时part创建过程"></a>临时part创建过程</h4><ul><li>创建block数据对应分区的临时part对象</li><li>计算分区min_max索引</li><li>处理排序和主键索引</li><li>ttl处理</li><li>其他二级索引处理</li><li>将处理过的block数据和索引写入临时part，根据配置的压缩方式压缩</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;请求处理&quot;&gt;&lt;a href=&quot;#请求处理&quot; class=&quot;headerlink&quot; title=&quot;请求处理&quot;&gt;&lt;/a&gt;请求处理&lt;/h3&gt;&lt;p&gt;ck请求处理过程&lt;br&gt;&lt;img src=&quot;/images/clickhouse/insert/1.png&quot; alt=&quot;cl
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse物化视图在滴滴的实践</title>
    <link href="http://yoursite.com/2022/04/01/clickhouse-materialized-view/"/>
    <id>http://yoursite.com/2022/04/01/clickhouse-materialized-view/</id>
    <published>2022-04-01T06:57:11.000Z</published>
    <updated>2022-08-22T10:09:00.256Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是物化视图"><a href="#什么是物化视图" class="headerlink" title="什么是物化视图"></a>什么是物化视图</h3><ul><li>普通视图（View）是从一张或者多张数据库表查询导出的虚拟表，可以反映出基础表之间的数据变化，但是本身是不存储数据的，每次的查询都会从基础表重新聚合出查询结果，所以普通视图查询其实等同于创建视图时的查询语句的查询效率。</li><li>物化视图（Materialized View）是查询结果集的一份持久化存储，也称为底表的快照（snapshot），查询结果集的范围很宽泛，可以是基础表中部分数据的一份简单拷贝，也可以是多表join之后产生的结果或其子集，或者原始数据的聚合指标等等。物化视图不会随着基础表的变化而变化，如果要更新数据的话，需要用户手动进行，如周期性执行SQL，或利用触发器等机制。</li></ul><p><img src="/images/clickhouse/crud/5.png" alt="avatar"></p><h3 id="如何创建物化视图"><a href="#如何创建物化视图" class="headerlink" title="如何创建物化视图"></a>如何创建物化视图</h3><blockquote><h4 id="MergeTree排序引擎"><a href="#MergeTree排序引擎" class="headerlink" title="MergeTree排序引擎"></a>MergeTree排序引擎</h4></blockquote><h4 id="ReplacingMergeTree去重引擎"><a href="#ReplacingMergeTree去重引擎" class="headerlink" title="ReplacingMergeTree去重引擎"></a>ReplacingMergeTree去重引擎</h4><h4 id="AggregatingMergeTree聚合引擎"><a href="#AggregatingMergeTree聚合引擎" class="headerlink" title="AggregatingMergeTree聚合引擎"></a>AggregatingMergeTree聚合引擎</h4><h4 id="UniqueMergeTree-实时去重引擎"><a href="#UniqueMergeTree-实时去重引擎" class="headerlink" title="UniqueMergeTree 实时去重引擎"></a>UniqueMergeTree 实时去重引擎</h4><h4 id="SummingMergeTree-求和引擎"><a href="#SummingMergeTree-求和引擎" class="headerlink" title="SummingMergeTree 求和引擎"></a>SummingMergeTree 求和引擎</h4><h4 id="CollapsingMergeTree-折叠引擎"><a href="#CollapsingMergeTree-折叠引擎" class="headerlink" title="CollapsingMergeTree 折叠引擎"></a>CollapsingMergeTree 折叠引擎</h4><h4 id="VersionedCollapsingMergeTree-版本折叠引擎"><a href="#VersionedCollapsingMergeTree-版本折叠引擎" class="headerlink" title="VersionedCollapsingMergeTree 版本折叠引擎"></a>VersionedCollapsingMergeTree 版本折叠引擎</h4><h3 id="如何写入物化视图"><a href="#如何写入物化视图" class="headerlink" title="如何写入物化视图"></a>如何写入物化视图</h3><h4 id="底表触发"><a href="#底表触发" class="headerlink" title="底表触发"></a>底表触发</h4><h4 id="同步任务触发"><a href="#同步任务触发" class="headerlink" title="同步任务触发"></a>同步任务触发</h4><h4 id="直接写入到分布式表"><a href="#直接写入到分布式表" class="headerlink" title="直接写入到分布式表"></a>直接写入到分布式表</h4><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;什么是物化视图&quot;&gt;&lt;a href=&quot;#什么是物化视图&quot; class=&quot;headerlink&quot; title=&quot;什么是物化视图&quot;&gt;&lt;/a&gt;什么是物化视图&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;普通视图（View）是从一张或者多张数据库表查询导出的虚拟表，可以反映出基础表之间的数据
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>alibaba.druid配置</title>
    <link href="http://yoursite.com/2022/03/21/java-druid-config/"/>
    <id>http://yoursite.com/2022/03/21/java-druid-config/</id>
    <published>2022-03-21T07:09:55.000Z</published>
    <updated>2022-08-22T10:09:00.259Z</updated>
    
    <content type="html"><![CDATA[<h4 id="两种思路"><a href="#两种思路" class="headerlink" title="两种思路"></a>两种思路</h4><ul><li>后端服务主动淘汰空闲超时120s的连接，然后再主动创建连接。</li><li>后端服务对空闲连接保活，周期小于120秒进行一次mysql.ping或select 1操作。（确认过dba两个操作均可使dbproxy保活连接）</li></ul><h4 id="hikari连接池能否支持上述方案："><a href="#hikari连接池能否支持上述方案：" class="headerlink" title="hikari连接池能否支持上述方案："></a>hikari连接池能否支持上述方案：</h4><ul><li>idleTimeout：设置最大空闲时间小于120秒，空闲超时主动淘汰链接，并且hikari会自动创建新连接填充。实际发现该参数不能解决此问题，因为该参数优先级低于minimumIdle，若池中已经保持低于minimumIdle的连接数就不会再进行空闲连接淘汰。所以该参数不能解决问题。</li><li>maxLifetime：设置最大生存时间小于120秒，生存超时主动淘汰连接，并且hikari会自动创建新连接填充。能够解决该问题，但是会每120s就要淘汰且新建连接。</li></ul><h4 id="druid连接池能否支持上述方案："><a href="#druid连接池能否支持上述方案：" class="headerlink" title="druid连接池能否支持上述方案："></a>druid连接池能否支持上述方案：</h4><ul><li>testWhileIdle：设置true开启周期性健康检查 会主动淘汰掉失效的连接。</li><li>maxEvictableIdleTimeMillis：最大空闲时间设置小于120s，该参数不受minIdle参数约束，只要超过最大空闲时间就会自动淘汰连接。<br>看似上面两个参数均能解决，但是由于durid默认不会主动创建新链接，所以上面两个参数使连接清空后 查询时仍旧会因创建连接而耗时增加。</li><li>keepAlive：设置true开启保活机制，可解决该问题</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">druid:</span><br><span class="line"># 初始化大小</span><br><span class="line">initialSize: 10</span><br><span class="line"># 最大连接数</span><br><span class="line">maxActive: 30</span><br><span class="line"># 最小空闲数（周期性清除时保留的最小连接数）</span><br><span class="line">minIdle: 10</span><br><span class="line"># 最大空闲数（返还连接时若超过最大空闲连接数就丢弃）</span><br><span class="line">maxIdle: 20</span><br><span class="line"># 有效性校验</span><br><span class="line">validationQuery: select 1</span><br><span class="line"># 周期性check空闲连接</span><br><span class="line">testWhileIdle: true</span><br><span class="line"># 借出时check（影响性能）</span><br><span class="line">testOnBorrow: false</span><br><span class="line"># 返还时check（影响性能）</span><br><span class="line">testOnReturn: false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 周期性检查的间隔时间，单位是毫秒</span><br><span class="line">timeBetweenEvictionRunsMillis: 60000</span><br><span class="line"># 连接的最小空闲时间，小于该时间不会被周期性check清除。单位是毫秒</span><br><span class="line">minEvictableIdleTimeMillis: 50000</span><br><span class="line"># 连接的最大空闲时间，大于该时间会被周期性check清除，且优先级大于minIdle。单位是毫秒</span><br><span class="line">maxEvictableIdleTimeMillis: 100000</span><br><span class="line"># 开启连接保活策略。作用：https:&#x2F;&#x2F;www.bookstack.cn&#x2F;read&#x2F;Druid&#x2F;d90f9643acdca5c0.md</span><br><span class="line">keepAlive: true</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;两种思路&quot;&gt;&lt;a href=&quot;#两种思路&quot; class=&quot;headerlink&quot; title=&quot;两种思路&quot;&gt;&lt;/a&gt;两种思路&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;后端服务主动淘汰空闲超时120s的连接，然后再主动创建连接。&lt;/li&gt;
&lt;li&gt;后端服务对空闲连接保活，周期小于
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="druid" scheme="http://yoursite.com/tags/druid/"/>
    
  </entry>
  
  <entry>
    <title>为什么需要关闭连接</title>
    <link href="http://yoursite.com/2022/03/21/java-why-need-close-connection/"/>
    <id>http://yoursite.com/2022/03/21/java-why-need-close-connection/</id>
    <published>2022-03-21T07:09:55.000Z</published>
    <updated>2022-08-22T10:09:00.259Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://stackoverflow.com/questions/25864235/why-we-should-close-the-connection-in-jdbc-if-we-dont-do-it-what-will-happen" target="_blank" rel="noopener">链接</a></p><h3 id="TCP标志"><a href="#TCP标志" class="headerlink" title="TCP标志"></a>TCP标志</h3><p>SYN: (同步序列编号,Synchronize Sequence Numbers)该标志仅在三次握手建立TCP连接时有效。表示一个新的TCP连接请求。</p><p>ACK: (确认编号,Acknowledgement Number)是对TCP请求的确认标志,同时提示对端系统已经成功接收所有数据。</p><p>FIN: (结束标志,FINish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据。</p><h3 id="TCP状态"><a href="#TCP状态" class="headerlink" title="TCP状态"></a>TCP状态</h3><blockquote><p>TCP状态在系统里都有对应的解释或设置,可见<code>man tcp</code>，以下介绍主要常见的几种</p></blockquote><p>1)、LISTEN: 首先服务端需要打开一个socket进行监听，状态为LISTEN.<br>/<em> The socket is listening for incoming connections. 侦听来自远方TCP端口的连接请求 </em>/</p><p>2)、SYN_SENT: 客户端通过应用程序调用connect进行active open.于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT.<br>/<em>The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求 </em>/</p><p>3)、SYN_RECV: 服务端应发出ACK确认客户端的SYN,同时自己向客户端发送一个SYN. 之后状态置为SYN_RECV<br>/<em> A connection request has been received from the network. 在收到和发送一个连接请求后等待对连接请求的确认 </em>/</p><p>4)、ESTABLISHED: 代表一个打开的连接，双方可以进行或已经在数据交互了。<br>/<em> The socket has an established connection. 代表一个打开的连接，数据可以传送给用户 </em>/</p><p>5)、FIN_WAIT1:主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态.<br>/<em> The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 </em>/</p><p>6)、CLOSE_WAIT: 被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT.<br>/<em> The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求 </em>/</p><p>7)、FIN_WAIT2: 主动关闭端接到ACK后，就进入了FIN-WAIT-2 .<br>/<em> Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求 </em>/</p><p>8)、LAST_ACK: 被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接。这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK .<br>/<em> The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认 </em>/</p><p>9)、TIME_WAIT: 只发生在主动关闭连接的一方。主动关闭方在接收到被动关闭方的FIN请求后，发送成功给对方一个ACK后,将自己的状态由FIN_WAIT2修改为TIME_WAIT，而必须再等2倍 的MSL(Maximum Segment Lifetime,MSL是一个数据报在internetwork中能存在的时间)时间之后双方才能把状态 都改为CLOSED以关闭连接。目前RHEL里保持TIME_WAIT状态的时间为60秒。<br>/<em> The socket is waiting after close to handle packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认 </em>/</p><p>10)、CLOSING: 比较少见.<br>/<em> Both sockets are shut down but we still don’t have all our data sent. 等待远程TCP对连接中断的确认 </em>/</p><p>11)、CLOSED: 被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束.<br>/<em> The socket is not being used. 没有任何连接状态 </em>/</p><h3 id="为什么创建的连接需要关闭？"><a href="#为什么创建的连接需要关闭？" class="headerlink" title="为什么创建的连接需要关闭？"></a>为什么创建的连接需要关闭？</h3><p>如果不关闭连接，这些系统资源将一直在内存中，等待下一次系统gc时才会被回收，而如果系统频繁的fgc将会导致你的程序明显卡顿，如果一直没有gc，也会导致你的内存泄漏</p><h3 id="如果一定要关闭，那是不是可以使用单例模式在所有地方使用？"><a href="#如果一定要关闭，那是不是可以使用单例模式在所有地方使用？" class="headerlink" title="如果一定要关闭，那是不是可以使用单例模式在所有地方使用？"></a>如果一定要关闭，那是不是可以使用单例模式在所有地方使用？</h3><h3 id="是不是可以使用连接池，一次创建n个连接，用完归还到池子里等下次再被使用？"><a href="#是不是可以使用连接池，一次创建n个连接，用完归还到池子里等下次再被使用？" class="headerlink" title="是不是可以使用连接池，一次创建n个连接，用完归还到池子里等下次再被使用？"></a>是不是可以使用连接池，一次创建n个连接，用完归还到池子里等下次再被使用？</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/25864235/why-we-should-close-the-connection-in-jdbc-if-we-dont-do-it-what-will-happen&quot; targe
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>使用clickhouse实现索引</title>
    <link href="http://yoursite.com/2022/02/21/clickhouse-index/"/>
    <id>http://yoursite.com/2022/02/21/clickhouse-index/</id>
    <published>2022-02-21T06:57:11.000Z</published>
    <updated>2022-08-22T10:09:00.256Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一级索引"><a href="#一级索引" class="headerlink" title="一级索引"></a>一级索引</h3><h4 id="index-granularity"><a href="#index-granularity" class="headerlink" title="index_granularity"></a>index_granularity</h4><p><img src="/images/clickhouse/crud/5.png" alt="avatar"></p><h3 id="二级索引"><a href="#二级索引" class="headerlink" title="二级索引"></a>二级索引</h3><h4 id="granularity"><a href="#granularity" class="headerlink" title="granularity"></a>granularity</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一级索引&quot;&gt;&lt;a href=&quot;#一级索引&quot; class=&quot;headerlink&quot; title=&quot;一级索引&quot;&gt;&lt;/a&gt;一级索引&lt;/h3&gt;&lt;h4 id=&quot;index-granularity&quot;&gt;&lt;a href=&quot;#index-granularity&quot; class=&quot;he
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>聊聊linux下的软硬链接</title>
    <link href="http://yoursite.com/2022/02/16/linux-ln/"/>
    <id>http://yoursite.com/2022/02/16/linux-ln/</id>
    <published>2022-02-16T07:48:18.000Z</published>
    <updated>2022-08-22T10:09:00.260Z</updated>
    
    <content type="html"><![CDATA[<h3 id="链接概念"><a href="#链接概念" class="headerlink" title="链接概念"></a>链接概念</h3><p>Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。<br>默认情况下，ln命令产生硬链接。</p><h3 id="硬连接"><a href="#硬连接" class="headerlink" title="硬连接"></a>硬连接</h3><ul><li>硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在Linux中，多个文件名指向同一索引节点是存在的。一般这种连接就是硬连接。</li><li>硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。</li></ul><h3 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h3><p>另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#创建一个测试文件f1</span><br><span class="line">[oracle@Linux]$ touch f1</span><br><span class="line"></span><br><span class="line">#创建f1的一个硬连接文件f2</span><br><span class="line">[oracle@Linux]$ ln f1 f2</span><br><span class="line"></span><br><span class="line">#创建f1的一个符号连接文件f3</span><br><span class="line">[oracle@Linux]$ ln -s f1 f3</span><br><span class="line"></span><br><span class="line"># -i参数显示文件的inode节点信息</span><br><span class="line">[oracle@Linux]$ ls -li   </span><br><span class="line">total 0</span><br><span class="line">9797648 -rw-r--r--  2 oracle oinstall 0 Apr 21 08:11 f1</span><br><span class="line">9797648 -rw-r--r--  2 oracle oinstall 0 Apr 21 08:11 f2</span><br><span class="line">9797649 lrwxrwxrwx  1 oracle oinstall 2 Apr 21 08:11 f3 -&gt; f1</span><br></pre></td></tr></table></figure><p>从上面的结果中可以看出，硬连接文件f2与原文件f1的inode节点相同，均为9797648，然而符号连接文件的inode节点不同。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 修改文件内容</span><br><span class="line">[oracle@Linux]$ echo &quot;I am f1 file&quot; &gt;&gt;f1</span><br><span class="line"></span><br><span class="line"># 打印f1文件内容</span><br><span class="line">[oracle@Linux]$ cat f1</span><br><span class="line">I am f1 file</span><br><span class="line"></span><br><span class="line"># 打印f2文件内容</span><br><span class="line">[oracle@Linux]$ cat f2</span><br><span class="line">I am f1 file</span><br><span class="line"></span><br><span class="line"># 打印f3文件内容</span><br><span class="line">[oracle@Linux]$ cat f3</span><br><span class="line">I am f1 file</span><br><span class="line"># 删除f1</span><br><span class="line">[oracle@Linux]$ rm -f f1</span><br><span class="line"></span><br><span class="line"># 打印f2文件内容，未受到影响，所以硬链接未受源文件删除的影响</span><br><span class="line">[oracle@Linux]$ cat f2</span><br><span class="line">I am f1 file</span><br><span class="line"></span><br><span class="line"># 打印f3文件内容，提示找不到该文件或者目录，所以软连接会受到源文件删除的影响</span><br><span class="line">[oracle@Linux]$ cat f3</span><br><span class="line">cat: f3: No such file or directory</span><br></pre></td></tr></table></figure><p>通过上面的测试可以看出：当删除原始文件f1后，硬连接f2不受影响，但是符号连接f1文件无效</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>1).删除符号连接f3,对f1,f2无影响；<br>2).删除硬连接f2，对f1,f3也无影响；<br>3).删除原文件f1，对硬连接f2没有影响，导致符号连接f3失效；<br>4).同时删除原文件f1,硬连接f2，整个文件会真正的被删除。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;链接概念&quot;&gt;&lt;a href=&quot;#链接概念&quot; class=&quot;headerlink&quot; title=&quot;链接概念&quot;&gt;&lt;/a&gt;链接概念&lt;/h3&gt;&lt;p&gt;Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。&lt;br&gt;默
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>由于overlay占用磁盘空间导致磁盘告警</title>
    <link href="http://yoursite.com/2022/02/15/linux-overlay-full/"/>
    <id>http://yoursite.com/2022/02/15/linux-overlay-full/</id>
    <published>2022-02-15T07:09:08.000Z</published>
    <updated>2022-08-22T10:09:00.260Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>早上突然收到告警短信，xxx服务磁盘使用率超过80%，如下图，赶紧上机器进行排查，下面是排查的过程<br><img src="/images/linux/disk_full/1.png" alt="avatar"></p><h3 id="查看机器整体磁盘使用情况"><a href="#查看机器整体磁盘使用情况" class="headerlink" title="查看机器整体磁盘使用情况"></a>查看机器整体磁盘使用情况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 下面是结果</span><br><span class="line">root@xxx.docker.ys:~&#x2F;dlap-manager$ df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">overlay          20G   18G  2.7G  87% &#x2F;</span><br><span class="line">tmpfs            64M     0   64M   0% &#x2F;dev</span><br><span class="line">tmpfs            63G     0   63G   0% &#x2F;sys&#x2F;fs&#x2F;cgroup</span><br><span class="line">&#x2F;dev&#x2F;sda3       219G   15G  193G   8% &#x2F;etc&#x2F;hosts</span><br><span class="line">shm              64M     0   64M   0% &#x2F;dev&#x2F;shm</span><br><span class="line">&#x2F;dev&#x2F;sdb1       4.4T  949G  3.5T  22% &#x2F;etc&#x2F;hostname</span><br><span class="line">tmpfs            63G   12K   63G   1% &#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount</span><br><span class="line">tmpfs            63G     0   63G   0% &#x2F;proc&#x2F;acpi</span><br><span class="line">tmpfs            63G     0   63G   0% &#x2F;proc&#x2F;scsi</span><br><span class="line">tmpfs            63G     0   63G   0% &#x2F;sys&#x2F;firmware</span><br></pre></td></tr></table></figure><p>看到overlay这个文件目录占了<code>87%</code>，仔细研究下这个文件夹是什么</p><h4 id="overlay介绍"><a href="#overlay介绍" class="headerlink" title="overlay介绍"></a>overlay介绍</h4><p>OverlayFS是一种堆叠文件系统，它依赖并建立在其它的文件系统之上（例如ext4fs和xfs等等），并不直接参与磁盘空间结构的划分，仅仅将原来底层文件系统中不同的目录进行“合并”，然后向用户呈现，这也就是联合挂载技术，对比于AUFS，OverlayFS速度更快，实现更简单。 而Linux 内核为Docker提供的OverlayFS驱动有两种：overlay和overlay2。而overlay2是相对于overlay的一种改进，在inode利用率方面比overlay更有效。但是overlay有环境需求：docker版本17.06.02+，宿主机文件系统需要是ext4或xfs格式。<br>overlayfs通过三个目录：lower目录、upper目录、以及work目录实现，其中lower目录可以是多个，work目录为工作基础目录，挂载后内容会被清空，且在使用过程中其内容用户不可见，最后联合挂载完成给用户呈现的统一视图称为为merged目录<br><img src="/images/linux/disk_full/2.png" alt="avatar"><br>在上述图中可以看到三个层结构，即：lowerdir、uperdir、merged，其中lowerdir是只读的image layer，其实就是rootfs，对应的lowerdir是可以有多个目录。而upperdir则是在lowerdir之上的一层，这层是读写层，在启动一个容器时候会进行创建，所有的对容器数据更改都发生在这里层。最后merged目录是容器的挂载点，也就是给用户暴露的统一视角。</p><h4 id="overlay如何工作"><a href="#overlay如何工作" class="headerlink" title="overlay如何工作"></a>overlay如何工作</h4><p>当容器中发生数据修改时候overlayfs存储驱动又是如何进行工作的？以下将阐述其读写过程：</p><ul><li><ol><li>读：</li></ol><ul><li>如果文件在容器层（upperdir），直接读取文件；</li><li>如果文件不在容器层（upperdir），则从镜像层（lowerdir）读取；</li></ul></li><li><ol><li>写：</li></ol><ul><li>首次写入： 如果在upperdir中不存在，overlay和overlay2执行copy_up操作，把文件从lowdir拷贝到upperdir，由于overlayfs是文件级别的（即使文件只有很少的一点修改，也会产生的copy_up的行为），后续对同一文件的在此写入操作将对已经复制到容器的文件的副本进行操作。这也就是常常说的写时复制（copy-on-write）</li><li>删除文件和目录： 当文件在容器被删除时，在容器层（upperdir）创建whiteout文件，镜像层(lowerdir)的文件是不会被删除的，因为他们是只读的，但without文件会阻止他们显示，当目录在容器内被删除时，在容器层（upperdir）一个不透明的目录，这个和上面whiteout原理一样，阻止用户继续访问，即便镜像层仍然存在。 </li></ul></li><li><ol><li>注意事项</li></ol><ul><li>copy_up操作只发生在文件首次写入，以后都是只修改副本,</li><li>overlayfs只适用两层目录，相比于比AUFS，查找搜索都更快。</li><li>容器层的文件删除只是一个“障眼法”，是靠whiteout文件将其遮挡，image层并没有删除，这也就是为什么使用docker commit 提交保存的镜像会越来越大，无论在容器层怎么删除数据，image层都不会改变。</li></ul></li></ul><h4 id="overlay镜像存储结构"><a href="#overlay镜像存储结构" class="headerlink" title="overlay镜像存储结构"></a>overlay镜像存储结构</h4><p>从仓库拉取ubuntu镜像，结果显示总共拉取了6层镜像如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bigdata-xxx.ys ~]$ sudo docker pull ubuntu-test:xenial</span><br><span class="line">Trying to pull repository ubuntu-test ...</span><br><span class="line">xenial: Pulling from ubuntu-test</span><br><span class="line">58690f9b18fc: Pull complete</span><br><span class="line">b51569e7c507: Pull complete</span><br><span class="line">da8ef40b9eca: Pull complete</span><br><span class="line">fb15d46c38dc: Pull complete</span><br><span class="line">4c7a0de79adc: Pull complete</span><br><span class="line">5eff12cba838: Pull complete</span><br><span class="line">Digest: sha256:d21c70f1203a5b0fe1d8a1b60bd1924ca5458ba450828f6b88c4e973db84c8e8</span><br><span class="line">Status: Downloaded newer image for ubuntu-test:xenial</span><br></pre></td></tr></table></figure><br>此时6层镜像被存储在/var/lib/docker/overlay2下，将镜像运行起来一个容器，如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -itd --name ubuntu-test ubuntu-test:xenial</span><br></pre></td></tr></table></figure><br>运行容器之后，查询出容器的元数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 获取容器ID</span><br><span class="line">sudo docker ps -a | grep ubuntu</span><br><span class="line">&#x2F;&#x2F; 通过容器ID查看元数据</span><br><span class="line">sudo docker inspect container-id</span><br></pre></td></tr></table></figure><br><img src="/images/linux/disk_full/4.png" alt="avatar"></p><p>进入容器创建一个文件，如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker exec -it ubuntu-test bash</span><br><span class="line">echo &#39;xxxxxx&#39; &gt; helloworld.txt</span><br></pre></td></tr></table></figure></p><p>通过tree命令查看新创建的文件出现在哪里<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree -L 3 &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;f90282cedadf6b7765ba06ea8983af306f78773baeca9ff1e418651d0b9d14f2&#x2F;diff</span><br></pre></td></tr></table></figure><br><img src="/images/linux/disk_full/3.png" alt="avatar"></p><h3 id="回到问题本身"><a href="#回到问题本身" class="headerlink" title="回到问题本身"></a>回到问题本身</h3><p>overlay目录使用了87%的存储，触发了磁盘告警，overlay目录是我们的弹性云容器运行的目录，发现通过nohup启动的jar包将所有stdout/stderr都输出到了nohup.out文件，该文件也持久化存在/var/lib/docker/overlay2下，所以需要对nohup.out进行处理，不可直接删除，因为jar启动后所有的输出流都会转存到nohup.out，这个文件句柄已经打开，所有的stdout/stderr仍然会输出，只是输出在/proc/pid/fd/1或者/proc/pid/fd/2这些目录下。</p><blockquote><p>0描述符 标准输入<br>1描述符 标注输出<br>2描述符 标注错误输出  </p></blockquote><p><strong>linux一切到可以看作文件</strong>，/proc/pid/fd/1 就是pid进程的标准输出，/proc/pid/fd/1 就是pid进程的标准错误输出，我们通过测试可以看到下面结果，就算我们删除了nohup.out文件，任何会将stdout/stderro输出到这个文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@xxx.ys ~]$ tree -L 3 &#x2F;proc&#x2F;143645&#x2F;fd</span><br><span class="line">&#x2F;proc&#x2F;143645&#x2F;fd</span><br><span class="line">├── 0 -&gt; &#x2F;dev&#x2F;null</span><br><span class="line">├── 1 -&gt; &#x2F;home&#x2F;hadoop&#x2F;nohup.out\ (deleted)</span><br><span class="line">├── 2 -&gt; &#x2F;home&#x2F;hadoop&#x2F;nohup.out\ (deleted)</span><br><span class="line">└── 3 -&gt; &#x2F;home&#x2F;hadoop&#x2F;test.txt</span><br><span class="line"></span><br><span class="line">0 directories, 4 files</span><br></pre></td></tr></table></figure></p><ul><li><ol><li>重启jar包，将所有错误/标准输出忽略<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; &gt;&#x2F;dev&#x2F;null是表示linux的空设备，是一个特殊的文件，写入到它的内容都会被丢弃(等同于 1&gt;&#x2F;dev&#x2F;null)</span><br><span class="line">&#x2F;&#x2F; 2&gt;&amp;1表示所有错误输出都重定向到标准输出</span><br><span class="line">&#x2F;&#x2F; 所以就是将错误输出重定向到标准输出，将标准输出重定向到空设备，就是禁止所有输出&#x2F;错误</span><br><span class="line">nohup java -jar xx.jar &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>删除nohup.out文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -f nohup.out</span><br></pre></td></tr></table></figure></li></ol></li></ul><h3 id="修改docker的根目录"><a href="#修改docker的根目录" class="headerlink" title="修改docker的根目录"></a>修改docker的根目录</h3><h4 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h4><ol><li>停止docker服务<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop docker</span><br></pre></td></tr></table></figure></li><li><p>创建新的docker工作目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;data2&#x2F;docker</span><br></pre></td></tr></table></figure><p>这个目录可以自定义，但是一定要保证在/root里面</p></li><li><p>迁移/var/lib/docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -avz &#x2F;var&#x2F;lib&#x2F;docker &#x2F;data2&#x2F;docker&#x2F;</span><br></pre></td></tr></table></figure></li><li>配置devicemapper.conf<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 不存在就创建</span><br><span class="line">sudo mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;</span><br><span class="line"># 不存在就创建</span><br><span class="line">sudo vi &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;devicemapper.conf</span><br></pre></td></tr></table></figure></li><li><p>在devicemapper.conf中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">ExecStart&#x3D;</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd  --graph&#x3D;&#x2F;data2&#x2F;docker</span><br></pre></td></tr></table></figure></li><li><p>重启docker服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line"> </span><br><span class="line">systemctl restart docker</span><br><span class="line"> </span><br><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure></li><li><p>确认是否配置成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure><p><img src="/images/linux/disk_full/5.png" alt="avatar"></p></li></ol><p>重新启动所有容器后，确认无误。即可删除/var/lib/docker里面所有文件。</p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>如果报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error response from daemon: Cannot restart container linyu: shim error: docker-runc not installed on system</span><br></pre></td></tr></table></figure></p><ul><li><ol><li>判断是否安装runc<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qi runc</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>未安装则先安装<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install runc</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>创建软连接<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;libexec&#x2F;docker&#x2F;</span><br><span class="line">sudo ln -s docker-runc-current docker-runc</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>重启服务<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker restart container_id</span><br></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;早上突然收到告警短信，xxx服务磁盘使用率超过80%，如下图，赶紧上机器进行排查，下面是排查的过程&lt;br&gt;&lt;img src=&quot;/images
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>使用clickhouse实现数据更新和删除</title>
    <link href="http://yoursite.com/2022/02/03/clickhouse-crud/"/>
    <id>http://yoursite.com/2022/02/03/clickhouse-crud/</id>
    <published>2022-02-03T14:57:11.000Z</published>
    <updated>2022-08-22T10:09:00.255Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是CRUD"><a href="#什么是CRUD" class="headerlink" title="什么是CRUD"></a>什么是CRUD</h3><blockquote><p><code>CRUD</code>是指在做计算处理时的增加(<code>Create</code>)、检索(<code>Retrieve</code>)、更新(<code>Update</code>)和删除(<code>Delete</code>)几个单词的首字母简写。<code>crud</code>主要被用在描述软件系统中数据库或者持久层的基本操作功能</p></blockquote><h3 id="clickhouse物理上的crud"><a href="#clickhouse物理上的crud" class="headerlink" title="clickhouse物理上的crud"></a>clickhouse物理上的crud</h3><ul><li><ol><li>create创建表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table account(</span><br><span class="line">  time DateTime default now() comment &#39;创建时间&#39;,</span><br><span class="line">  name String default &#39;&#39; comment &#39;唯一标示&#39;,</span><br><span class="line">  alias String default &#39;&#39; comment &#39;别名&#39;,</span><br><span class="line">  age UInt64 default 0 comment &#39;年龄&#39;,</span><br><span class="line">  version UInt64 default 0 comment &#39;版本号&#39;,</span><br><span class="line">  is_delete UInt8 default 0 comment &#39;是否删除，0否，1是&#39;</span><br><span class="line">) </span><br><span class="line">engine &#x3D; MergeTree </span><br><span class="line">partition by toYYYYMMDD(time) </span><br><span class="line">order by name</span><br></pre></td></tr></table></figure><img src="/images/clickhouse/crud/5.png" alt="avatar"></li></ol></li><li><ol><li>insert写入数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into default.account(time, name, alias, age, version, is_delete) values (&#39;2022-01-01 11:11:11&#39;, &#39;blanklin&#39;, &#39;superhero&#39;, 20, 1, 0)</span><br></pre></td></tr></table></figure><blockquote><p>写入数据后会按照partitionby生成对应的分区part目录<br><img src="/images/clickhouse/crud/6.png" alt="avatar"></p></blockquote></li></ol></li><li><ol><li>update更新<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alter table default.account</span><br><span class="line">update alias &#x3D; &#39;super_hero&#39;</span><br><span class="line">where alias &#x3D; &#39;superhero&#39;</span><br></pre></td></tr></table></figure><blockquote><p>这里是 mutation 操作,会生成一个mutation_version.txt<br><img src="/images/clickhouse/crud/4.png" alt="avatar"></p></blockquote></li></ol></li><li><ol><li>delete删除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table delete where id &#x3D; 1</span><br></pre></td></tr></table></figure><blockquote><p>这里是 mutation 操作,会生成一个mutation_version.txt<br><img src="/images/clickhouse/crud/7.png" alt="avatar"></p></blockquote></li></ol></li><li><ol><li>retrieve检索<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select time, name, alias, age, version, is_delete from account where is_delete &#x3D; 0 order by version desc</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>可以通过system.mutations查询执行计划<br><img src="/images/clickhouse/crud/9.png" alt="avatar"><br>当mutation操作执行完成后，system.mutations表中对应的mutation记录中is_done字段的值会变为1。</li></ol></li><li><ol><li>可以通过system.parts查询执行结果<br><img src="/images/clickhouse/crud/8.png" alt="avatar"><br>当旧的数据片段移除后，system.parts表中旧数据片段对应的记录会被移除。</li></ol></li></ul><blockquote><p>可以看到mutation操作完成后，之前的目录已经被删除<br><img src="/images/clickhouse/crud/10.png" alt="avatar"></p></blockquote><h3 id="clickhouse的mutation是什么"><a href="#clickhouse的mutation是什么" class="headerlink" title="clickhouse的mutation是什么"></a>clickhouse的mutation是什么</h3><h4 id="官方文档解释"><a href="#官方文档解释" class="headerlink" title="官方文档解释"></a>官方文档解释</h4><p>从官方对于mutaiton的解释<a href="https://clickhouse.com/docs/zh/sql-reference/statements/alter/#alter-mutations" target="_blank" rel="noopener">链接</a>中，我们需要注意到几个关键词，如下</p><ul><li><ol><li>manipulate table data<br>操作表数据</li></ol></li><li><ol><li>asynchronous background processes<br>异步后台处理</li></ol></li><li><ol><li>rewriting whole data parts<br>重写全部数据part</li></ol></li><li><ol><li>a SELECT query that started executing during a mutation will see data from parts that have already been mutated along with data from parts that have not been mutated yet<br>在突变期间的查询语句，将看到已经完成突变的数据part和还未发生突变的part</li></ol></li><li><ol><li>data that was inserted into the table before the mutation was submitted will be mutated and data that was inserted after that will not be mutated<br>在提交突变之前插入表中的数据将被突变，而在此之后插入的数据将不会被突变</li></ol></li><li><ol><li>There is no way to roll back the mutation once it is submitted, but if the mutation is stuck for some reason it can be cancelled with the KILL MUTATION query<br>突变一旦被提交就没有方式可以回滚，但是如果突变由于一些原因被卡住，可以使用<code>KILL MUTATION</code>取消突变</li></ol></li></ul><h4 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h4><p>当用户执行一个如上的Mutation操作获得返回时，ClickHouse内核其实只做了两件事情：<br><img src="/images/clickhouse/crud/1.png" alt="avatar"></p><ul><li><ol><li>检查Mutation操作是否合法；<blockquote><p>主体逻辑在MutationsInterpreter::validate函数</p></blockquote></li></ol></li><li><ol><li>保存Mutation命令到存储文件中，唤醒一个异步处理merge和mutation的工作线程；<blockquote><p>主体逻辑在StorageMergeTree::mutate函数中。</p></blockquote></li></ol></li></ul><p>Merge逻辑<br><strong>StorageMergeTree::merge</strong>函数是<code>MergeTree</code>异步<code>Merge</code>的核心逻辑，<code>Data Part Merge</code>的工作除了通过后台工作线程自动完成，用户还可以通过<code>Optimize</code>命令来手动触发。自动触发的场景中，系统会根据后台空闲线程的数据来启发式地决定本次<code>Merge</code>最大可以处理的数据量大小，<strong>max_bytes_to_merge_at_min_space_in_pool</strong>: 决定当空闲线程数最大时可处理的数据量上限（默认150GB）<br><strong>max_bytes_to_merge_at_max_space_in_pool</strong>: 决定只剩下一个空闲线程时可处理的数据量上限（默认1MB）<br>当用户的写入量非常大的时候，应该适当调整工作线程池的大小和这两个参数。当用户手动触发<code>merge</code>时，系统则是根据<code>disk</code>剩余容量来决定可处理的最大数据量。</p><p>Mutation逻辑<br>系统每次都只会订正一个<code>Data Part</code>，但是会聚合多个<code>mutation</code>任务批量完成，这点实现非常的棒。因为在用户真实业务场景中一次数据订正逻辑中可能会包含多个<code>Mutation</code>命令，把这多个<code>mutation</code>操作聚合到一起订正效率上就非常高。系统每次选择一个排序键最小的并且需要订正<code>Data Part</code>进行操作，本意上就是把数据从前往后进行依次订正。<br><img src="/images/clickhouse/crud/11.png" alt="avatar"><br><img src="/images/clickhouse/crud/13.png" alt="avatar"><br><img src="/images/clickhouse/crud/12.png" alt="avatar"></p><p>mutation和merge相互独立执行。看完本文前面的分析，大家应该也注意到了目前Data Part的merge和mutation是相互独立执行的，Data Part在同一时刻只能是在merge或者mutation操作中。对于MergeTree这种存储彻底Immutable的设计，数据频繁merge、mutation会引入巨大的IO负载。实时上merge和mutation操作是可以合并到一起去考虑的，这样可以省去数据一次读写盘的开销。对数据写入压力很大又有频繁mutation的场景，会有很大帮助</p><h3 id="clickhouse逻辑CRUD"><a href="#clickhouse逻辑CRUD" class="headerlink" title="clickhouse逻辑CRUD"></a>clickhouse逻辑CRUD</h3><p><a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/versionedcollapsingmergetree/" target="_blank" rel="noopener">VersionedCollapsingMergeTree介绍</a>，引擎继承自 <code>MergeTree</code> 并将折叠行的逻辑添加到合并数据部分的算法中。 <code>VersionedCollapsingMergeTree</code> 用于相同的目的 折叠树 但使用不同的折叠算法，允许以多个线程的任何顺序插入数据。 特别是， <code>Version</code> 列有助于正确折叠行，即使它们以错误的顺序插入。 相比之下, <code>CollapsingMergeTree</code> 只允许严格连续插入。</p><h4 id="创建VersionedCollapsingMergeTree表"><a href="#创建VersionedCollapsingMergeTree表" class="headerlink" title="创建VersionedCollapsingMergeTree表"></a>创建VersionedCollapsingMergeTree表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table test_version_collapsing(</span><br><span class="line">  time DateTime default now() comment &#39;创建时间&#39;,</span><br><span class="line">  name String default &#39;&#39; comment &#39;唯一标示&#39;,</span><br><span class="line">  alias String default &#39;&#39; comment &#39;别名&#39;,</span><br><span class="line">  age UInt64 default 0 comment &#39;年龄&#39;,</span><br><span class="line">  version UInt64 default 0 comment &#39;版本号&#39;,</span><br><span class="line">  sign Int8 default 0 comment &#39;是否删除，0否，1是&#39;</span><br><span class="line">) </span><br><span class="line">engine &#x3D; VersionedCollapsingMergeTree(sign, version) </span><br><span class="line">partition by toYYYYMMDD(time) </span><br><span class="line">order by name</span><br></pre></td></tr></table></figure><h4 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into default.test_version_collapsing(time, name, alias, age, version, sign) values (&#39;2022-01-01 11:11:11&#39;, &#39;blanklin&#39;, &#39;superhero&#39;, 20, 1, 1)</span><br></pre></td></tr></table></figure><h4 id="更新数据"><a href="#更新数据" class="headerlink" title="更新数据"></a>更新数据</h4><ul><li><ol><li>先找出要更新的这条数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select time, name, alias, age, version, sign from default.test_version_collapsing</span><br><span class="line">where name &#x3D; &#39;blanklin&#39; and sign &#x3D; 1</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>假设要更新alias=update_super_hero，其他值不变，将version由1.捞出的值上+1，类似以下sql<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 先将旧行标示为删除，就是将sign &#x3D; -1</span><br><span class="line">insert into default.test_version_collapsing(time, name, alias, age, version, sign) values (&#39;2022-01-01 11:11:11&#39;, &#39;blanklin&#39;, &#39;superhero&#39;, 20, 1, -1);</span><br><span class="line"></span><br><span class="line"># 再去插入新行，包含要更新的列</span><br><span class="line">insert into default.test_version_collapsing(time, name, alias, age, version, sign) values (&#39;2022-01-01 11:11:11&#39;, &#39;blanklin&#39;, &#39;update_superhero&#39;, 20, 2, 1).</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>捞出更新后的那条数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select name, argMax(age, version), argMax(alias, version) from default.test_version_collapsing group by name having sum(sign) &gt; 0;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h4><ul><li><ol><li>先找出要更新的这条数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select time, name, alias, age, version, sign from default.test_version_collapsing</span><br><span class="line">where name &#x3D; &#39;blanklin&#39; and sign &#x3D; 1</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>假设要删除alias=update_super_hero，其他值不变，将sign=1,类似以下sql<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into default.test_version_collapsing(time, name, alias, age, version, sign) values (&#39;2022-01-01 11:11:11&#39;, &#39;blanklin&#39;, &#39;update_superhero&#39;, 20, 2, -1)</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>捞出更新后的那条数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select name, argMax(age, version), argMax(alias, version) from default.test_version_collapsing group by name having sum(sign) &gt; 0;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="注意项"><a href="#注意项" class="headerlink" title="注意项"></a>注意项</h4><ul><li><ol><li>只有相同分区内的数据才能删除和更新</li></ol></li><li><ol><li>如果不使用 <strong>having sum(sign) &gt; 0</strong>的方式去查询，则可以使用<code>final</code>方式查询<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from test_version_collapsing final;</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>也可以使用<code>optimize</code>方式强制合并分区，再查询，但是这个方式可能会造成集群cpu飙高，而且<code>optimize</code>一个大表需要时间很长，效率极低<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimize table test_version_collapsing final</span><br></pre></td></tr></table></figure></li></ol></li><li><ol><li>sign必须要唯一，添加用1，则删除一定是-1，才可以被折叠处理</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;什么是CRUD&quot;&gt;&lt;a href=&quot;#什么是CRUD&quot; class=&quot;headerlink&quot; title=&quot;什么是CRUD&quot;&gt;&lt;/a&gt;什么是CRUD&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;CRUD&lt;/code&gt;是指在做计算处理时的增加(&lt;code&gt;C
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse query on cluster源码解读</title>
    <link href="http://yoursite.com/2022/01/21/clickhouse-ddl-on-cluster/"/>
    <id>http://yoursite.com/2022/01/21/clickhouse-ddl-on-cluster/</id>
    <published>2022-01-21T07:04:11.000Z</published>
    <updated>2022-08-22T10:09:00.255Z</updated>
    
    <content type="html"><![CDATA[<h3 id="分布式DDL执行链路"><a href="#分布式DDL执行链路" class="headerlink" title="分布式DDL执行链路"></a>分布式DDL执行链路</h3><blockquote><p>在介绍具体的分布式<code>DDL</code>执行链路之前，先为大家梳理一下哪些操作是可以走分布式<code>DDL</code>执行链路的，大家也可以自己在源码中查看一下<code>ASTQueryWithOnCluster</code>的继承类有哪些：<br><img src="/images/clickhouse/ddloncluster/1.png" alt="cgi">  </p></blockquote><ul><li>ASTAlterQuery：<br>包括<code>ATTACH_PARTITION</code>、<code>FETCH_PARTITION</code>、<code>FREEZE_PARTITION</code>、<code>FREEZE_ALL</code>等操作（对表的数据分区粒度进行操作）。</li><li>ASTCreateQuery：<br>包括常见的建库、建表、建视图，还有<code>ClickHouse</code>独有的<code>Attach Table</code>（可以从存储文件中直接加载一个之前卸载的数据表）。</li><li>ASTCreateQuotaQuery:<br>包括对租户的配额操作语句，例如<code>create quaota</code>，或者<code>alter quota</code>语句</li><li>ASTCreateRoleQuery：<br>包括对租户角色操作语句，例如<code>create/alter/drop/set/set default/show create role</code>语句，或者<code>show roles</code></li><li>ASTCreateRowPolicyQuery<br>对表的查询做行级别的策略限制，例如<code>create row policy</code> 或者 <code>alter row policy</code></li><li>ASTCreateSettingsProfileQuery<br>对角色或者租户的资源限制和约束，例如<code>create settings profile</code> 或者 <code>alter settings profile</code></li><li>ASTCreateUserQuery<br>对租户的操作语句，例如<code>create create user</code> 或者 <code>alter create user</code></li><li>ASTDropAccessEntityQuery<br>涉及到了clickhouse权限相关的所有删除语句，包括<code>DROP USER</code>,<code>DROP ROLE</code>,<code>DROP QUOTA</code>,<code>DROP [ROW] POLICY</code>,<code>DROP [SETTINGS] PROFILE</code></li><li>ASTDropQuery：<br>其中包含了三种不同的删除操作（<code>Drop</code> / <code>Truncate</code> / <code>Detach</code>），<code>Detach Table</code>和<code>Attach Table</code>对应，它是表的卸载动作，把表的存储目录整个移到专门的<code>detach</code>文件夹下，然后关闭表在节点<code>RAM</code>中的”引用”，这张表在节点中不再可见。</li><li>ASTGrantQuery:<br>这是授权相关的<code>RBAC</code>，可以对库/表授予或者撤销读/写等权限命令，例如<code>GRANT insert on db.tb to acount</code>，或者<code>REVOKE all on db.tb from account</code>。</li><li>ASTKillQueryQuery：<br>可以<code>Kill</code>正在运行的<code>Query</code>，也可以<code>Kill</code>之前发送的<code>Mutation</code>命令。</li><li>ASTOptimizeQuery：<br>这是<code>MergeTree</code>表引擎特有的操作命令，它可以手动触发<code>MergeTree</code>表的合并动作，并可以强制数据分区下的所有<code>Data Part</code>合并成一个。</li><li>ASTRenameQuery：<br>修改表名，可更改到不同库下。</li></ul><h3 id="DDL-Query-Task分发"><a href="#DDL-Query-Task分发" class="headerlink" title="DDL Query Task分发"></a>DDL Query Task分发</h3><p><img src="/images/clickhouse/ddloncluster/2.png" alt="cgi"><br><code>ClickHouse</code>内核对每种<code>SQL</code>操作都有对应的<code>IInterpreter</code>实现类，其中的<code>execute</code>方法负责具体的操作逻辑。而以上列举的<code>ASTQuery</code>对应的<code>IInterpreter</code>实现类中的<code>execute</code>方法都加入了分布式<code>DDL</code>执行判断逻辑，把所有分布式<code>DDL</code>执行链路统一都<code>DDLWorker::executeDDLQueryOnCluster</code>方法中。<br><code>executeDDLQueryOnCluster</code>的过程大致可以分为三个步骤：</p><h4 id="检查DDLQuery的合法性，"><a href="#检查DDLQuery的合法性，" class="headerlink" title="检查DDLQuery的合法性，"></a>检查<code>DDLQuery</code>的合法性，</h4><ul><li>1、校验query规则<br><img src="/images/clickhouse/ddloncluster/3.png" alt="cgi"></li><li>2、初始化DDLWorker，取config.xml表的配置<br><img src="/images/clickhouse/ddloncluster/4.png" alt="cgi"></li><li>3、替换query里的数据库名称<br><img src="/images/clickhouse/ddloncluster/5.png" alt="cgi"><br>这里替换库名的逻辑是，</li><li>3.1、如果query里有带上库名称，则直接使用，若无，则走2</li><li>3.2、metrika.xml里配置了shard的默认库<code>&lt;default_database&gt;default&lt;/default_database&gt;</code>，则使用默认库，否则走3</li><li>3.3、使用当前session的database<br><img src="/images/clickhouse/ddloncluster/6.png" alt="cgi"></li></ul><h4 id="把DDLQuery写入到Zookeeper任务队列中"><a href="#把DDLQuery写入到Zookeeper任务队列中" class="headerlink" title="把DDLQuery写入到Zookeeper任务队列中"></a>把<code>DDLQuery</code>写入到<code>Zookeeper</code>任务队列中</h4><ul><li>1、构造DDLLogEntry对象，把entry对象加入到queue队列中<br><img src="/images/clickhouse/ddloncluster/7.png" alt="cgi"><br>注意：queue_dir是由<strong>config.xml</strong>配置的，如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;distributed_ddl&gt;</span><br><span class="line">    &lt;path&gt;&#x2F;clickhouse&#x2F;task_queue&#x2F;ddl&lt;&#x2F;path&gt;</span><br><span class="line">&lt;&#x2F;distributed_ddl&gt;</span><br></pre></td></tr></table></figure></li><li>2、去zookeeper执行创建znode，把entry序列化存入znode<br><img src="/images/clickhouse/ddloncluster/8.png" alt="cgi"></li><li>3、在znode下创建active和finished的znode<br><img src="/images/clickhouse/ddloncluster/9.png" alt="cgi"><blockquote><p>下面截图为query-xxx的记录的entry内容<br><img src="/images/clickhouse/ddloncluster/10.png" alt="cgi"></p></blockquote></li></ul><h4 id="等待Zookeeper任务队列的反馈把结果返回给用户。"><a href="#等待Zookeeper任务队列的反馈把结果返回给用户。" class="headerlink" title="等待Zookeeper任务队列的反馈把结果返回给用户。"></a>等待<code>Zookeeper</code>任务队列的反馈把结果返回给用户。</h4><p><img src="/images/clickhouse/ddloncluster/11.png" alt="cgi"></p><h4 id="DDL-Query-Task执行线程"><a href="#DDL-Query-Task执行线程" class="headerlink" title="DDL Query Task执行线程"></a>DDL Query Task执行线程</h4><ul><li>1、DDLWorker构造函数去取了config.xml配置，并且开启了2个线程，分别是执行线程和清理线程<br><img src="/images/clickhouse/ddloncluster/12.png" alt="cgi"></li><li>2、执行线程加入到线程池后，执行ddl task<br><img src="/images/clickhouse/ddloncluster/13.png" alt="cgi"></li><li>3、过滤掉 query 中带有 on cluster xxx的语句，根据不同的query选择不同执行方式<br><img src="/images/clickhouse/ddloncluster/14.png" alt="cgi"></li><li>4、alter、optimize、truncate语句需要在leader节点执行<br><img src="/images/clickhouse/ddloncluster/15.png" alt="cgi"><blockquote><p>注意：Replcated表的alter、optimize、truncate这些query是会先判断是否leader节点，不是则不处理，在执行时，会先给zookeeper加一个分布式锁，锁住这个任务防止被修改，执行时都是把自己的host:port注册到znode/query-xxx/active下，执行完成后，结果写到znode/query-xxx/finished下。</p></blockquote></li></ul><h4 id="DDL-Query-Task清理线程"><a href="#DDL-Query-Task清理线程" class="headerlink" title="DDL Query Task清理线程"></a>DDL Query Task清理线程</h4><ul><li>1、DDLWorker构造函数去取了config.xml配置，并且开启了2个线程，分别是执行线程和清理线程<br><img src="/images/clickhouse/ddloncluster/12.png" alt="cgi"></li><li>2、执行清理逻辑，每次执行后，下一次执行需要过1分钟后才可以接着做清理<br><img src="/images/clickhouse/ddloncluster/16.png" alt="cgi"></li></ul><h3 id="分布式DDL的执行链路总结"><a href="#分布式DDL的执行链路总结" class="headerlink" title="分布式DDL的执行链路总结"></a>分布式DDL的执行链路总结</h3><ul><li><p>1）节点收到用户的分布式<code>DDL</code>请求</p></li><li><p>2）节点校验分布式DDL请求合法性，在<code>Zookeeper</code>的任务队列中创建<code>Znode</code>并上传<strong>DDL LogEntry（query-xxxx）</strong>，同时在<code>LogEntry</code>的<code>Znode</code>下创建<code>active</code>和<code>finish</code>两个状态同步的<code>Znode</code></p></li><li><p>3）<code>Cluster</code>中的节点后台线程消费<code>Zookeeper</code>中的<code>LogEntry</code>队列执行处理逻辑，处理过程中把自己注册到<code>acitve Znode</code>下，并把处理结果写回到<code>finish Znode</code>下<br><img src="/images/clickhouse/ddloncluster/12.png" alt="cgi"></p></li><li><p>4）用户的原始请求节点，不断轮询<code>LogEntry Znode</code>下的<code>active</code>和<code>finish</code>状态<code>Znode</code>，当目标节点全部执行完成任务或者触发超时逻辑时，用户就会获得结果反馈</p></li></ul><p>这个分发逻辑中有个值得注意的点：分布式<code>DDL</code>执行链路中有超时逻辑，如果触发超时用户将无法从客户端返回中确定最终执行结果，需要自己去<code>Zookeeper</code>上<code>check</code>节点返回结果（也可以通过<strong>system.zookeeper</strong>系统表查看）。<strong>每个节点只有一个后台线程在消费执行DDL任务</strong>，碰到某个DDL任务（典型的是optimize任务）执行时间很长时，会导致DDL任务队列积压从而产生大面积的超时反馈。</p><p>可以看出Zookeeper在分布式DDL执行过程中主要充当DDL Task的分发、串行化执行、结果收集的一致性介质。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;分布式DDL执行链路&quot;&gt;&lt;a href=&quot;#分布式DDL执行链路&quot; class=&quot;headerlink&quot; title=&quot;分布式DDL执行链路&quot;&gt;&lt;/a&gt;分布式DDL执行链路&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;在介绍具体的分布式&lt;code&gt;DDL&lt;/code&gt;
      
    
    </summary>
    
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
      <category term="clickhouse" scheme="http://yoursite.com/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>记一次线上fgc排查过程</title>
    <link href="http://yoursite.com/2022/01/19/java_full_gc/"/>
    <id>http://yoursite.com/2022/01/19/java_full_gc/</id>
    <published>2022-01-19T11:04:11.000Z</published>
    <updated>2022-08-22T10:09:00.259Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>用户反馈api请求时快时慢，慢的时候网页打开很久都没有结果，直到超时给出500错误</p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><h4 id="初步怀疑"><a href="#初步怀疑" class="headerlink" title="初步怀疑"></a>初步怀疑</h4><p>当前程序分布式部署在多个节点上，通过<code>vip</code>进行负载均衡，所以对于直接将反馈慢的页面打开的请求通过<code>curl</code>方式登陆生产环境所有节点，进行轮训一遍，发现其中02节点需要等待很久，其他节点均正常，所以问题应该出在02节点，<br>注意：这个定位过程当然也可以使用监控图就一目了然了。推荐使用<code>grafana prometheus spring boot dashboard</code>在<code>google</code>搜寻下相关配置就可以了。<br><img src="/images/java/fgc/8.png" alt="cgi"></p><h4 id="查询该节点负载"><a href="#查询该节点负载" class="headerlink" title="查询该节点负载"></a>查询该节点负载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 得到进程的pid</span><br><span class="line">jps -mlv </span><br><span class="line"># 通过top命令查询节点实际负载</span><br><span class="line">top -Hp pid</span><br></pre></td></tr></table></figure><p><img src="/images/java/fgc/1.png" alt="cgi"></p><blockquote><p>通过top命令发现该节点memory使用了接近<code>90%</code>，怀疑出现内存泄露</p></blockquote><h4 id="查询该节点内存使用情况"><a href="#查询该节点内存使用情况" class="headerlink" title="查询该节点内存使用情况"></a>查询该节点内存使用情况</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jstat -gcutil pid 1000</span><br></pre></td></tr></table></figure><p><img src="/images/java/fgc/2.png" alt="cgi"></p><blockquote><p>如我所料，几乎不到<code>1s</code>就开始做一次<code>fgc</code>，所以服务才越来越慢响应</p></blockquote><h3 id="内存分析"><a href="#内存分析" class="headerlink" title="内存分析"></a>内存分析</h3><h4 id="使用jmap分析内存概要"><a href="#使用jmap分析内存概要" class="headerlink" title="使用jmap分析内存概要"></a>使用jmap分析内存概要</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -heap pid ｜ head -n20</span><br></pre></td></tr></table></figure><p><img src="/images/java/fgc/10.png" alt="cgi"></p><h4 id="使用jmap打印堆内存的对象，带上live，则只统计活着的对象"><a href="#使用jmap打印堆内存的对象，带上live，则只统计活着的对象" class="headerlink" title="使用jmap打印堆内存的对象，带上live，则只统计活着的对象"></a>使用jmap打印堆内存的对象，带上live，则只统计活着的对象</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jmap -histo pid ｜ head -n20</span><br><span class="line">jmap -histo:live pid ｜ head -n20</span><br></pre></td></tr></table></figure><p><img src="/images/java/fgc/11.png" alt="cgi"></p><h4 id="打印进程的内存使用情况"><a href="#打印进程的内存使用情况" class="headerlink" title="打印进程的内存使用情况"></a>打印进程的内存使用情况</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format&#x3D;b,file&#x3D;dumpFileName pid</span><br></pre></td></tr></table></figure><p><img src="/images/java/fgc/3.png" alt="cgi"></p><blockquote><p>dump出来了<code>12G</code>的文件，通过scp工具转存到本地</p></blockquote><h4 id="jprofiler分析堆内存"><a href="#jprofiler分析堆内存" class="headerlink" title="jprofiler分析堆内存"></a>jprofiler分析堆内存</h4><ul><li>1、把dumpFileName文件转存为.hprof格式后直接双击打开，按照instance count逆序排列<br><img src="/images/java/fgc/5.png" alt="cgi"></li><li>2、会发现有hashmap类型占了最大头，但是这个类型，双击它，选择<code>merged incoming reference</code>，查看合并后的来源引用统计<br><img src="/images/java/fgc/6.png" alt="cgi"></li><li>3、还是选最大头的文件数一直拆到最里层，找出来源引用是<code>org.apache.ibatis.executor.result.DefaultResultHandler</code>这个类，基本能定位到问题根源了，是我们连接<code>clickhouse</code>客户端去查询结果时，未对结果集做限制，导致了一个很大的结果集返回到内存中。<br><img src="/images/java/fgc/7.png" alt="cgi"></li></ul><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>当然是对结果集做限制，检测用户输入的sql，是否包含limit条数限制，若未限制，则对sql进行改写，增加<code>500</code>条数限制<br><img src="/images/java/fgc/9.png" alt="cgi"></p><h3 id="GC的运行原理"><a href="#GC的运行原理" class="headerlink" title="GC的运行原理"></a>GC的运行原理</h3><p><code>GC</code>（<code>garbage collection</code>）：垃圾回收，主要是指<code>YGC</code>和<code>FGC</code><br><code>YGC</code>（minor garbage collection）：新生代垃圾回收<br><code>FGC</code>（major garbage collection）：老年代垃圾回收</p><h4 id="堆内存结构"><a href="#堆内存结构" class="headerlink" title="堆内存结构"></a>堆内存结构</h4><p><img src="/images/java/fgc/12.png" alt="cgi"><br>堆内存采用了分代结构，包括新生代和老年代，新生代分为：<code>eden</code>区、<code>from survivor</code>区（简称<code>s0</code>）、<code>to survivor</code>区（简称<code>s1</code>），三者默认比例上8:1:1，另外新生代和老年代的比例则是1:2。<br>堆内存之所以采用分代结构，是因为绝大多数对象都是短生命周期的，这样设计可以把不同的生命周期的对象放在不同的区域中，然后针对新生代和老年代采用不同的垃圾回收算法，从而使得<code>GC</code>效率最高。</p><h4 id="YGC是什么时候触发的？"><a href="#YGC是什么时候触发的？" class="headerlink" title="YGC是什么时候触发的？"></a>YGC是什么时候触发的？</h4><p>大多数情况下，对象直接在年轻代中的<code>Eden</code>区进行分配，如果<code>Eden</code>区域没有足够的空间，那么就会触发<code>YGC</code>（<code>Minor GC</code>），<code>YGC</code>处理的区域只有新生代。因为大部分对象在短时间内都是可收回掉的，因此YGC后只有极少数的对象能存活下来，而被移动到S0区（采用的是复制算法）。<br>当触发下一次YGC时，会将Eden区和S0区的存活对象移动到S1区，同时清空Eden区和S0区。当再次触发YGC时，这时候处理的区域就变成了Eden区和S1区（即S0和S1进行角色交换）。每经过一次YGC，存活对象的年龄就会加1。</p><h4 id="FGC是什么时候触发的？"><a href="#FGC是什么时候触发的？" class="headerlink" title="FGC是什么时候触发的？"></a>FGC是什么时候触发的？</h4><ul><li><p>1、<code>YGC</code>时，<code>To Survivor</code>区不足以存放存活的对象，对象会直接进入到老年代。经过多次<code>YGC</code>后，如果存活对象的年龄达到了设定阈值，则会晋升到老年代中。动态年龄判定规则，<code>To Survivor</code>区中相同年龄的对象，如果其大小之和占到了 <code>To Survivor</code> 区一半以上的空间，那么大于此年龄的对象会直接进入老年代，而不需要达到默认的分代年龄。大对象：由<code>-XX:PretenureSizeThreshold</code>启动参数控制，若对象大小大于此值，就会绕过新生代, 直接在老年代中分配。当晋升到老年代的对象大于了老年代的剩余空间时，就会触发<code>FGC</code>（<code>Major GC</code>），<code>FGC</code>处理的区域同时包括新生代和老年代。老年代的内存使用率达到了一定阈值（可通过参数调整），直接触发<code>FGC</code>。</p></li><li><p>2、空间分配担保：在<code>YGC</code>之前，会先检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。如果小于，说明<code>YGC</code>是不安全的，则会查看参数 <code>HandlePromotionFailure</code> 是否被设置成了允许担保失败，如果不允许则直接触发<code>Full GC</code>；如果允许，那么会进一步检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果小于也会触发 <code>Full GC</code>。</p></li><li><p>3、<code>Metaspace</code>（元空间）在空间不足时会进行扩容，当扩容到了<code>-XX:MetaspaceSize</code> 参数的指定值时，也会触发<code>FGC</code>。</p></li><li><p>4、<code>System.gc</code>() 或者<code>Runtime.gc</code>() 被显式调用时，触发<code>FGC</code>。</p></li></ul><h4 id="GC对程序会产生什么影响"><a href="#GC对程序会产生什么影响" class="headerlink" title="GC对程序会产生什么影响"></a>GC对程序会产生什么影响</h4><p>不管是YGC还是FGC，都会造成一定程度上的程序卡顿（stop the world问题：GC线程开始工作，其他工作线程被挂起），即使采用ParNew、CMS、G1这些更先进的垃圾回收算法，也只是减少卡顿的时间，并不能完全消除卡顿</p><ul><li>FGC过于频繁：<br>FGC通常是比较慢的，少则几百号秒，多则几秒，正常情况下FGC每隔几个小时或者几天才会执行一次，对系统的影响是可接受的，所以一旦出现FGC频繁（比如几分钟/几十分钟出现一次）会导致工作线程频繁被停掉，让系统看起来就一直卡顿，使得程序的整体性能变差。</li><li>YGC耗时过长：<br>一般来说YGC的总耗时指需要几十毫秒或上百毫秒，对于系统来说几乎无感知，所以如果YGC耗时达到1秒甚至几秒（快赶上FGC的耗时），那么卡顿就会加剧，加上YGC本身会比较频繁发生，就可能导致服务响应时间超时。</li><li>FGC耗时过长：<br>FGC耗时增加，卡顿时间也会随之增加，尤其对于高并发服务，可能导致FGC期间比较多的超时问题，可用性降低，这种也需要关注</li><li>YGC过于频繁：<br>即使YGC不会引起服务超时，但是YGC过于频繁也会降低服务的整体性能，对于高并发服务也是需要关注的。</li></ul><blockquote><p>其中，「FGC过于频繁」和「YGC耗时过长」，这两种情况属于比较典型的GC问题，大概率会对程序的服务质量产生影响。剩余两种情况的严重程度低一些，但是对于高并发或者高可用的程序也需要关注。</p></blockquote><h4 id="导致FGC的原因总结"><a href="#导致FGC的原因总结" class="headerlink" title="导致FGC的原因总结"></a>导致FGC的原因总结</h4><ul><li>大对象：系统一次性加载了过多数据到内存中（比如SQL查询未做分页），导致大对象进入了老年代。（即本文中的案例）</li><li>内存泄漏：频繁创建了大量对象，但是无法被回收（比如IO对象使用完后未调用close方法释放资源），先引发FGC，最后导致OOM.</li><li>程序频繁生成一些长生命周期的对象，当这些对象的存活年龄超过分代年龄时便会进入老年代，最后引发FGC. </li><li>程序BUG导致动态生成了很多新类，使得 Metaspace 不断被占用，先引发FGC，最后导致OOM.</li><li>代码中显式调用了gc方法，包括自己的代码甚至框架中的代码。</li><li>JVM参数设置问题：包括总内存大小、新生代和老年代的大小、Eden区和S区的大小、元空间大小、垃圾回收算法等等。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;p&gt;用户反馈api请求时快时慢，慢的时候网页打开很久都没有结果，直到超时给出500错误&lt;/p&gt;
&lt;h3 id=&quot;排查过程&quot;&gt;&lt;a
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="springboot" scheme="http://yoursite.com/tags/springboot/"/>
    
  </entry>
  
  <entry>
    <title>记一次线上core dump</title>
    <link href="http://yoursite.com/2022/01/19/linux_min_free_kbytes/"/>
    <id>http://yoursite.com/2022/01/19/linux_min_free_kbytes/</id>
    <published>2022-01-19T11:04:11.000Z</published>
    <updated>2022-08-22T10:41:13.800Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>官方解释<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">min_free_kbytes:</span><br><span class="line">This is used to force the Linux VM to keep a minimum number</span><br><span class="line">of kilobytes free.  The VM uses this number to compute a</span><br><span class="line">watermark[WMARK_MIN] value for each lowmem zone in the system.</span><br><span class="line">Each lowmem zone gets a number of reserved free pages based</span><br><span class="line">proportionally on its size.</span><br><span class="line">Some minimal amount of memory is needed to satisfy PF_MEMALLOC</span><br><span class="line">allocations; if you set this to lower than 1024KB, your system will</span><br><span class="line">become subtly broken, and prone to deadlock under high loads.</span><br><span class="line">Setting this too high will OOM your machine instantly.</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 3145728 &gt;&gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -a | grep min_free</span><br></pre></td></tr></table></figure><p><img src="/images/min_free_kbytes/2.png" alt="docker-bridge"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a | grep oom</span><br></pre></td></tr></table></figure><p><img src="/images/min_free_kbytes/1.png" alt="docker-bridge"></p><p>min_free_kbytes大小的影响<br>min_free_kbytes设的越大，watermark的线越高，同时三个线之间的buffer量也相应会增加。这意味着会较早的启动kswapd进行回收，且会回收上来较多的内存（直至watermark[high]才会停止），这会使得系统预留过多的空闲内存，从而在一定程度上降低了应用程序可使用的内存量。极端情况下设置min_free_kbytes接近内存大小时，留给应用程序的内存就会太少而可能会频繁地导致OOM的发生。</p><p>min_free_kbytes设的过小，则会导致系统预留内存过小。kswapd回收的过程中也会有少量的内存分配行为（会设上PF_MEMALLOC）标志，这个标志会允许kswapd使用预留内存；另外一种情况是被OOM选中杀死的进程在退出过程中，如果需要申请内存也可以使用预留部分。这两种情况下让他们使用预留内存可以避免系统进入deadlock状态。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;p&gt;官方解释&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;
      
    
    </summary>
    
    
      <category term="sre" scheme="http://yoursite.com/categories/sre/"/>
    
    
      <category term="cpp" scheme="http://yoursite.com/tags/cpp/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
</feed>
