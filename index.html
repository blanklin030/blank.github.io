<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="say something about me">
<meta property="og:type" content="website">
<meta property="og:title" content="BlankLin">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="BlankLin">
<meta property="og:description" content="say something about me">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Blank Lin">
<meta property="article:tag" content="lazy and boring">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>BlankLin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BlankLin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BlankLin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">lazy and boring</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/" class="post-title-link" itemprop="url">常用的linux排查命令</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-04 23:04:11 / 修改时间：23:05:15" itemprop="dateCreated datePublished" datetime="2023-01-04T23:04:11+08:00">2023-01-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tool/" itemprop="url" rel="index"><span itemprop="name">tool</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何看查占用cpu最多的进程？"><a href="#如何看查占用cpu最多的进程？" class="headerlink" title="如何看查占用cpu最多的进程？"></a>如何看查占用cpu最多的进程？</h2><ul>
<li><p>方法一<br>核心指令：ps<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps H -eo pid,pcpu | sort -nk2 | tail</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps H -eo pid,pcpu | sort -nk2 | tail</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">30904  1.0</span><br><span class="line">30914  1.0</span><br></pre></td></tr></table></figure>
<p>结果：<br>瞧见了吧，最耗cpu的pid=30914。<br>画外音：实际上是31396。</p>
</li>
<li><p>方法二<br>核心指令：top<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">top</span><br><span class="line">Shift + t</span><br></pre></td></tr></table></figure>
<h2 id="找到了最耗CPU的进程ID，对应的服务名是什么呢？"><a href="#找到了最耗CPU的进程ID，对应的服务名是什么呢？" class="headerlink" title="找到了最耗CPU的进程ID，对应的服务名是什么呢？"></a>找到了最耗CPU的进程ID，对应的服务名是什么呢？</h2></li>
<li><p>方法一<br>核心指令：ps<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | fgrep pid</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps aux | fgrep 30914</span><br><span class="line">work 30914  1.0  0.8 309568 71668 ?  Sl   Feb02 124:44 .&#x2F;router2 –conf&#x3D;rs.conf</span><br></pre></td></tr></table></figure>
<p>结果：<br>瞧见了吧，进程是./router2</p>
</li>
<li><p>方法二<br>直接查proc即可。<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll &#x2F;proc&#x2F;pid</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ll &#x2F;proc&#x2F;30914</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 cwd -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 exe -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2&#x2F;router2</span><br></pre></td></tr></table></figure>
<p>画外音：这个好，全路径都出来了。</p>
</li>
</ul>
<h2 id="如何查看某个端口的连接情况？"><a href="#如何查看某个端口的连接情况？" class="headerlink" title="如何查看某个端口的连接情况？"></a>如何查看某个端口的连接情况？</h2><ul>
<li><p>方法一<br>核心指令：netstat<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lap | fgrep port</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ netstat -lap | fgrep 22022</span><br><span class="line">tcp        0      0 1.2.3.4:22022          *:*                         LISTEN      31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46642          ESTABLISHED 31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46640          ESTABLISHED 31396&#x2F;imui</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法二<br>核心指令：lsof<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i :port</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ &#x2F;usr&#x2F;sbin&#x2F;lsof -i :22022</span><br><span class="line">COMMAND   PID USER   FD   TYPE   DEVICE SIZE NODE NAME</span><br><span class="line">router  30904 work   50u  IPv4 69065770       TCP 1.2.3.4:46638-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   51u  IPv4 69065772       TCP 1.2.3.4:46639-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   52u  IPv4 69065774       TCP 1.2.3.4:46640-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br></pre></td></tr></table></figure>
<p>学废了吗？</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/30/2022/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/30/2022/" class="post-title-link" itemprop="url">2022年终总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-12-30 16:04:11 / 修改时间：15:46:15" itemprop="dateCreated datePublished" datetime="2022-12-30T16:04:11+08:00">2022-12-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blank/" itemprop="url" rel="index"><span itemprop="name">blank</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><p>2022年仓皇而逃，在恍惚间我仍然有一丝怀疑，此时此刻是2019年的冬天，我们在准备着即将到来到2020年，订好了机票，订好了酒店，在跨年夜的云层之上，陌生人在沉睡，而我即将见到相隔数月思念日日夜夜的人儿。</p>
</li>
<li><p>这种怀疑在这3年间时不时的击碎我，重建我，沉溺我，到最后清醒我，时间过得太快了啊，快到迅雷烈风，快到掩耳而逝，我不是还在17/8岁的高中课堂上汗流浃背准备高考吗？怎么这一眨眼之间，我竟已是前额白发丝丝黑眼圈蜡黄脸的中年人了呢？</p>
</li>
<li><p>人世间的痛不知道何时能结束，人世间的离愁不知道何时能消散，虚度30余载光阴，到如今即将到来的第二个本命年，我有何成就吗？我有实现过理想吗？我活着的这些时刻，有给这个世界带来什么美好吗？我不自知只觉惭愧。</p>
</li>
<li><p>麻木的躯壳麻木的灵魂，肤浅的知识肤浅的见识，2022年在核酸、隔离、阳性中结束，这浅浅6个字结束了我的又一个365天，我是谁？我在2022年都做了什么事？有什么可以拿说来细说一二吗？有什么不足和遗憾吗？希望2023年可以改变吗？希望改变什么呢？</p>
</li>
<li><p>没有答案，我甚至连流水账都写不出来，哦，我这个可笑的中年人。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">clickhouse集群cpu飙高问题排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-08 19:35:11" itemprop="dateCreated datePublished" datetime="2022-12-08T19:35:11+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-15 22:00:44" itemprop="dateModified" datetime="2022-12-15T22:00:44+08:00">2022-12-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse是分布式系统，一条查询sql会经过pipeline处理后通过后台的查询线程池开启多线程查询，而经常会收到cpu飙高告警，则是因为这条sql开启了系统所有的cpu资源（多个线程）进行计算</p>
<h3 id="告警图"><a href="#告警图" class="headerlink" title="告警图"></a>告警图</h3><p>如下图所示，提示cpu飙高<br><img src="/images/clickhouse/cpu_high/1.png" alt="clickhouse"><br>如下图所示，ganglia上展示了当时的cpu顶着线在跑<br><img src="/images/clickhouse/cpu_high/2.png" alt="clickhouse"></p>
<h3 id="system-query-log"><a href="#system-query-log" class="headerlink" title="system.query_log"></a>system.query_log</h3><blockquote>
<p><a href="https://clickhouse.com/docs/en/operations/system-tables/query_log" target="_blank" rel="noopener">地址</a>，该表包含了所有进入到ck的sql语句。<br>该表包含字段如下：</p>
<ul>
<li>type (Enum8) — 执行查询时的事件类型. 值:<ul>
<li>‘QueryStart’ = 1 — 查询成功启动.</li>
<li>‘QueryFinish’ = 2 — 查询成功完成.</li>
</ul>
</li>
<li>‘ExceptionBeforeStart’ = 3 — 查询执行前有异常.</li>
<li>‘ExceptionWhileProcessing’ = 4 — 查询执行期间有异常.</li>
<li>event_date (Date) — 查询开始日期.</li>
<li>event_time (DateTime) — 查询开始时间.</li>
<li>event_time_microseconds (DateTime64) — 查询开始时间（毫秒精度）.</li>
<li>query_start_time (DateTime) — 查询执行的开始时间.</li>
<li>query_start_time_microseconds (DateTime64) — 查询执行的开始时间（毫秒精度）.</li>
<li>query_duration_ms (UInt64) — 查询消耗的时间（毫秒）.</li>
<li>read_rows (UInt64) — 从参与了查询的所有表和表函数读取的总行数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_rows 包括在所有副本上读取的行总数。 每个副本发送它的 read_rows 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li>
<li>read_bytes (UInt64) — 从参与了查询的所有表和表函数读取的总字节数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_bytes 包括在所有副本上读取的字节总数。 每个副本发送它的 read_bytes 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li>
<li>written_rows (UInt64) — 对于 INSERT 查询，为写入的行数。 对于其他查询，值为0。</li>
<li>written_bytes (UInt64) — 对于 INSERT 查询时，为写入的字节数。 对于其他查询，值为0。</li>
<li>result_rows (UInt64) — SELECT 查询结果的行数，或INSERT 的行数。</li>
<li>result_bytes (UInt64) — 存储查询结果的RAM量.</li>
<li>memory_usage (UInt64) — 查询使用的内存.</li>
<li>query (String) — 查询语句.</li>
<li>exception (String) — 异常信息.</li>
<li>exception_code (Int32) — 异常码.</li>
<li>stack_trace (String) — Stack Trace. 如果查询成功完成，则为空字符串。</li>
<li>is_initial_query (UInt8) — 查询类型. 可能的值:<ul>
<li>1 — 客户端发起的查询.</li>
<li>0 — 由另一个查询发起的，作为分布式查询的一部分.</li>
</ul>
</li>
<li>user (String) — 发起查询的用户.</li>
<li>query_id (String) — 查询ID.</li>
<li>address (IPv6) — 发起查询的客户端IP地址.</li>
<li>port (UInt16) — 发起查询的客户端端口.</li>
<li>initial_user (String) — 初始查询的用户名（用于分布式查询执行）.</li>
<li>initial_query_id (String) — 运行初始查询的ID（用于分布式查询执行）.</li>
<li>initial_address (IPv6) — 运行父查询的IP地址.</li>
<li>initial_port (UInt16) — 发起父查询的客户端端口.</li>
<li>interface (UInt8) — 发起查询的接口. 可能的值:<ul>
<li>1 — TCP.</li>
<li>2 — HTTP.</li>
</ul>
</li>
<li>os_user (String) — 运行 clickhouse-client的操作系统用户名.</li>
<li>client_hostname (String) — 运行clickhouse-client 或其他TCP客户端的机器的主机名。</li>
<li>client_name (String) — clickhouse-client 或其他TCP客户端的名称。</li>
<li>client_revision (UInt32) — clickhouse-client 或其他TCP客户端的Revision。</li>
<li>client_version_major (UInt32) — clickhouse-client 或其他TCP客户端的Major version。</li>
<li>client_version_minor (UInt32) — clickhouse-client 或其他TCP客户端的Minor version。</li>
<li>client_version_patch (UInt32) — clickhouse-client 或其他TCP客户端的Patch component。</li>
<li>http_method (UInt8) — 发起查询的HTTP方法. 可能值:<ul>
<li>0 — TCP接口的查询.</li>
<li>1 — GET</li>
<li>2 — POST</li>
</ul>
</li>
<li>http_user_agent (String) — http请求的客户端参数</li>
<li>quota_key (String) — 在quotas 配置里设置的“quota key” （见 keyed).</li>
<li>revision (UInt32) — ClickHouse revision.</li>
<li>ProfileEvents (Map(String, UInt64))) — 不同指标的计数器 </li>
<li>Settings (Map(String, String)) — 当前请求里的setting部分参数</li>
<li>thread_ids (Array(UInt64)) — 参与查询的线程数.</li>
<li>Settings.Names (Array（String)) — 客户端运行查询时更改的设置的名称。 要启用对设置的日志记录更改，请将log_query_settings参数设置为1。</li>
<li>Settings.Values (Array（String)) — Settings.Names 列中列出的设置的值。</li>
</ul>
</blockquote>
<h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><ul>
<li>查询当前时间内耗cpu最高的sql前10条<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select initial_user, event_time, query,</span><br><span class="line">read_rows, read_bytes,</span><br><span class="line">written_rows, written_bytes,</span><br><span class="line">result_rows, result_bytes,</span><br><span class="line">memory_usage, length(thread_ids) as thread_count</span><br><span class="line">from system.query_log</span><br><span class="line">WHERE event_time &gt; &#39;2022-10-27 18:30:00&#39; AND event_time &lt; &#39;2022-10-27 18:35:00&#39; </span><br><span class="line">and initial_user&lt;&gt;&#39;default&#39;</span><br><span class="line">order by thread_count desc</span><br><span class="line">limit 10;</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/clickhouse%20%E8%A1%A8readonly%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/08/clickhouse%20%E8%A1%A8readonly%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">clickhouse 表写入报readonly排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-08 16:04:11" itemprop="dateCreated datePublished" datetime="2022-12-08T16:04:11+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-15 13:32:36" itemprop="dateModified" datetime="2023-01-15T13:32:36+08:00">2023-01-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse的Replicated<em>MergeTree表是通过zookeeper来完成同一个shard之间的副本数据的同步，当<em>*Table is in readonly mode</em></em>的原因是zookeeper当下压力过大，这个可以从system.replication_queue看到同步队列的数量，或者通过system.part_log看当前part的处理数量</p>
<h3 id="system-replicas"><a href="#system-replicas" class="headerlink" title="system.replicas"></a>system.replicas</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/replicas" target="_blank" rel="noopener">官方地址</a>所解释,该表记录了驻留在本地服务器上的复制表的信息和状态，如下图所示，我们看到每个表的每个副本在zookeeper上的状态</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select table,zookeeper_path,replica_path from &#96;system&#96;.replicas limit 10</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/readonly/1.png" alt="clickhouse"><br>如上图所示，<strong>zookeeper_path</strong>字段可以查看到在哪个shard上的哪个副本处于readonly状态。</p>
<h3 id="system-part-log"><a href="#system-part-log" class="headerlink" title="system.part_log"></a>system.part_log</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/part_log" target="_blank" rel="noopener">官方地址</a>，该表包含part操作的所有记录，包括drop/merge/download/new等。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT *    </span><br><span class="line">FROM system.part_log</span><br><span class="line">WHERE  concat(database, &#39;.&#39;, table) &#x3D; &#39;i9xiaoapp_stream.dwd_pope_core_action_diff_di_local&#39;</span><br><span class="line">and event_time &gt;&#x3D; &#39;2023-01-15 12:00:00&#39;</span><br><span class="line">order by event_time desc</span><br></pre></td></tr></table></figure><br><img src="/images/clickhouse/readonly/2.png" alt="clickhouse"></p>
</blockquote>
<h3 id="解决副本不同步问题"><a href="#解决副本不同步问题" class="headerlink" title="解决副本不同步问题"></a>解决副本不同步问题</h3><blockquote>
<p>注意，此时先让用户把写入任务停掉，已方便观察！</p>
<ul>
<li>可以通过system.part_log查询mergepart状态的写入是否都已经完成</li>
<li>根据system.replicas表给出的readonly状态的zookeeper_path，去到当前所指向的副本机器上，删除这个副本的表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table i9xiaoapp_stream.dwd_pope_core_action_diff_di_local</span><br></pre></td></tr></table></figure></li>
<li>操作之后，可以通过查询system.replicas里看是否还有处于readonly，发现已经消失<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select table,zookeeper_path,replica_path from &#96;system&#96;.replicas where concat(database, &#39;.&#39;, table) &#x3D; &#39;i9xiaoapp_stream.dwd_pope_core_action_diff_di_local&#39; limit 10</span><br></pre></td></tr></table></figure></li>
<li>再在出问题的副本上，重新创建该本，之后可通过system.replicas找到该副本再次出现，并且已恢复<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table</span><br></pre></td></tr></table></figure></li>
<li>如果还没恢复，则去对应出错的副本节点，去检查一下zookeeper上的对应路径是否也已经删除<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmr zookeeper_path</span><br></pre></td></tr></table></figure></li>
<li>此时再查询system.replicas表readonly的队列应该已经被清理掉了<br>可以继续操作元数据修改</li>
</ul>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">clickhouse副本不同步问题排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-08 16:04:11" itemprop="dateCreated datePublished" datetime="2022-12-08T16:04:11+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-22 17:32:56" itemprop="dateModified" datetime="2022-12-22T17:32:56+08:00">2022-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse的Replicated**MergeTree表是通过zookeeper来完成同一个shard之间的副本数据的同步，因为clickhouse本身不是master/slave的架构，我们通过proxy的方式，设定了同一个shard里某个副本是读/写节点角色，另一个节点是读角色，当用户写入时，proxy会路由到读/写节点去完成写入操作，然后通过zk发起同步任务，把日志进入到system.replcation_queue。<br>今天早上用户和我们反馈说数据查询时每一次结果都不一样，用户查询也是通过连我们的proxy进行读节点的路由，所以用户反馈的结果不一致，其实是第一次路由到读/写节点查询结果是a，第二次路由到读节点，查询结果是b，由于a和b两个副本数据没有同步，导致了查询结果不一致，下面是排查的过程。</p>
<h3 id="system-replication-queue"><a href="#system-replication-queue" class="headerlink" title="system.replication_queue"></a>system.replication_queue</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/replication_queue/" target="_blank" rel="noopener">官方地址</a>所解释,该表记录了当前的副本任务队列的所有信息，如下图所示，我们看到当前副本同步出现大量异常错误  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where data_files &gt; 0</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/replicas_data_diff/1.png" alt="clickhouse"><br>如下图所示，<strong>type</strong>字段可以查看到当前是什么类型的操作导致的，发现是<strong>MUTATE_PART</strong>操作，<strong>last_exception</strong>字段显示当前我们在操作ttl时创建的元数据目录下columns.txt无法打开，这个是已知bug。<br><img src="/images/clickhouse/replicas_data_diff/2.png" alt="clickhouse"></p>
<h3 id="system-mutations"><a href="#system-mutations" class="headerlink" title="system.mutations"></a>system.mutations</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/mutations" target="_blank" rel="noopener">官方地址</a>，该表包含了所有mutation操作的日志信息，<a href="https://clickhouse.com/docs/en/sql-reference/statements/alter/#mutations" target="_blank" rel="noopener">mutation</a>操作包括修改字段类型/修改表ttl操作/按条件删表的数据/按条件更新表的数据等，这些操作都是异步后台线程去处理，都会去回溯该表的所有parts，需要rewrite每个part的信息并且这个操作还不是原子性的，所以如果某个节点操作失败，可能引发该表无法使用。  </p>
</blockquote>
<p>该表包含字段如下：</p>
<ul>
<li><p>database (String) — 数据库名称.</p>
</li>
<li><p>table (String) — 表名称.</p>
</li>
<li><p>data_path (String) — 本地文件的路径.</p>
</li>
<li><p>mutation_id (String) — mutation的唯一标识，可通过该标识直接kill mutation.</p>
</li>
<li><p>command (String) — mutation命令.</p>
</li>
<li><p>create_time (DateTime) —  mutation创建时间.</p>
</li>
<li><p>block_numbers (Map) — partition_id：需要进行mutation操作的分区id，number：需要进行mutation的分区对应的block序号.</p>
</li>
<li><p>parts_to_do_names (Array) — 即将完成的需要进行mutation的数组.</p>
</li>
<li><p>parts_to_do (Int64) — 准备进行mutation操作的part序号.</p>
</li>
<li><p>is_done (UInt8) — 该操作是否已完成.</p>
</li>
<li><p>latest_failed_part (String) — mutation操作最后失败的part名称.</p>
</li>
<li><p>latest_fail_time (DateTime) — 最后失败的时间.</p>
</li>
<li><p>latest_fail_reason (String) — 最后失败的原因.</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.mutations where latest_failed_part !&#x3D; &#39;&#39;</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/replicas_data_diff/3.png" alt="clickhouse"></p>
<blockquote>
<p>根据异常错误的<strong>command</strong>字段，我们看到错误是通过<code>MATERIALIZE TTL FAST 16070400</code>引起的，mutation操作在写节点处理完成后，也会通过zookeeper进行副本数据同步</p>
</blockquote>
<h3 id="解决副本不同步问题"><a href="#解决副本不同步问题" class="headerlink" title="解决副本不同步问题"></a>解决副本不同步问题</h3><ul>
<li>终止该失败的mutation<br>具体操作语句可以查看<a href="https://clickhouse.com/docs/zh/sql-reference/statements/kill/#:~:text=KILL%20MUTATION%E2%80%8B&amp;text=Tries%20to%20cancel%20and%20remove,list%20of%20mutations%20to%20stop." target="_blank" rel="noopener">官方文档</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill mutation where database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39; and mutation_id &#x3D; &#39;zz&#39;;</span><br></pre></td></tr></table></figure></li>
<li>操作之后，可以通过查询shard里每个副本的count是否一致来判断数据是否已经进行同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(dt), dt from xx.yy group by dt;</span><br></pre></td></tr></table></figure></li>
<li>操作之后，可以通过上面的<code>system.replication_queue</code>表来观察是否开始进行副本数据同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where type&#x3D;&#39;GET_PART&#39; and database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39;</span><br></pre></td></tr></table></figure></li>
<li>如果还没恢复，则去对应出错的副本节点，将本地表删除后重建（出错节点可以从上一步里看出来）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists xx.yy</span><br><span class="line">create table xx.yy</span><br></pre></td></tr></table></figure></li>
<li>此时再查询replication_queue表出错的队列应该已经被清理掉了<br>可以继续操作元数据修改</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/11/19/starrocks-load/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/starrocks-load/" class="post-title-link" itemprop="url">深入浅出starrocks离线导入</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-19 19:04:11" itemprop="dateCreated datePublished" datetime="2022-11-19T19:04:11+08:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-11 00:32:56" itemprop="dateModified" datetime="2023-12-11T00:32:56+08:00">2023-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><ul>
<li><ol>
<li>什么是StarRocks<br>StarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容MySQL协议和MySQL生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。StarRocks致力于在全场景OLAP业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</li>
</ol>
</li>
<li><ol>
<li>滴滴olap现状<br>当前在滴滴内部,主要大数据olap生态包括clickhouse/druid/starrocks等,而写入olap可以划分为两个方向,分布是实时和离线:<ul>
<li>2.1 实时方向,包括网约车/金融等大部分业务侧均是使用kafka/ddmq作为source端,将数据通过flink实时攒批写入到大数据olap存储侧</li>
<li>2.2 离线方向,包括网约车/金融等大部分业务侧则是将数据通过hive/mysql/hdfs这个source端,经过各项加工后流入到olap这个sink存储侧</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>本文主题<br>本篇文章主要是介绍在starrocks这一侧,如果实现将hive数据进行加工后写入到starrock,涉及到starrokc的源码解析、组件介绍、及相关平台架构</li>
</ol>
</li>
</ul>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>如下图,是当前滴滴内部hive2sr导入的实现架构图,用户主要是通过访问同步中心,配置hive表和sr表的字段映射及默认值,同步中心会将映射关系通过srm开放的http接口传给srm侧,srm进行相关处理后提交给sr的各个组件,组件之间通过thrift server进行rpc通信,而srm则通过http方式进行任务最终状态监听,当任务完成后返回给数梦的任务调度系统,最终完成整个全链路流程.<br><img src="/images/starrocks/hive2sr/structure.png" alt="架构图"></p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ol>
<li>创建目标分区<br>同步中心会将hive2sr当前批次需要导入hive的分区列表传入srm,srm会校验当前sr表对应分区是否存在,不存在则按照hive分区值对应创建出sr表分区  </li>
<li>创建目标分区<br>上面这一步创建出目标分区后,srm会对应目标分区range范围,在sr目标表中创建出对应的临时分区,临时分区的作用是这样的,srm可以在一张已经定义分区规则的分区表上，创建临时分区，并为这些临时分区设定单独的数据分布策略。将hive数据写入指定的临时分区后,通过原子覆盖写操作(调整分区分桶策略)，srm可以将临时分区作为临时可用的数据载体覆盖到对应的目标分区,用以实现覆盖写操作</li>
<li>创建load任务<br>3.1 这个load首先需要配置涉及sr访问hadoop环境的相关参数,这个操作相当于在sr中配置hdfs-site.xml和core-site.xml<br>3.2 其实load任务需要组装出提交给spark的driver任务内容,通过cluster模式,以master on yarn的方式在fe节点提交出spark任务</li>
<li>fe进程调度etl阶段<br>在etl阶段,fe会开启spark driver等待yarn队列资源充足时,提交给yarn取执行spark任务,Spark集群执行ETL完成对导入数据的预处理。包括全局字典构建（BITMAP类型）、分区、排序、聚合等。预处理后的数据按parquet数据格式落盘HDFS存储侧。</li>
<li>etl完成后fe会调度load阶段<br>ETL 任务完成后，FE获取预处理过的每个分片的hdfs数据路径，并调度相关的 broker 执行 load 任务</li>
<li>broker加载hdfs文件后通知be进行push<br>BE 通过 Broker 进程读取 HDFS 数据，转化为 StarRocks 存储格式。此时临时分区完成对应hive分区数据的加载过程</li>
<li>将临时分区替换到目标分区<br>临时分区的数据将完成的替换到目标分区去,而目标分区数据将被删除,查询后将是新的hive分区数据</li>
</ol>
<h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><h4 id="fe进程启动及相关调度处理"><a href="#fe进程启动及相关调度处理" class="headerlink" title="fe进程启动及相关调度处理"></a>fe进程启动及相关调度处理</h4><ul>
<li><p>入口文件-&gt;StarRocksFE.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">feServer.start();</span><br><span class="line">httpServer.start();</span><br><span class="line">qeService.start();</span><br></pre></td></tr></table></figure>
<p>主要是初始化配置和启动服务，分别是mysql server端口、thrift server端口、http端口</p>
</li>
<li><p>mysq服务启动-&gt;QeService.java<br>由于我们都是通过tcp协议来连sr,所以主要关注QeService</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void start() throws IOException &#123;</span><br><span class="line">        if (!mysqlServer.start()) &#123;</span><br><span class="line">            LOG.error(&quot;mysql server start failed&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(&quot;QE service start.&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>MysqlServer.java<br>这里主要是开启mysql协议的服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public boolean start() &#123;</span><br><span class="line">    ......</span><br><span class="line">    &#x2F;&#x2F; 打开fe的mysql协议的socket管道</span><br><span class="line">    &#x2F;&#x2F; 开启一个常驻线程用以监听mysql协议</span><br><span class="line">    listener &#x3D; ThreadPoolManager.newDaemonCacheThreadPool(1, &quot;MySQL-Protocol-Listener&quot;, true);</span><br><span class="line">    running &#x3D; true;</span><br><span class="line">    listenerFuture &#x3D; listener.submit(new Listener());</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交本次连接的上下文到连接调度器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">........</span><br><span class="line">clientChannel &#x3D; serverChannel.accept();</span><br><span class="line">if (clientChannel &#x3D;&#x3D; null) &#123;</span><br><span class="line">    continue;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 初始化本次session的上下文信息到连接调度器</span><br><span class="line">&#x2F;&#x2F; submit this context to scheduler</span><br><span class="line">ConnectContext context &#x3D; new ConnectContext(clientChannel, sslContext);</span><br><span class="line">&#x2F;&#x2F; Set globalStateMgr here.</span><br><span class="line">context.setGlobalStateMgr(GlobalStateMgr.getCurrentState());</span><br><span class="line">if (!scheduler.submit(context)) &#123;</span><br><span class="line">    LOG.warn(&quot;Submit one connect request failed. Client&#x3D;&quot; + clientChannel.toString());</span><br><span class="line">    &#x2F;&#x2F; clear up context</span><br><span class="line">    context.cleanup();</span><br><span class="line">&#125;</span><br><span class="line">............</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#96;&#96;&#96;  </span><br><span class="line"></span><br><span class="line">+ 提交本次连接的上下文给线程池</span><br></pre></td></tr></table></figure>
<p>public boolean submit(ConnectContext context) {<br>  if (context == null) {</p>
<pre><code>  return false;
</code></pre><p>  }</p>
<p>  context.setConnectionId(nextConnectionId.getAndAdd(1));<br>  // no necessary for nio.<br>  if (context instanceof NConnectContext) {</p>
<pre><code>  return true;
</code></pre><p>  }<br>  // 这里是将Runnable提交到connect-scheduler-pool线程池<br>  if (executor.submit(new LoopHandler(context)) == null) {</p>
<pre><code>  LOG.warn(&quot;Submit one thread failed.&quot;);
  return false;
</code></pre><p>  }<br>  return true;<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoopHandler.java (实现Runnable接口)</span><br></pre></td></tr></table></figure>
<p>public void run() {<br>  …..<br>  // 注册本次连接,sr会计算当前fe节点总的连接数,在每次连接超过1024后进行sleep连接的驱逐流程<br>  if (registerConnection(context)) {</p>
<pre><code>  MysqlProto.sendResponsePacket(context);
</code></pre><p>  } else {</p>
<pre><code>  context.getState().setError(&quot;Reach limit of connections&quot;);
  MysqlProto.sendResponsePacket(context);
  return;
</code></pre><p>  }<br>  ………<br>  // 常驻,进行核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan<br>  ConnectProcessor processor = new ConnectProcessor(context);<br>  processor.loop();<br>  ……….<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; loop</span><br></pre></td></tr></table></figure>
<p>public void loop() {<br>  while (!ctx.isKilled()) {</p>
<pre><code>  try {
      processOnce();
  } catch (Exception e) {
      // TODO(zhaochun): something wrong
      LOG.warn(&quot;Exception happened in one seesion(&quot; + ctx + &quot;).&quot;, e);
      ctx.setKilled();
      break;
  }
</code></pre><p>  }<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; processOnce</span><br></pre></td></tr></table></figure>
<p>// handle one process<br>public void processOnce() throws IOException {<br>  // 重置上下文的状态<br>  ctx.getState().reset();<br>  executor = null;</p>
<p>  // 重置mysql协议的顺序标识符<br>  final MysqlChannel channel = ctx.getMysqlChannel();<br>  channel.setSequenceId(0);<br>  // 从通道里获取数据包<br>  try {</p>
<pre><code>  packetBuf = channel.fetchOnePacket();
  if (packetBuf == null) {
      throw new IOException(&quot;Error happened when receiving packet.&quot;);
  }
</code></pre><p>  } catch (AsynchronousCloseException e) {</p>
<pre><code>  // when this happened, timeout checker close this channel
  // killed flag in ctx has been already set, just return
  return;
</code></pre><p>  }</p>
<p>  // 调度,这里主要是上面介绍核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan过程<br>  dispatch();<br>  ………….<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; dispatch</span><br></pre></td></tr></table></figure>
<p>…….<br>// 这里主要是实现mysql协议的几种状态<br>switch (command) {<br>  case COM_INIT_DB:</p>
<pre><code>  handleInitDb();
  break;
</code></pre><p>  case COM_QUIT:</p>
<pre><code>  handleQuit();
  break;
</code></pre><p>  case COM_QUERY:<br>  // 这里是完整的sql处理的总入口</p>
<pre><code>  handleQuery();
  ctx.setStartTime();
  break;
</code></pre><p>  case COM_FIELD_LIST:</p>
<pre><code>  handleFieldList();
  break;
</code></pre><p>  case COM_CHANGE_USER:</p>
<pre><code>  handleChangeUser();
  break;
</code></pre><p>  case COM_RESET_CONNECTION:</p>
<pre><code>  handleResetConnnection();
  break;
</code></pre><p>  case COM_PING:</p>
<pre><code>  handlePing();
  break;
</code></pre><p>  default:</p>
<pre><code>  ctx.getState().setError(&quot;Unsupported command(&quot; + command + &quot;)&quot;);
  LOG.warn(&quot;Unsupported command(&quot; + command + &quot;)&quot;);
  break;
</code></pre><p>}<br>……</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; handleQuery</span><br></pre></td></tr></table></figure>
<p>….<br>StatementBase parsedStmt = null;<br>try {<br>  ctx.setQueryId(UUIDUtil.genUUID());<br>  List<StatementBase> stmts;<br>  try {</p>
<pre><code>  //通过antlr4进行sql的解析,获取sql解析ast树列表
  stmts = com.starrocks.sql.parser.SqlParser.parse(originStmt, ctx.getSessionVariable());
</code></pre><p>  } catch (ParsingException parsingException) {</p>
<pre><code>  throw new AnalysisException(parsingException.getMessage());
</code></pre><p>  }<br>  // 对ast语法树进行analyze分析过程<br>  for (int i = 0; i &lt; stmts.size(); ++i) {</p>
<pre><code>  ..........
  // Only add the last running stmt for multi statement,
  // because the audit log will only show the last stmt.
  if (i == stmts.size() - 1) {
      addRunningQueryDetail(parsedStmt);
  }

  executor = new StmtExecutor(ctx, parsedStmt);
  ctx.setExecutor(executor);

  ctx.setIsLastStmt(i == stmts.size() - 1);

  executor.execute();

  // 如果sql有一条执行失败,后续不再执行
  if (ctx.getState().getStateType() == QueryState.MysqlStateType.ERR) {
      break;
  }
  ........
</code></pre><p>  }<br>}<br>….</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#### 对sql进行Parser语法解析</span><br><span class="line">+ SqlParser.java -&gt; parser</span><br></pre></td></tr></table></figure>
<p>// 首先，我们需要初始化 StarRocksLexer，即词法解析器。在这里，StarRocksLexer 是根据上文介绍的StarRocksLex.g4 词法文件，使用 Antlr4 自动生成的代码类。<br>StarRocksLexer lexer = new StarRocksLexer(new CaseInsensitiveStream(CharStreams.fromString(sql)));<br>lexer.setSqlMode(sessionVariable.getSqlMode());<br>// 然后，代码将词法解析器 StarRocksLexer 作为参数，传入语法解析器中。语法解析器类StarRocksParser，同样是根据上文介绍的 StarRocks.g4 语法文件自动生成的代码类。<br>CommonTokenStream tokenStream = new CommonTokenStream(lexer);<br>StarRocksParser parser = new StarRocksParser(tokenStream);<br>// 到这里，我们就完成了语法解析类的构建。之后再调用 parser.addErrorListener(new ErrorHandler())，将 Antlr4 的默认错误处理规则，替换为自定义的错误处理逻辑即可。<br>parser.removeErrorListeners();<br>parser.addErrorListener(new ErrorHandler());<br>parser.removeParseListeners();<br>parser.addParseListener(new TokenNumberListener(sessionVariable.getParseTokensLimit(),<br>        Math.max(Config.expr_children_limit, sessionVariable.getExprChildrenLimit())));<br>……<br>List<StatementBase> statements = Lists.newArrayList();<br>// 调用 parser.sqlStatements() 返回值 StarRocksParser.SqlStatementsContext，这是一套 antlr 自定义的抽象语法树，根据语法文件生成。<br>List<StarRocksParser.SingleStatementContext> singleStatementContexts = parser.sqlStatements().singleStatement();<br>for (int idx = 0; idx &lt; singleStatementContexts.size(); ++idx) {<br>    // 将 antlr 的语法树转换为 StarRocks 的抽象语法树<br>    StatementBase statement = (StatementBase) new AstBuilder(sessionVariable.getSqlMode())<br>            .visitSingleStatement(singleStatementContexts.get(idx));<br>    statement.setOrigStmt(new OriginStatement(sql, idx));<br>    statements.add(statement);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StarRocks.g4 -&gt; loadStatement</span><br><span class="line">因为本文主要介绍的导入流程,所以以LoadStatement来举例</span><br></pre></td></tr></table></figure><br>loadStatement<br>    : LOAD LABEL label=labelName<br>        data=dataDescList?<br>        broker=brokerDesc?<br>        (BY system=identifierOrString)?<br>        (PROPERTIES props=propertyList)?<br>    | LOAD LABEL label=labelName<br>        data=dataDescList?<br>        resource=resourceDesc<br>        (PROPERTIES props=propertyList)?<br>    ;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">antlr 的语法文件采用 BNF 范式，用&#39;|&#39;表示分支选项，&#39;?&#39;表达0次或一次，其他符号可类比正则表达式。</span><br><span class="line">+ AstBuilder.java -&gt; visitLoadStatement</span><br><span class="line">Antlr4 会根据语法文件生成一份 Visitor 模式的代码，这样就可以做到动作代码与文法产生式解耦，利于文法产生式的重用。而自定义的 AstBuilder 文件则继承了 StarRocksBaseVisitor，用于将 antlr 内部的 AST 翻译成 StarRocks 自定义的 AST。</span><br></pre></td></tr></table></figure><br>public ParseNode visitLoadStatement(StarRocksParser.LoadStatementContext context) {<br>    // 解析出load里写的label名称<br>    LabelName label = getLabelName(context.labelName());<br>    // 解析出load里的字段映射默认值配置等描述<br>    List<DataDescription> dataDescriptions = null;<br>    if (context.data != null) {<br>        dataDescriptions = context.data.dataDesc().stream().map(this::getDataDescription)<br>                .collect(toList());<br>    }<br>    // 解析出load里关于spark任务配置的property属性<br>    Map<String, String> properties = null;<br>    if (context.props != null) {<br>        properties = Maps.newHashMap();<br>        List<Property> propertyList = visit(context.props.property(), Property.class);<br>        for (Property property : propertyList) {<br>            properties.put(property.getKey(), property.getValue());<br>        }<br>    }<br>    // 解析出load里引用的spark外部资源名称<br>    if (context.resource != null) {<br>        ResourceDesc resourceDesc = getResourceDesc(context.resource);<br>        return new LoadStmt(label, dataDescriptions, resourceDesc, properties);<br>    }<br>    // 解析出broker的配置<br>    BrokerDesc brokerDesc = getBrokerDesc(context.broker);<br>    String cluster = null;<br>    if (context.system != null) {<br>        cluster = ((Identifier) visit(context.system)).getValue();<br>    }<br>    //  visit 的返回值返回一个 AST 的基类，在StarRocks 中称为 ParseNode<br>    return new LoadStmt(label, dataDescriptions, brokerDesc, cluster, properties);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们对 Parser 源码进行了重点分析，包括 ANTLR4、SqlParser 和 ASTBuilder。与此同时，我们还通过一个例子，介绍了如何将一条文本类型的 SQL 语句，一步步解析成 StarRocks 内部使用的 AST。</span><br><span class="line">可以看到，Parser 能判断出用户的 SQL 中是否存在明显的语法错误，如 SQL 语句select * from;会在Parser 阶段报错。但如果 SQL 语句select * from foo;没有语法错误，StarRocks 中也没有 foo 这张表，那么 StarRocks 该如何做到错误处理呢？这就需要依赖下一节的 Analyzer 模块去判断了。</span><br><span class="line"></span><br><span class="line">#### 对sql进行analyzer语法解析</span><br><span class="line">+ StmtExecutor.java-&gt; executor</span><br></pre></td></tr></table></figure><br>……..<br>// 本文主要介绍load流程,所以这里值关注ddl的词法分析过程<br>} else if (parsedStmt instanceof DdlStmt) {<br>    handleDdlStmt();<br>}<br>……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br> if (parsedStmt instanceof ShowStmt) {<br>    com.starrocks.sql.analyzer.Analyzer.analyze(parsedStmt, context);<br>    PrivilegeChecker.check(parsedStmt, context);</p>
<pre><code>QueryStatement selectStmt = ((ShowStmt) parsedStmt).toSelectStmt();
if (selectStmt != null) {
    parsedStmt = selectStmt;
    execPlan = StatementPlanner.plan(parsedStmt, context);
}
</code></pre><p> } else {<br>    // 这里是analyze解析的入口函数<br>        execPlan = StatementPlanner.plan(parsedStmt, context);<br> }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StatementPlanner.java -&gt; plan</span><br></pre></td></tr></table></figure><br>// 注意在analyze阶段,也是会进行锁库操作(db.readLock();)<br>lock(dbs);<br>try (PlannerProfile.ScopedTimer ignored = PlannerProfile.getScopedTimer(“Analyzer”)) {<br>    Analyzer.analyze(stmt, session);<br>}</p>
<p>PrivilegeChecker.check(stmt, session);<br>if (stmt instanceof QueryStatement) {<br>    OptimizerTraceUtil.logQueryStatement(session, “after analyze:\n%s”, (QueryStatement) stmt);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Analyzer 类是所有语句的语义解析的主入口，采用了 Visitor 设计模式，会根据处理的语句类型的不同，调用不同语句的 Analyzer。不同语句的处理逻辑会包含在单独的语句 Analyzer 中，交由不同的 Analyzer 处理</span><br><span class="line"></span><br><span class="line">+ LoadExecutor.java -&gt; accept</span><br></pre></td></tr></table></figure><br>public <R, C> R accept(AstVisitor<R, C> visitor, C context) {<br>    return visitor.visitLoadStatement(this, context);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;real_analyze.png)</span><br><span class="line"></span><br><span class="line">+ LoadStmtAnalyzer.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public Void visitLoadStatement(LoadStmt statement, ConnectContext context) {<br>    analyzeLabel(statement, context);<br>    analyzeDataDescriptions(statement);<br>    analyzeProperties(statement);<br>    return null;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如上所示,LoadStatement交由LoadStmtAnalyzer来专门处理解析,这样做的优点是，可以将 Statement 的定义文件和 Analyze 的处理逻辑分开，并且同一类型的语句也交由特定的 Analyzer 处理，做到不同功能之间代码的解耦合。</span><br><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br>  ……..<br>// 这里是ddl 执行分析的总入口<br>ShowResultSet resultSet = DDLStmtExecutor.execute(parsedStmt, context);<br>  ……..<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; execute</span><br></pre></td></tr></table></figure><br>……..<br>// ddl statement analyze 处理逻辑的基类<br>return stmt.accept(StmtExecutorVisitor.getInstance(), context);<br>……..<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;analyze.png)</span><br><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public ShowResultSet visitLoadStatement(LoadStmt stmt, ConnectContext context) {<br>    ErrorReport.wrapWithRuntimeException(() -&gt; {<br>        EtlJobType jobType = stmt.getEtlJobType();<br>        if (jobType == EtlJobType.UNKNOWN) {<br>            throw new DdlException(“Unknown load job type”);<br>        }<br>        if (jobType == EtlJobType.HADOOP &amp;&amp; Config.disable_hadoop_load) {<br>            throw new DdlException(“Load job by hadoop cluster is disabled.”</p>
<pre><code>                + &quot; Try using broker load. See &#39;help broker load;&#39;&quot;);
    }
    // 这里进行真正的创建load任务的分析过程
    context.getGlobalStateMgr().getLoadManager().createLoadJobFromStmt(stmt, context);
});
return null;
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### 创建load处理pending阶段</span><br><span class="line"></span><br><span class="line">+ LoadManager.java -&gt; createLoadJobFromStmt</span><br></pre></td></tr></table></figure><br>public void createLoadJobFromStmt(LoadStmt stmt) throws DdlException {<br>    // 校验库元数据<br>  Database database = checkDb(stmt.getLabel().getDbName());<br>  long dbId = database.getId();<br>  LoadJob loadJob = null;<br>  // 加写锁,防止并发<br>  writeLock();<br>  try {<br>    // 校验lable是否已使用<br>      checkLabelUsed(dbId, stmt.getLabel().getLabelName());<br>      if (stmt.getBrokerDesc() == null &amp;&amp; stmt.getResourceDesc() == null) {<br>          throw new DdlException(“LoadManager only support the broker and spark load.”);<br>      }<br>      // 判断queue队列长度是否超过1024,超过就报错<br>      if (loadJobScheduler.isQueueFull()) {<br>          throw new DdlException(<br>                  “There are more than “ + Config.desired_max_waiting_jobs + “ load jobs in waiting queue, “</p>
<pre><code>                      + &quot;please retry later.&quot;);
  }
  // 按load类型初始化出LoadJob
  loadJob = BulkLoadJob.fromLoadStmt(stmt);
  // 在内存中记录该load元数据
  createLoadJob(loadJob);
</code></pre><p>  } finally {<br>      writeUnlock();<br>  }<br>  // 通知bdbje加入load元数据同步<br>  Catalog.getCurrentCatalog().getEditLog().logCreateLoadJob(loadJob);</p>
<p>  // loadJob守护进程从load job schedule queue取出任务去执行<br>  loadJobScheduler.submitJob(loadJob);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过 &#96;createLoadJobFromStmt&#96; 创建load任务</span><br><span class="line">+ LoadJobScheduler.java -&gt; process</span><br><span class="line">注意：LoadJobScheduler 继承自 MasterDaemon，MasterDaemon 继承自 Daemon，</span><br><span class="line">Daemon继承自Thread，重载了run方法，里面有一个loop，主要执行runOneCycle</span><br><span class="line">MasterDaemon 又重写了 runOneCycle，执行 runAfterCatalogReady 函数</span><br><span class="line">LoadJobScheduler 又重写了 runAfterCatalogReady 主要就是干process处理，里面是一个死循环，不断从LinkedBlockingQueue类型的needScheduleJobs里出栈取要执行的job</span><br></pre></td></tr></table></figure><br>while (true) {<br>  // take one load job from queue<br>  LoadJob loadJob = needScheduleJobs.poll();<br>  if (loadJob == null) {<br>      return;<br>  }</p>
<p>  // schedule job<br>  try {<br>      loadJob.execute();<br>  }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadJob.java -&gt; execute</span><br></pre></td></tr></table></figure><br>public void execute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>        DuplicatedRequestException, LoadException {<br>            // 加写锁,这里防止多并发提交<br>    writeLock();<br>    try {<br>        unprotectedExecute();<br>    } finally {<br>        writeUnlock();<br>    }<br>}<br>// 这里主要关注beginTxn,做了事务处理,目的是防止fe节点突然挂掉再拉起后其他follower节点切换到leader节点时会重新执行这个任务,而这个时候旧leader节点已经执行过一段这个任务,导致fe节点之间执行无法同步,所以直接抛出label已经存在不再继续执行<br>public void unprotectedExecute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>            DuplicatedRequestException, LoadException {<br>    // check if job state is pending<br>    if (state != JobState.PENDING) {<br>        return;<br>    }<br>    // the limit of job will be restrict when begin txn<br>    beginTxn();<br>    unprotectedExecuteJob();<br>    // update spark load job state from PENDING to ETL when pending task is finished<br>    if (jobType != EtlJobType.SPARK) {<br>        unprotectedUpdateState(JobState.LOADING);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadJob.java -&gt; unprotectedExecuteJob</span><br></pre></td></tr></table></figure><br>protected void unprotectedExecuteJob() throws LoadException {<br>    // create pending task<br>    LoadTask task = new SparkLoadPendingTask(this, fileGroupAggInfo.getAggKeyToFileGroups(),<br>            sparkResource, brokerDesc);<br>    task.init();<br>    idToTasks.put(task.getSignature(), task);<br>    // 注意这里的线程池,初始化时只有5个核心线程(最大线程数),而队列长度只有1024,这里会影响任务长期处于pending状态,如果当前5个任务在下面的spark driver阶段一直等不到yarn资源,则一直处于这个线程执行状态,其他提交进来的任务执行在queue队列中等待<br>    submitTask(Catalog.getCurrentCatalog().getPendingLoadTaskScheduler(), task);<br>    ……….<br>    // this.pendingLoadTaskScheduler = new LeaderTaskExecutor(“pending_load_task_scheduler”, Config.max_broker_load_job_concurrency,Config.desired_max_waiting_jobs, !isCheckpointCatalog);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; init</span><br><span class="line">&#x2F;&#x2F; 初始化任务的配置参数</span><br></pre></td></tr></table></figure><br>public void init() throws LoadException {<br>    createEtlJobConf();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadTask -&gt; exec</span><br><span class="line">&#x2F;&#x2F; 由于SparkLoadTask继承自LoadTask,而LoadTask实现了PriorityRunnable接口,所以上面的submitTask实际上就是自动执行这里的exec方法</span><br></pre></td></tr></table></figure><br>@Override<br>protected void exec() {<br>    boolean isFinished = false;<br>    try {<br>        // execute pending task<br>        executeTask();<br>        // callback on pending task finished<br>        callback.onTaskFinished(attachment);<br>        isFinished = true;<br>    ………<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### load任务处理etl阶段</span><br><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; executeTask</span><br></pre></td></tr></table></figure><br>void executeTask() throws LoadException {<br>    LOG.info(“begin to execute spark pending task. load job id: {}”, loadJobId);<br>    submitEtlJob();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>private void submitEtlJob() throws LoadException {<br>    SparkPendingTaskAttachment sparkAttachment = (SparkPendingTaskAttachment) attachment;<br>    // 配置etl阶段清洗出的文件在hdfs上的存放路径<br>    etlJobConfig.outputPath = EtlJobConfig.getOutputPath(resource.getWorkingDir(), dbId, loadLabel, signature);<br>    sparkAttachment.setOutputPath(etlJobConfig.outputPath);</p>
<pre><code>// 提交etl任务
SparkEtlJobHandler handler = new SparkEtlJobHandler();
handler.submitEtlJob(loadJobId, loadLabel, etlJobConfig, resource, brokerDesc, sparkLoadAppHandle,
        sparkAttachment);
LOG.info(&quot;submit spark etl job success. load job id: {}, attachment: {}&quot;, loadJobId, sparkAttachment);
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkEtlJobHandler.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,<br>                             BrokerDesc brokerDesc, SparkLoadAppHandle handle, SparkPendingTaskAttachment attachment)<br>      throws LoadException {<br>  // delete outputPath<br>  // init local dir<br>  // prepare dpp archive<br>  SparkLauncher launcher = new SparkLauncher(envs);<br>  // master      |  deployMode<br>  // ——————|——————-<br>  // yarn        |  cluster<br>  // spark://xx  |  client<br>  launcher.setMaster(resource.getMaster())<br>          .setDeployMode(resource.getDeployMode().name().toLowerCase())<br>          .setAppResource(appResourceHdfsPath)<br>          .setMainClass(SparkEtlJob.class.getCanonicalName())<br>          .setAppName(String.format(ETL_JOB_NAME, loadLabel))<br>          .setSparkHome(sparkHome)<br>          .addAppArgs(jobConfigHdfsPath)<br>          .redirectError();</p>
<p>  // spark configs</p>
<p>  // start app<br>  State state = null;<br>  String appId = null;<br>  String logPath = null;<br>  String errMsg = “start spark app failed. error: “;<br>  try {<br>     // 提交spark任务给yarn<br>      Process process = launcher.launch();<br>      handle.setProcess(process);<br>      if (!FeConstants.runningUnitTest) {<br>        // 监听spark driver输出的日志,直到任务被yarn接收后,退出driver进程<br>          SparkLauncherMonitor.LogMonitor logMonitor = SparkLauncherMonitor.createLogMonitor(handle);<br>          logMonitor.setSubmitTimeoutMs(GET_APPID_TIMEOUT_MS);<br>          logMonitor.setRedirectLogPath(logFilePath);<br>          logMonitor.start();<br>          try {<br>              logMonitor.join();<br>          } catch (InterruptedException e) {<br>              logMonitor.interrupt();<br>              throw new LoadException(errMsg + e.getMessage());<br>          }<br>      }<br>      appId = handle.getAppId();<br>      state = handle.getState();<br>      logPath = handle.getLogPath();<br>  } catch (IOException e) {<br>      LOG.warn(errMsg, e);<br>      throw new LoadException(errMsg + e.getMessage());<br>  }<br>……….<br>  // success<br>  attachment.setAppId(appId);<br>  attachment.setHandle(handle);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### load任务处理loading阶段</span><br><span class="line">+ 1、spark load 有一个 LoadEtlChecker定时调度任务,每5s执行一次</span><br></pre></td></tr></table></figure><br>protected void runAfterCatalogReady() {<br>    try {<br>        loadManager.processEtlStateJobs();<br>    } catch (Throwable e) {<br>        LOG.warn(“Failed to process one round of LoadEtlChecker with error message {}”, e.getMessage(), e);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 2、这个定时调度就是定时检查load任务状态,有变化就去更新,简单说就是load状态扭转控制器</span><br></pre></td></tr></table></figure><br>// only for those jobs which have etl state, like SparkLoadJob<br>public void processEtlStateJobs() {<br>    idToLoadJob.values().stream().filter(job -&gt; (job.jobType == EtlJobType.SPARK &amp;&amp; job.state == JobState.ETL))<br>            .forEach(job -&gt; {<br>                try {<br>                    ((SparkLoadJob) job).updateEtlStatus();<br>                } catch (DataQualityException e) {<br>                    LOG.info(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(FailMsg.CancelType.ETL_QUALITY_UNSATISFIED,<br>                                    DataQualityException.QUALITY_FAIL_MSG),<br>                            true, true);<br>                } catch (TimeoutException e) {<br>                    // timeout, retry next time<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                } catch (UserException e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(CancelType.ETL_RUN_FAIL, e.getMessage()), true, true);<br>                } catch (Exception e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                }<br>            });<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 3、当任务状态走到loading时,会提交load任务,函数是submitPushTasks(),这里简单说就是把表的排序健&#x2F;分区&#x2F;be副本等查询出来,后面要把这些元数据通过rpc调用be来处理</span><br></pre></td></tr></table></figure><br>if (tablet instanceof LocalTablet) {<br>    for (Replica replica : ((LocalTablet) tablet).getImmutableReplicas()) {<br>        long replicaId = replica.getId();<br>        tabletAllReplicas.add(replicaId);<br>        long backendId = replica.getBackendId();<br>        Backend backend = GlobalStateMgr.getCurrentState().getCurrentSystemInfo()<br>                .getBackend(backendId);</p>
<pre><code>    pushTask(backendId, tableId, partitionId, indexId, tabletId,
            replicaId, schemaHash, params, batchTask, tabletMetaStr,
            backend, replica, tabletFinishedReplicas, TTabletType.TABLET_TYPE_DISK);
}

if (tabletAllReplicas.size() == 0) {
    LOG.error(&quot;invalid situation. tablet is empty. id: {}&quot;, tabletId);
}

// check tablet push states
if (tabletFinishedReplicas.size() &gt;= quorumReplicaNum) {
    quorumTablets.add(tabletId);
    if (tabletFinishedReplicas.size() == tabletAllReplicas.size()) {
        fullTablets.add(tabletId);
    }
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、遍历生成出的每个tablet,再去遍历每个副本replica,都会给每个replica提交一个load任务,那这里要注意的是会构造</span><br></pre></td></tr></table></figure><br>PushTask pushTask = new PushTask(backendId, dbId, tableId, partitionId,<br>        indexId, tabletId, replicaId, schemaHash,<br>        0, id, TPushType.LOAD_V2,<br>        TPriority.NORMAL, transactionId, taskSignature,<br>        tBrokerScanRange, params.tDescriptorTable,<br>        params.useVectorized, timezone, tabletType);<br>if (AgentTaskQueue.addTask(pushTask)) {<br>    batchTask.addTask(pushTask);<br>    if (!tabletToSentReplicaPushTask.containsKey(tabletId)) {<br>        tabletToSentReplicaPushTask.put(tabletId, Maps.newHashMap());<br>    }<br>    tabletToSentReplicaPushTask.get(tabletId).put(replicaId, pushTask);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、会通过thrift rpc 通知这个副本的be,主要是这个地方client.submit_tasks(agentTaskRequests);</span><br></pre></td></tr></table></figure><br> // create AgentClient<br>address = new TNetworkAddress(backend.getHost(), backend.getBePort());<br>client = ClientPool.backendPool.borrowObject(address);<br>List<TAgentTaskRequest> agentTaskRequests = new LinkedList<TAgentTaskRequest>();<br>for (AgentTask task : tasks) {<br>    agentTaskRequests.add(toAgentTaskRequest(task));<br>}<br>client.submit_tasks(agentTaskRequests);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、这个submit_tasks方法主要是调用BackendService类,这个类实现在be这一侧</span><br></pre></td></tr></table></figure><br>public com.starrocks.thrift.TAgentResult submit_tasks(java.util.List<com.starrocks.thrift.TAgentTaskRequest> tasks) throws org.apache.thrift.TException<br>{<br>    send_submit_tasks(tasks);<br>    return recv_submit_tasks();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个submit_tasks方法是 Thrift 生成的 Java 客户端代码。它和 C++ 服务端的对应关系是:</span><br><span class="line">1) fe发送 Thrift RPC 请求,调用这个 Java submit_tasks方法。</span><br><span class="line">2) Thrift框架会将参数 tasks 序列化成数据包,发送给服务端be。</span><br><span class="line">3) be服务端 Thrift 接收到数据包,反序列化出 tasks 参数。</span><br><span class="line">4) 根据方法名submit_tasks映射到 C++ 的 BackendService::submit_tasks 方法,将 tasks 作为参数调用它。</span><br><span class="line">5) C++ 方法执行完成,将返回值通过 Thrift 序列化后发送给客户端。</span><br><span class="line">6) Java 端 recv_submit_tasks 反序列化出返回结果,赋值给 submit_tasks 方法的返回值。</span><br><span class="line">7) submit_tasks 返回结果,Java 前端获得调用结果。</span><br><span class="line"></span><br><span class="line">#### be部分如何处理loading的代码逻辑</span><br><span class="line">+ 1、BackendService的rpc服务在启动be时已开启</span><br></pre></td></tr></table></figure><br>// Begin to start services<br>// 1. Start thrift server with ‘be_port’.<br>auto thrift_server = BackendService::create<BackendService>(exec_env, starrocks::config::be_port);<br>if (auto status = thrift_server-&gt;start(); !status.ok()) {<br>    LOG(ERROR) &lt;&lt; “Fail to start BackendService thrift server on port “ &lt;&lt; starrocks::config::be_port &lt;&lt; “: “<br>                &lt;&lt; status;<br>    starrocks::shutdown_logging();<br>    exit(1);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 2、客户端BackendServiceClient::submit_tasks()会序列化tasks参数,然后发送RPC请求到服务端,服务端的RPC框架收到请求,根据方法名映射到BackendService::submit_tasks()</span><br></pre></td></tr></table></figure><br>void BackendService::submit_tasks(TAgentResult&amp; return_value, const std::vector<TAgentTaskRequest>&amp; tasks) {<br>    _agent_server-&gt;submit_tasks(return_value, tasks);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 3、而agent_server里处理submit_tasks的核心逻辑是再次调用PushTaskWorkerPool下的submit_tasks</span><br></pre></td></tr></table></figure><br>// batch submit push tasks<br>if (!push_divider.empty()) {<br>    LOG(INFO) &lt;&lt; “begin batch submit task: “ &lt;&lt; tasks[0].task_type;<br>    for (const auto&amp; push_item : push_divider) {<br>        const auto&amp; push_type = push_item.first;<br>        auto all_push_tasks = push_item.second;<br>        switch (push_type) {<br>        case TPushType::LOAD_V2:<br>            _push_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        case TPushType::DELETE:<br>        case TPushType::CANCEL_DELETE:<br>            _delete_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        default:<br>            ret_st = Status::InvalidArgument(strings::Substitute(“tasks(type=$0, push_type=$1) has wrong task type”,<br>                                                                    TTaskType::PUSH, push_type));<br>            LOG(WARNING) &lt;&lt; “fail to batch submit push task. reason: “ &lt;&lt; ret_st.get_error_msg();<br>        }<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、在TaskWorkPool里处理submit_tasks的,主要是实现批量注册task信息,做去重和幂等校验,成功后进行遍历,核心函数是_convert_task</span><br></pre></td></tr></table></figure><br>for (size_t i = 0; i &lt; task_count; i++) {<br>    if (failed_task[i] == 0) {<br>        auto new_task = _convert_task(*tasks[i], recv_time);<br>        _tasks.emplace_back(std::move(new_task));<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、_convert_task它会将TAgentTaskRequest转换成PushReqAgentTaskRequest类型的任务请求,这个是通用线程池,构造函数里回调了_worker_thread_callback,构造出的EngineBatchLoadTask对象后取执行task</span><br></pre></td></tr></table></figure><br>EngineBatchLoadTask engine_task(push_req, &amp;tablet_infos, agent_task_req-&gt;signature, &amp;status,<br>                                ExecEnv::GetInstance()-&gt;load_mem_tracker());<br>StorageEngine::instance()-&gt;execute_task(&amp;engine_task);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、在EngineBatchLoadTask::execute()里追踪了内存使用后,会检查tablet是否存在,目录是否达到存储限制等,继续执行load_v2(sparkload&#x2F;brokerload)的_process,而_process也是做了下校验后提交到_push里</span><br></pre></td></tr></table></figure><br>if (status == STARROCKS_SUCCESS) {<br>    uint32_t retry_time = 0;<br>    while (retry_time &lt; PUSH_MAX_RETRY) {<br>        status = _process();</p>
<pre><code>    if (status == STARROCKS_PUSH_HAD_LOADED) {
        LOG(WARNING) &lt;&lt; &quot;transaction exists when realtime push, &quot;
                        &quot;but unfinished, do not report to fe, signature: &quot;
                        &lt;&lt; _signature;
        break; // not retry anymore
    }
    // Internal error, need retry
    if (status == STARROCKS_ERROR) {
        LOG(WARNING) &lt;&lt; &quot;push internal error, need retry.signature: &quot; &lt;&lt; _signature;
        retry_time += 1;
    } else {
        break;
    }
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 7、在_push()里初始化了PushHandler类,现在才开始处理数据的摄入过程</span><br></pre></td></tr></table></figure><br>vectorized::PushHandler push_handler;<br>res = push_handler.process_streaming_ingestion(tablet, request, type, tablet_info_vec);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 8、在核心逻辑_do_streaming_ingestion函数里,开始检查这个tablet是否能迁移,能的话,拿回tablet的写锁后做load转换过程</span><br></pre></td></tr></table></figure><br>Status st = Status::OK();<br>if (push_type == PUSH_NORMAL_V2) {<br>    st = _load_convert(tablet_vars-&gt;at(0).tablet, &amp;(tablet_vars-&gt;at(0).rowset_to_add));<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 9、而在_load_convert里就是初始化出RowsetWriterContext这个上下文对象和RowsetWriter,RowsetWriter对象主要是需要去读出tablet的每一行,用这个wirter写回</span><br></pre></td></tr></table></figure><br>st = reader-&gt;init(t_scan_range, _request);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “fail to init reader. res=” &lt;&lt; st.to_string() &lt;&lt; “, tablet=” &lt;&lt; cur_tablet-&gt;full_name();<br>    return st;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 10、而reader的初始化就是构造PushBrokerReader对象,在PushBrokerReader里会按照文件类型构造出扫描对象,们目前通过sparkload处理的数据,都是使用parquet格式,所以这个初始化就是初始化出ParquetScanner</span><br></pre></td></tr></table></figure><br>// init scanner<br>FileScanner* scanner = nullptr;<br>switch (t_scan_range.ranges[0].format_type) {<br>case TFileFormatType::FORMAT_PARQUET: {<br>    scanner = new ParquetScanner(_runtime_state.get(), _runtime_profile, t_scan_range, _counter.get());<br>    if (scanner == nullptr) {<br>        return Status::InternalError(“Failed to create scanner”);<br>    }<br>    break;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 11、再回到PushHandler里的_load_conver函数的读取tablet过程,也是按行来去读一个个chunk,而这个next_chunk在PushBrokerReader里又是如何处理的呢?</span><br></pre></td></tr></table></figure><br>ChunkPtr chunk = ChunkHelper::new_chunk(schema, 0);<br>while (!reader-&gt;eof()) {<br>    st = reader-&gt;next_chunk(&amp;chunk);<br>    if (!st.ok()) {<br>        LOG(WARNING) &lt;&lt; “fail to read next chunk. err=” &lt;&lt; st.to_string() &lt;&lt; “, read_rows=” &lt;&lt; num_rows;<br>        return st;<br>    } else {<br>        if (reader-&gt;eof()) {<br>            break;<br>        }</p>
<pre><code>    st = rowset_writer-&gt;add_chunk(*chunk);
    if (!st.ok()) {
        LOG(WARNING) &lt;&lt; &quot;fail to add chunk to rowset writer&quot;
                        &lt;&lt; &quot;. res=&quot; &lt;&lt; st &lt;&lt; &quot;, tablet=&quot; &lt;&lt; cur_tablet-&gt;full_name()
                        &lt;&lt; &quot;, read_rows=&quot; &lt;&lt; num_rows;
        return st;
    }

    num_rows += chunk-&gt;num_rows();
    chunk-&gt;reset();
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 12、next_chunk函数会通过scanner对象,去读每行,直到读完为止,再把读出来的结果转换成chunk</span><br></pre></td></tr></table></figure><br>auto res = _scanner-&gt;get_next();<br>if (res.status().is_end_of_file()) {<br>    _eof = true;<br>    return Status::OK();<br>} else if (!res.ok()) {<br>    return res.status();<br>}</p>
<p>return _convert_chunk(res.value(), chunk);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 13、在parquet_scanner里是如何读每个行的呢?当然是按批来读,把读出的批追加到chunk去,chunk满了就返回,这种方式就是通用的流式数据处理,避免了一次新把整个parquet读到内存里,在这里主要关注到next_batch函数,就是Parquet文件的读取流程</span><br></pre></td></tr></table></figure><br>const TBrokerRangeDesc&amp; range_desc = _scan_range.ranges[_next_file];<br>Status st = create_random_access_file(range_desc, _scan_range.broker_addresses[0], _scan_range.params,<br>                                        CompressionTypePB::NO_COMPRESSION, &amp;file);</p>
<p>14、而在create_random_access_file函数由于parquet_scanner是继承自file_scanner,统一回来按文件类型去创建RandomAccessFile的逻辑<br>int64_t timeout_ms = _state-&gt;query_options().query_timeout * 1000 / 4;<br>timeout_ms = std::max(timeout_ms, static_cast<int64_t>(DEFAULT_TIMEOUT_MS));<br>BrokerFileSystem fs_broker(address, params.properties, timeout_ms);<br>if (config::use_local_filecache_for_broker_random_access_file) {<br>    ASSIGN_OR_RETURN(auto broker_file, fs_broker.new_sequential_file(range_desc.path));</p>
<pre><code>std::string dest_path = create_tmp_file_path();
LOG(INFO) &lt;&lt; &quot;broker load cache file: &quot; &lt;&lt; dest_path;
ASSIGN_OR_RETURN(auto dest_file, FileSystem::Default()-&gt;new_writable_file(dest_path));

auto res = fs::copy(broker_file.get(), dest_file.get(), 10 * 1024 * 1024);
std::shared_ptr&lt;RandomAccessFile&gt; local_file;
ASSIGN_OR_RETURN(local_file, FileSystem::Default()-&gt;new_random_access_file(dest_path));
src_file = std::make_shared&lt;TempRandomAccessFile&gt;(dest_path, local_file);
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 15、使用BrokerFileSystem通过broker来打开文件(这里的address就是我们在fe节点里创建tBrokerScanRange传入的),用broker把hdfs上的文件下载到本地临时目录下,再打开本地临时目录</span><br></pre></td></tr></table></figure><br>Status st = call_method(_broker_addr, &amp;BrokerServiceClient::openReader, request, &amp;response);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; st;<br>    return st;<br>}<br>if (response.opStatus.statusCode != TBrokerOperationStatusCode::OK) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; response.opStatus.message;<br>    return to_status(response.opStatus);<br>}</p>
<p>// Get file size.<br>ASSIGN_OR_RETURN(const uint64_t file_size, get_file_size(path));<br>auto stream = std::make_shared<BrokerInputStream>(_broker_addr, response.fd, file_size);<br>return std::make_unique<SequentialFile>(std::move(stream), path);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">注意,这里又是通过rpc来调用broker了</span><br><span class="line"></span><br><span class="line">+ 16、再回来看parquet_scanner里的get_next()函数,就是整个parquet文件的核心逻辑,而核心里的调用就是初始化chunk的逻辑</span><br></pre></td></tr></table></figure><br>Status ParquetScanner::initialize_src_chunk(ChunkPtr<em> chunk) {<br>    SCOPED_RAW_TIMER(&amp;_counter-&gt;init_chunk_ns);<br>    _pool.clear();<br>    (</em>chunk) = std::make_shared<Chunk>();<br>    size_t column_pos = 0;<br>    _chunk_filter.clear();<br>    for (auto i = 0; i &lt; _num_of_columns_from_file; ++i) {<br>        SlotDescriptor<em> slot_desc = _src_slot_descriptors[i];<br>        if (slot_desc == nullptr) {<br>            continue;<br>        }<br>        auto</em> array = _batch-&gt;column(column_pos++).get();<br>        ColumnPtr column;<br>        RETURN_IF_ERROR(new_column(array-&gt;type().get(), slot_desc, &amp;column, &amp;_conv_funcs[i], &amp;_cast_exprs[i]));<br>        column-&gt;reserve(_max_chunk_size);<br>        (*chunk)-&gt;append_column(column, slot_desc-&gt;id());<br>    }<br>    return Status::OK();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1) 清空内存池</span><br><span class="line">2) 创建新的空chunk</span><br><span class="line">3) 遍历所有columns:</span><br><span class="line">  3.1、如果column在schema中,获取对应的SlotDescriptor</span><br><span class="line">  3.2、根据数组类型和SlotDescriptor创建新的Column</span><br><span class="line">  3.3、预留chunk大小内存</span><br><span class="line">  3.4、将Column添加到chunk</span><br><span class="line">4) 返回OK状态</span><br><span class="line">这样就构造了一个空的、符合schema的chunk。</span><br><span class="line">主要逻辑是:</span><br><span class="line">- 清理内存池重用</span><br><span class="line">- 创建空chunk</span><br><span class="line">- 为每个column创建对应的Column对象</span><br><span class="line">- 预留内存</span><br><span class="line">- 添加到chunk</span><br><span class="line">这是非常标准的按schema构建chunk的过程。</span><br><span class="line">这种实现可以高效构建chunk,同时预留内存减少后续内存分配,并可以重用内存池避免频繁new&#x2F;delete。</span><br><span class="line">构建一个符合schema并预分配内存的空chunk是vectorized执行的基础,这个初始化实现是非常重要的一步</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 演进之路</span><br><span class="line">&gt; 上面主要解读了整个离线导入过程的核心代码,在离线导入模块集成到同步中心后,也遇到了一些问题,进行了3次升级来支持不同业务侧对离线导入的需求,下面主要介绍下几个大feature的升级</span><br><span class="line"></span><br><span class="line">#### 国际化业务需要字符串分区</span><br><span class="line">滴滴内部的国际化业务,由于不同国家有时区问题,所以业务侧在hive建表时总会创建出dt&#x3D;yyyy-mm-dd&#x2F;country_code&#x3D;xx的表结构,但如果要将这种hive表结构导入到sr,在当前官方版本都是不支持的,一直到3.0+版本我们和社区一起共建了string类型分区这个feature,并且在离线导入这一块进行了深度迭代升级,用以支撑国际化业务的快速落地sr,这个feature我们已经和入到官方版本中,当前社区版本需要在3.0+才可以支持string类型分区,而滴滴内部在2.3和2.5都已经支持了string类型分区功能,如下表结构</span><br></pre></td></tr></table></figure><br>CREATE TABLE <code>db</code>.<code>tb</code> (<br>  <code>is_passenger</code> varchar(65533),<br>  <code>country_code</code> varchar(65533),<br>  <code>dt</code> varchar(65533),<br>  <code>pid</code> varchar(65533),<br>  <code>resource_stage_1_show_cnt</code> bigint(20)<br>) ENGINE=OLAP<br>DUPLICATE KEY(<code>is_passenger</code>, <code>country_code</code>, <code>dt</code>)<br>COMMENT “xxxx”<br>PARTITION BY LIST(<code>country_code</code>,<code>dt</code>)(<br>  PARTITION pMX_20231208 VALUES IN ((‘MX’, ‘2023-12-08’))<br>)<br>DISTRIBUTED BY HASH(<code>pid</code>) BUCKETS 48<br>PROPERTIES (<br>“replication_num” = “3”,<br>“in_memory” = “false”,<br>“storage_format” = “DEFAULT”,<br>“enable_persistent_index” = “false”<br>);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### spark任务提交由于yarn资源缺乏只能等待5分钟便终止等待load退出</span><br><span class="line">滴滴内部yarn资源队列core和memory均做了一定限制,在降本增效的前提下,很多业务侧无法再去扩容yarn队列,而sr官方社区版本在spark任务提交给yarn之后,默认只给等待5分钟,如果始终等不到yarn接收任务,则自动退出,针对这种特定场景,我们优化了load任务的执行流程,引入了&#96;&quot;spark_load_submit_timeout&quot; &#x3D; &quot;7200&quot;&#96;参数,在每个load任务提交后都可以自动化配置这个任务需要等待yarn多少时间,并且如果spark任务确实等待超时后无法提交,也加入了通知yarn去杀死这个等待的spark任务,防止后续等待到yarn资源后又再次提交执行,而sr这一侧load已经退出,白白浪费资源,如下load结构</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.lable_name_xxx (<br>  DATA INFILE(<br>    “hdfs://DClusterUS1/xxxxx”<br>  )<br>  INTO TABLE tb<br>  TEMPORARY PARTITION(temp<strong>partition)<br>  FORMAT AS “ORC”<br>  (<code>column_1</code>, <code>column_2</code>)<br>  SET (<br>    <code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br>    <code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>    <code>dt</code> = “2023-12-09”,<br>    <code>country_code</code> = “MX”<br>)<br>) WITH RESOURCE ‘external_spark_resource’ (<br>  “spark.yarn.tags” = “xxxxxxx”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 由于spark driver进程在fe节点中占用过多内存,会导致cgroup自动杀死fe进程</span><br><span class="line">目前我们的k8s集群对fe节点的配置只有12g,所以大批量spark driver进程在fe节点中,按照之前统计每个dirver进程大约会占用200-300mb,而fe进程的元数据占用内存大约在5g-10g之间,所以留给spark driverfe进程的可用内存其实非常少,但是目前大量业务侧使用离线导入功能,这样就会造成fe进程不定时被kill的困局,针对这个场景,我们开发了load限流功能,将并发提交的任务转入限流队列中,由队列资源来控制当前任务的并行度,在fe节点内存资源达到高峰期时,进行相应的限流策略,用以保障fe节点的稳定性,如下所示</span><br><span class="line">![架构图](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;structure.png)</span><br><span class="line"></span><br><span class="line">#### 由于hivesql写入的hive表产出的hdsf文件自动生成的格式是_col1..._coln,在通过字段导入时会造成hive表字段和sr表字段映射失败</span><br><span class="line">在离线导入功能演进过程中,这个场景在大量业务侧被迫切提出需求,业务侧由于业务口径的变更,需要对hive表进行修改,而一修改hive表结构,历史数据回溯时就会出现分区映射的hdfs文件无法正确映射成功,这个场景和通过hive sql导入hive表时一样的,hive和sr之间字段无法映射成功,就造成导入失败,针对这一场景我们升级了离线导入模块,引入了spark sql模式来读取hive表,而不是之前的spark 文件方式读取,这种方式成功解决了用户变更表字段回溯场景和hive sql导入场景,如下load结构:</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.label_name (<br>DATA FROM TABLE hive_tb<br>INTO TABLE sr_tb<br>TEMPORARY PARTITION(temp</strong>partition)<br>SET (<br><code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br><code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>…….<br>)<br>WHERE (<code>dt</code> = ‘2023-12-09’)<br>)WITH RESOURCE ‘spark_external_resource’ (<br>  “spark.yarn.tags” = “h2s_foit_150748320231209914ff11b87c94c85947ab13f84ff4622”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br>```<br>注意:目前spark sql导入只在明细模型(duplicate key)和聚合模型(aggregate key)上支持</p>
<h3 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h3><p>目前starrocks离线导入功能已经集成在滴滴的同步中心,用户可以通过同步中心按hive表结构自动创建sr表结构,并且配置出hive表字段和sr表字段的映射关系后,构造出导入配置相应参数,调用sr的离线导入模块进行数据导入,而这个导入功能我们在2024年也会有以下规划:</p>
<h4 id="spark-sql支持主键模型-primary-key"><a href="#spark-sql支持主键模型-primary-key" class="headerlink" title="spark sql支持主键模型(primary key)"></a>spark sql支持主键模型(primary key)</h4><p>目前有一部分业务侧会通过主键模型将hive分区数据定时导回给sr,当前还不支持spark sql方式导入,2024年我们会进行这个feature开发</p>
<h4 id="独立sr构建工具"><a href="#独立sr构建工具" class="headerlink" title="独立sr构建工具"></a>独立sr构建工具</h4><p>当前的etl构建过程仍然需要在fe进程中进行交互处理,但fe节点我们在k8s初始化时只有12g,虽然在演进之路里我们加入了限流功能来保障fe稳定性,但是这个方式也会将hive导入耗时加大,对于一些需要高优保障的任务仍然需要按时产出,所以我们规划在2024年可以将这个构建工具从sr的fe进程中独立出来,单独部署在构建集群中,在完成etl阶段后,通过rpc方式通知给sr的fe进程来拉取</p>
<h4 id="hive外表联邦查询"><a href="#hive外表联邦查询" class="headerlink" title="hive外表联邦查询"></a>hive外表联邦查询</h4><p>当前很多业务侧的hive表分区压缩后也会在100g+,这种case导入到sr后查询性能也不一定会很好,但是导入过程却很浪费资源,我们计划在2024年完成hive外表的集成功能,让用户可以直接通过sr来查询hive表,而不需要再进行分区导入这一部分操作</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/11/04/clickhouse-parser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/04/clickhouse-parser/" class="post-title-link" itemprop="url">ClickHouse解析器大揭秘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-11-04 14:09:55 / 修改时间：17:01:49" itemprop="dateCreated datePublished" datetime="2022-11-04T14:09:55+08:00">2022-11-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>我们知道语法分析器的作用是根据给定的<a href="https://baike.baidu.com/item/%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95/2447403" target="_blank" rel="noopener">形式文法</a>对由词法单元（Token）序列构成的输入进行语法检查、并构建由输入的词法单元（Token）组成的数据结构（一般是<a href="https://baike.baidu.com/item/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E6%A0%91/20452541" target="_blank" rel="noopener">语法分析树</a>、<a href="https://baike.baidu.com/item/%E6%8A%BD%E8%B1%A1%E8%AF%AD%E6%B3%95%E6%A0%91/6129952" target="_blank" rel="noopener">抽象语法树</a>等层次化的数据结构）。而一提到语法解析目前市面上有很多语法解析器，其中解析sql更是数不胜数，例如最为人所知的antlr和jflex，而本文的主人公ClickHouse却自己去纯手工打造实现了一套sql解析器，本篇文章就来聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制。</p>
<h2 id="简单入门"><a href="#简单入门" class="headerlink" title="简单入门"></a>简单入门</h2><blockquote>
<p>首先来简单入门解决个小问题，那就是我们如何去连接ck，如何将query传递ck呢，如何设置传递给ck的query长度呢？   </p>
</blockquote>
<h3 id="通过TCP方式请求"><a href="#通过TCP方式请求" class="headerlink" title="通过TCP方式请求"></a>通过TCP方式请求</h3><blockquote>
<p>通过tcp方式使用clickhouse自己的客户端，连接clickhouse，在会话session里先使用<strong>set max_query_size=xx</strong>的方式让当前这个会话修改query的长度，如下图：  </p>
</blockquote>
<p><img src="/images/clickhouse/maxquerysize/1.png" alt="clickhouse"></p>
<h3 id="通过HTTP方式请求"><a href="#通过HTTP方式请求" class="headerlink" title="通过HTTP方式请求"></a>通过HTTP方式请求</h3><blockquote>
<p>通过http方式请求，<a href="http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写">http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写</a><br>注意chproxy只允许最大max_query_size为512mb，超过此长度会直接报错  </p>
</blockquote>
<h3 id="通过sql创建setting授权给登陆用户"><a href="#通过sql创建setting授权给登陆用户" class="headerlink" title="通过sql创建setting授权给登陆用户"></a>通过sql创建setting授权给登陆用户</h3><ol>
<li>创建setting profile<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create settings profile if not exists role_max_query_size SETTINGS max_query_size &#x3D; 100000000000;</span><br></pre></td></tr></table></figure></li>
<li>将profile赋值给某个用户<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant role_max_query_size to prod_voyager_stats_events;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="源码解析ck是如何处理max-query-size的"><a href="#源码解析ck是如何处理max-query-size的" class="headerlink" title="源码解析ck是如何处理max_query_size的"></a>源码解析ck是如何处理max_query_size的</h3><blockquote>
<p>由于源码较多，只抽出具体实现函数进行源码讲解，本次讲解基于clickhouse v20.6.3.28-stable（该版本与最新版出入较大）。  </p>
</blockquote>
<p><img src="/images/clickhouse/maxquerysize/2.png" alt="clickhouse"></p>
<ol>
<li>如上图所示，在<code>HTTPHandler.cpp</code>下进行各种http的协议处理时，有一个变量叫<strong>HTMLForm</strong>类型的<code>params</code>，承载的是http请求里的<code>uri</code>，并且在代码的484行进行了此变量的处理，如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">for (const auto &amp; [key, value] : params)</span><br><span class="line">&#123;</span><br><span class="line">    if (key &#x3D;&#x3D; &quot;database&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (database.empty())</span><br><span class="line">            database &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (key &#x3D;&#x3D; &quot;default_format&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (default_format.empty())</span><br><span class="line">            default_format &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (param_could_be_skipped(key))</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        &#x2F;&#x2F;&#x2F; Other than query parameters are treated as settings.</span><br><span class="line">        if (!customizeQueryParam(context, key, value))</span><br><span class="line">            settings_changes.push_back(&#123;key, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>而<code>customizeQueryParam</code>会判断该参数param是否等于query，如果是则不会进入setting的设置，再判断是否是param_开头的如果是则会传入context（理解为这次session会话中需要设置的各种上下文内容）则也不会进行setting处理，不是前面2个case则进行setting处理，重载系统默认的setting里的参数，如下图<br><img src="/images/clickhouse/maxquerysize/3.png" alt="clickhouse"></li>
<li>虽然第二步已经设置了setting，但注意代码的512行，这行代码会走向<strong>SettingsConstraints.cpp</strong>类的<strong>checkImpl</strong>校验逻辑里，有一些配置是不允许修改的，例如<strong>profile</strong>，例如配置就是如果已经通过grant授权配置了<strong>setting profile</strong>了，会去看这个用户的相关权限，如果不符合则会直接抛出exception，不再进行处理，注意这里还有一个问题，抛了异常后，不再将此次请求写入到system.query_log中，之后我们会修复此问题<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; For external data we also want settings</span><br><span class="line">context.checkSettingsConstraints(settings_changes);</span><br><span class="line">context.applySettingsChanges(settings_changes);</span><br></pre></td></tr></table></figure></li>
<li>做完setting的约束校验后，都符合条件，则我们已经重载了setting里的<strong>max_query_size</strong>，之后就走入了<strong>executeQuery.cpp</strong>执行query逻辑，在它的构造函数里我们就可以看到query是根据<strong>max_query_size</strong>来读取的，如下图<br><img src="/images/clickhouse/maxquerysize/4.png" alt="clickhouse"></li>
</ol>
<h2 id="深入探知"><a href="#深入探知" class="headerlink" title="深入探知"></a>深入探知</h2><p>介绍完了<strong>max_query_size</strong>的处理逻辑后，其实我们已经大致明白ck在query的处理流程是如何流转的，那么现在问题来了，我们知道可以通过<code>select xx from tb SETTINGS max_query_size=12112</code>这种方式传入自定义的setting参数，但是有些参数有生效，而select语法却对max_query_size不生效，原因是什么呢？好了，别着急现在我们就来解答ck是如何处理setting这层逻辑的。</p>
<h3 id="为什么max-query-size的select中不生效？"><a href="#为什么max-query-size的select中不生效？" class="headerlink" title="为什么max_query_size的select中不生效？"></a>为什么max_query_size的select中不生效？</h3><p><img src="/images/clickhouse/maxquerysize/5.png" alt="clickhouse"><br>原因很简单，只要我们读过了上面的流转过程，就知道max_query_size这个参数的处理系统默认是256kb，那么如果未通过uri方式传入<strong>max_query_size</strong>，则在截取query长度前，默认都是256kb，注意截取query时是还未进行ck的parser逻辑处理的，我们可以看到query里的setting是需要经过ck的parser解析后，才会重载进去(如下图6)，所以呢如果你的select query在256kb范围内，则截取完整query后，经过ck的parser解析出ast树，是会带上新的setting，但此时已经没有意义了，而相反的如果你的query超过了256kb，则只截取到256kb前的query，此时setting也不会走到<strong>ParserSelectQuery</strong>里，同时因为你的query被不完整截取后，会直接报ast语法错误<br><img src="/images/clickhouse/maxquerysize/6.png" alt="图6"></p>
<h2 id="源码看解析器"><a href="#源码看解析器" class="headerlink" title="源码看解析器"></a>源码看解析器</h2><h3 id="1-HTTPHandler-cpp-gt-processQuery"><a href="#1-HTTPHandler-cpp-gt-processQuery" class="headerlink" title="1. HTTPHandler.cpp =&gt; processQuery"></a>1. HTTPHandler.cpp =&gt; processQuery</h3><blockquote>
<p>每一个http请求都在clickhouse都会起一个叫<strong>HTTPHandler</strong>的线程去处理，根据http请求header和body，初始化请求上下文环境：包括session、用户信息、当前database、响应信息等，另外还处理限流，用户权限，根据配置取到setting信息进行设置，本文重点是调用<code>executeQuery</code>方法处理<code>query</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">executeQuery(*in, *used_output.out_maybe_delayed_and_compressed, &#x2F;* allow_into_outfile &#x3D; *&#x2F; false, context,</span><br><span class="line">        [&amp;response] (const String &amp; current_query_id, const String &amp; content_type, const String &amp; format, const String &amp; timezone)</span><br><span class="line">        &#123;</span><br><span class="line">            response.setContentType(content_type);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Query-Id&quot;, current_query_id);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Format&quot;, format);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Timezone&quot;, timezone);</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="2-executeQuery-cpp-gt-executeQuery"><a href="#2-executeQuery-cpp-gt-executeQuery" class="headerlink" title="2. executeQuery.cpp =&gt; executeQuery"></a>2. executeQuery.cpp =&gt; executeQuery</h3><p>从流中读出字节到buffer里，根据设置的<code>max_query_size</code>判断buffer是否已满，复制到LimitReadBuffer里，重点是执行<strong>executeQueryImpl</strong>，返回tuple类型的(ast, stream)，从stream里提取出<strong>pipeline(流水线)</strong>，根据ast构造出<code>IBlockInputStream</code>或者<code>IBlockOutputStream</code>，传给pipeline后执行pipeline的execute方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::tie(ast, streams) &#x3D; executeQueryImpl(begin, end, context, false, QueryProcessingStage::Complete, may_have_tail, &amp;istr);</span><br></pre></td></tr></table></figure></p>
<h3 id="2-executeQuery-cpp-gt-executeQueryImpl"><a href="#2-executeQuery-cpp-gt-executeQueryImpl" class="headerlink" title="2. executeQuery.cpp =&gt; executeQueryImpl"></a>2. executeQuery.cpp =&gt; executeQueryImpl</h3><p>按照解析出的ast，构造出Interpreter，调用Interpreter的exec方法去执行后返回pipeline，执行结果记录到query_log里，最后把构造出对应的ast和pipeline返回<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 这里是实现了ParserQuery对象，继承了IParserBase，IParserBase继承自IParser，等下走到6时，才知道虚函数parseImpl会通过ParserQuery对象实现</span><br><span class="line">ParserQuery parser(end, settings.enable_debug_queries);</span><br><span class="line">.........</span><br><span class="line">ast &#x3D; parseQuery(parser, begin, end, &quot;&quot;, max_query_size, settings.max_parser_depth);</span><br><span class="line">.........</span><br><span class="line">auto interpreter &#x3D; InterpreterFactory::get(ast, context, stage);</span><br><span class="line">.........</span><br><span class="line">res &#x3D; interpreter-&gt;execute();</span><br><span class="line">QueryPipeline &amp; pipeline &#x3D; res.pipeline;</span><br><span class="line">.........</span><br><span class="line">return std::make_tuple(ast, std::move(res));</span><br></pre></td></tr></table></figure></p>
<h3 id="3-parseQuery-cpp-gt-parseQueryAndMovePosition"><a href="#3-parseQuery-cpp-gt-parseQueryAndMovePosition" class="headerlink" title="3. parseQuery.cpp =&gt; parseQueryAndMovePosition"></a>3. parseQuery.cpp =&gt; parseQueryAndMovePosition</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr res &#x3D; tryParseQuery(parser, pos, end, error_message, false, query_description, allow_multi_statements, max_query_size, max_parser_depth);</span><br></pre></td></tr></table></figure>
<h3 id="4-parseQuery-cpp-gt-tryParseQuery"><a href="#4-parseQuery-cpp-gt-tryParseQuery" class="headerlink" title="4. parseQuery.cpp =&gt; tryParseQuery"></a>4. parseQuery.cpp =&gt; tryParseQuery</h3><blockquote>
<p>尝试解析SQL，将sql通过语法树规则装入TokenIterator，返回ASTPtr  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ClickHouse词法分析器是由Tokens和Lexer类来实现，token是最基础的元祖，之间是没有任何关联的，只是一堆词组和符号，通过lexer语法进行解析后，把元祖里的token建立起关系。</span><br><span class="line">Tokens tokens(pos, end, max_query_size);</span><br><span class="line">IParser::Pos token_iterator(tokens, max_parser_depth);</span><br><span class="line">&#x2F;&#x2F; 注意这里，TokenIterator对-&gt;使用了重载，在重载函数里去初始化TOKEN，主要是从第一个字符开始使用pos++的方式进行判断，可以进入Token Lexer::nextTokenImpl()进行查看</span><br><span class="line">if (token_iterator-&gt;isEnd() || token_iterator-&gt;type &#x3D;&#x3D; TokenType::Semicolon) &#123;</span><br><span class="line">    out_error_message &#x3D; &quot;Empty query&quot;;</span><br><span class="line">    pos &#x3D; token_iterator-&gt;begin;</span><br><span class="line">    return nullptr;</span><br><span class="line">&#125;</span><br><span class="line">.....</span><br><span class="line">Expected expected;</span><br><span class="line">......</span><br><span class="line">ASTPtr res;</span><br><span class="line">bool parse_res &#x3D; parser.parse(token_iterator, res, expected);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<code>IParser</code>的<code>parse</code>方法是<code>virtual</code>虚函数，<code>IParser</code>作为接口角色，被<code>IParserBase</code>继承，在<code>IParserBase</code>里实现了<code>parse</code>方法。</p>
</blockquote>
<h3 id="5-IParserBase-cpp-gt-parse"><a href="#5-IParserBase-cpp-gt-parse" class="headerlink" title="5. IParserBase.cpp =&gt; parse"></a>5. IParserBase.cpp =&gt; parse</h3><blockquote>
<p>在解每个token时都会根据当前的token进行预判（parseImpl返回的结果），返回true才会进入下一个子token  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bool IParserBase::parse(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    expected.add(pos, getName());</span><br><span class="line"></span><br><span class="line">    return wrapParseImpl(pos, IncreaseDepthTag&#123;&#125;, [&amp;]</span><br><span class="line">    &#123;</span><br><span class="line">        bool res &#x3D; parseImpl(pos, node, expected);</span><br><span class="line">        if (!res)</span><br><span class="line">            node &#x3D; nullptr;</span><br><span class="line">        return res;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意到parseImpl在IParserBase中是一个虚函数，将被继承自IParserBase类的子类实现，而在 <strong><em> 第2步 </em></strong>中我们定义的子类是ParserQuery，所以此时是直接调用到ParserQuery子类的parseImpl方法</p>
</blockquote>
<h3 id="6-ParserQuery-cpp-gt-parseImpl"><a href="#6-ParserQuery-cpp-gt-parseImpl" class="headerlink" title="6. ParserQuery.cpp =&gt; parseImpl"></a>6. ParserQuery.cpp =&gt; parseImpl</h3><blockquote>
<p>Parser的主要类（也都是继承自IParserBase）分别定义出来后，每个去尝试解析，如果都不在这几个主要Parser里，则返回false，否则返回true，clickhouse把query分类成以下14类，但本质上可以归纳为2类，第一类是有结果输出可对应show/select/desc/create等，第二类是无结果输出可对应insert/use/set等  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">bool ParserQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ParserQueryWithOutput query_with_output_p(enable_explain);</span><br><span class="line">    ParserInsertQuery insert_p(end);</span><br><span class="line">    ParserUseQuery use_p;</span><br><span class="line">    ParserSetQuery set_p;</span><br><span class="line">    ParserSystemQuery system_p;</span><br><span class="line">    ParserCreateUserQuery create_user_p;</span><br><span class="line">    ParserCreateRoleQuery create_role_p;</span><br><span class="line">    ParserCreateQuotaQuery create_quota_p;</span><br><span class="line">    ParserCreateRowPolicyQuery create_row_policy_p;</span><br><span class="line">    ParserCreateSettingsProfileQuery create_settings_profile_p;</span><br><span class="line">    ParserDropAccessEntityQuery drop_access_entity_p;</span><br><span class="line">    ParserGrantQuery grant_p;</span><br><span class="line">    ParserSetRoleQuery set_role_p;</span><br><span class="line">    ParserExternalDDLQuery external_ddl_p;</span><br><span class="line"></span><br><span class="line">    bool res &#x3D; query_with_output_p.parse(pos, node, expected)</span><br><span class="line">        || insert_p.parse(pos, node, expected)</span><br><span class="line">        || use_p.parse(pos, node, expected)</span><br><span class="line">        || set_role_p.parse(pos, node, expected)</span><br><span class="line">        || set_p.parse(pos, node, expected)</span><br><span class="line">        || system_p.parse(pos, node, expected)</span><br><span class="line">        || create_user_p.parse(pos, node, expected)</span><br><span class="line">        || create_role_p.parse(pos, node, expected)</span><br><span class="line">        || create_quota_p.parse(pos, node, expected)</span><br><span class="line">        || create_row_policy_p.parse(pos, node, expected)</span><br><span class="line">        || create_settings_profile_p.parse(pos, node, expected)</span><br><span class="line">        || drop_access_entity_p.parse(pos, node, expected)</span><br><span class="line">        || grant_p.parse(pos, node, expected)</span><br><span class="line">        || external_ddl_p.parse(pos, node, expected);</span><br><span class="line"></span><br><span class="line">    return res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意看这个parseImpl方法，进来后都会先去接<code>ParserQueryWithOutput</code>类解析相关ast，这里类涉及到了<code>explain</code>、<code>select</code>、<code>show</code>，<code>create</code>、<code>alter</code>等相关语法的解析，如果解析不过，则直接报错，解析成功后会处理我们这篇文章中提到的<strong>SETTING</strong>，如下图7定义的，将setting传入的变量存入到<code>s_settings</code>指针中。<br><img src="/images/clickhouse/maxquerysize/7.png" alt="图7"></p>
<h3 id="clcikhouse的parser总结："><a href="#clcikhouse的parser总结：" class="headerlink" title="clcikhouse的parser总结："></a>clcikhouse的parser总结：</h3><ul>
<li><ol>
<li>ClickHouse词法分析器<br>词法解析的主要任务是读入源程序的输入字符、将它们组成词素，生成并输出一个词法单元（Token）序列，每个词法单元对应于一个词素。<code>ClickHouse</code>中的每个词法单元（<code>Token</code>）使用一个<code>struct Tocken</code>结构体对象来进行存储，结构体中存储了词法单元的<code>type</code>和<code>value</code>。<br>ClickHouse词法分析器是由<code>Tokens</code>和<code>Lexer</code>类来实现， <strong><em>DB::Lexer::nextTokenImpl()</em></strong>函数用来对<code>SQL</code>语句进行词法分析的具体实现</li>
</ol>
</li>
<li><ol>
<li>ClickHouse语法解析器<br>ClickHouse中定义了不同的Parser用来对不同类型的SQL语句进行语法分析，例如：ParserInsertQuery（Insert语法分析器）、ParserCreateQuery（Create语法分析器）、ParserAlterQuery（Alter语法分析器）等等。<br>Parser首先判断输入的Token序列是否是该类型的SQL，若是该类型的SQL，则继续检查语法的正确性，正确则生成AST返回，语法错误的则抛出语法错误异常，否则直接返回空AST语法树</li>
</ol>
</li>
</ul>
<p><img src="/images/clickhouse/parser/1.png" alt="clickhouse"></p>
<h3 id="解答setting生效问题"><a href="#解答setting生效问题" class="headerlink" title="解答setting生效问题"></a>解答setting生效问题</h3><p><img src="/images/clickhouse/maxquerysize/8.png" alt="图7"><br>如上图所示，当前原生ck只支持InterpreterSelectQuery和InterpreterInsertQuery对query传入setting进行了重载处理。<br>InterpreterSelectQuery是在自己的构造函数里初始化了setting到context里<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">void InterpreterSelectQuery::initSettings()</span><br><span class="line">&#123;</span><br><span class="line">    auto &amp; query &#x3D; getSelectQuery();</span><br><span class="line">    if (query.settings())</span><br><span class="line">        InterpreterSetQuery(query.settings(), *context).executeForCurrentContext();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>InterpreterInsertQuery是在parser解析出ast后，在<code>executeQueryImpl</code>进行的setting重载context。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto * insert_query &#x3D; ast-&gt;as&lt;ASTInsertQuery&gt;();</span><br><span class="line"></span><br><span class="line">if (insert_query &amp;&amp; insert_query-&gt;settings_ast)</span><br><span class="line">    InterpreterSetQuery(insert_query-&gt;settings_ast, context).executeForCurrentContext();</span><br></pre></td></tr></table></figure><br>剩下的setting都已经是通过Interpreter执行结束后再处理的，对于我们需要在前置传入时没有效果了。</p>
<h3 id="解决业务困扰"><a href="#解决业务困扰" class="headerlink" title="解决业务困扰"></a>解决业务困扰</h3><p>当前我们的离线导入hive2ck，其实是通过将数据写入到临时表，这张临时表是按照目标表的表结构重新创建了一个MergeTree表，通过spark任务将hive数据以流式方式写入到临时表分区，生产出分区对应的多个part，生产过程中我们会将part拉回到临时表对应的detach目录，这个过程叫<a href="http://way.xiaojukeji.com/article/29557" target="_blank" rel="noopener">离线构建</a>，再将再将part通过ck的attach命令激活，这时候临时表就对该分区可见了，然后再通过replace partition的方式，将临时表的分区替换到我们的目标表去，这整个过程，就是我们的hive2ck处理流程，如下图所示：<br><img src="/images/clickhouse/maxquerysize/9.png" alt="图10"></p>
<blockquote>
<p>我们将此离线构建继承到数梦的同步中心后，陆续遇到了业务方来咨询相关问题，下面是问题汇总及如何解决的  </p>
</blockquote>
<h4 id="如何在离线导入中将明细数据写入到关联的物化视图"><a href="#如何在离线导入中将明细数据写入到关联的物化视图" class="headerlink" title="如何在离线导入中将明细数据写入到关联的物化视图"></a>如何在离线导入中将明细数据写入到关联的物化视图</h4><p>熟悉clickhouse的同学们都知道，原生ck对于物化视图的写入，唯一的方式是在明细表通过insert写入时，才会将数据经过物化视图的触发器写入关联的物化视图，而在离线构建过程中，ck是不支持的，但很多业务方跟我们提出这个需求，希望离线构建可以支持将分区数据写入到关联物化视图去，于是我们对ck的replace partition 进行了改造。<br>改造前的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221102&#39; FROM test.visits_basic_tmp;</span><br></pre></td></tr></table></figure><br>改造后的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221101&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW;</span><br></pre></td></tr></table></figure><br>在离线构建走到替换分区这一步时，我们改造了AstAlterQuery，让<code>ParserAlterQuery</code>增加了对<code>and trigger view</code>的语法解析，解析之后进入到<code>InterpreterAlterQuery</code>时，如果ast返回的trigger view是true，则程序流程会流转到取出明细表元数据，查询是否有关联物化视图，重新构造出类似下面的sql，交过pipeline进行执行，由此将该分区数据写入到物化视图去。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into 物化视图 select from 明细表 where 分区&#x3D;xx</span><br></pre></td></tr></table></figure></p>
<h4 id="分区过大导入失败如何解决"><a href="#分区过大导入失败如何解决" class="headerlink" title="分区过大导入失败如何解决"></a>分区过大导入失败如何解决</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx has timed out! (120000ms) Possible deadlock avoided. Client should retry</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/maxquerysize/10.png" alt="图10"><br>如上图所示，替换分区前，会给该明细表加一把锁，并设定锁时间（lock_acquire_timeout），系统默认时120s，如果该分区过大，替换过程超过120s，则会爆上面错误，而本文最开始已经讲解过如何处理setting，考虑到ck原生只支持insert和select时interpreter对setting重载，由此进行改造让InterpreterAlterQuery也支持通过sql传入锁时间，如下面语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221108&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW SETTINGS lock_acquire_timeout&#x3D;86400000;</span><br></pre></td></tr></table></figure><br>因为ParserQueryWithOutput已经对setting进行了解析，而AstAlterQuery其实是继承自ASTQueryWithOutput，所以我们已经获得了setting这一块的ast，无需再自己初始化新的ast，只要在InterpreterAlterQuery里把setting重载就行了，如图11<br><img src="/images/clickhouse/maxquerysize/11.png" alt="图10"></p>
<h4 id="替换分区成功，物化视图数据写入报错如何解决"><a href="#替换分区成功，物化视图数据写入报错如何解决" class="headerlink" title="替换分区成功，物化视图数据写入报错如何解决"></a>替换分区成功，物化视图数据写入报错如何解决</h4><ol>
<li>首先我们在数梦平台上控制了相关的ddl语句修改，如果用户要删明细表字段，则必须先去处理关联的物化视图字段，如果用户要删明细表，则必须先删物化视图</li>
<li>遇到替换分区成功，而物化视图写入失败，报错都是锁明细表超时，对于这种case可直接解锁明细表的锁，让物化视图自己去写，不再锁明细表，所以只需要做简单的锁释放便可</li>
</ol>
<p>以上是对ck进行改造过程中遇到的3个问题，此改造过程主要是满足离线导入可写入物化视图，未来我们还将对ck进行更多改造，以满足不同业务需求，各个 业务线大佬们如果在使用ck过程中有遇到任何问题，欢迎加入ck用户群，和我们一起沟通解决。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/" class="post-title-link" itemprop="url">starrocks在滴滴的落地</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-17 14:48:24" itemprop="dateCreated datePublished" datetime="2022-10-17T14:48:24+08:00">2022-10-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-07 23:53:33" itemprop="dateModified" datetime="2023-01-07T23:53:33+08:00">2023-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><h4 id="ClickHouse介绍"><a href="#ClickHouse介绍" class="headerlink" title="ClickHouse介绍"></a>ClickHouse介绍</h4><blockquote>
<p><code>ClickHouse</code>是由俄罗斯的第一大搜索引擎<code>Yandex</code>公司开源的列存数据库。令人惊喜的是，<code>ClickHouse</code>相较于很多商业<code>MPP</code>数据库，比如<code>Vertica</code>，<code>InfiniDB</code>有着极大的性能提升。除了<code>Yandex</code>以外，越来越多的公司开始尝试使用<code>ClickHouse</code>等列存数据库。对于一般的分析业务，结构性较强且数据变更不频繁，可以考虑将需要进行关联的表打平成宽表，放入<code>ClickHouse</code>中。</p>
<ul>
<li>配置丰富，只依赖与<code>Zookeeper</code></li>
<li>线性可扩展性，可以通过添加服务器扩展集群</li>
<li>容错性高，不同分片间采用异步多主复制</li>
<li>单表性能极佳，采用向量计算，支持采样和近似计算等优化手段</li>
<li>功能强大支持多种表引擎</li>
</ul>
</blockquote>
<h4 id="StarRocks介绍"><a href="#StarRocks介绍" class="headerlink" title="StarRocks介绍"></a>StarRocks介绍</h4><blockquote>
<p><code>StarRocks</code>是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容<code>MySQL</code>协议和<code>MySQL</code>生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。<code>StarRocks</code>致力于在全场景<code>OLAP</code>业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</p>
<ul>
<li>不依赖于大数据生态，同时外表的联邦查询可以兼容大数据生态</li>
<li>提供多种不同的模型，支持不同维度的数据建模</li>
<li>支持在线弹性扩缩容，可以自动负载均衡</li>
<li>支持高并发分析查询</li>
<li>实时性好，支持数据秒级写入</li>
<li>兼容MySQL 5.7协议和MySQL生态</li>
</ul>
</blockquote>
<h4 id="二者的对比"><a href="#二者的对比" class="headerlink" title="二者的对比"></a>二者的对比</h4><p><strong>相似之处</strong></p>
<ul>
<li>都可以提供极致的性能</li>
<li>都不依赖于<code>Hadoop</code>生态</li>
<li>底层存储分片都提供了主主的复制高可用机制。</li>
<li>都是<code>MPP</code>架构</li>
<li>都是列式存储</li>
<li>都支持表述SQL语法</li>
<li>都提供了MOLAP库的预聚合能力</li>
</ul>
<p><strong>差异性</strong></p>
<ul>
<li><code>ClickHouse</code>在更适用于大宽表的场景，<code>TP</code>的数据通过<code>CDC</code>工具的，可以考虑在<code>Flink</code>中将需要关联的表打平，以大宽表的形式写入<code>ClickHouse</code></li>
<li><code>StarRocks</code>对于<code>join</code>的能力更强，<code>ClickHouse虽</code>然提供了<code>join</code>的语义，但使用上对大表关联的能力支撑较弱，复杂的关联查询经常会引起<code>OOM</code></li>
<li><code>ClickHouse</code>对高并发的业务并不友好，即使一个查询，也会用服务器一半的<code>CPU</code>去查询</li>
<li><code>StarRocks</code>可以支撑数千用户同时进行分析查询，在部分场景下，高并发能力能够达到万级。<code>StarRocks</code>在数据存储层，采用先分区再分桶的策略，增加了数据的指向性，利用前缀索引可以快读对数据进行过滤和查找，减少磁盘的<code>I/O</code>操作，提升查询性能</li>
<li>对于用户的原有的查询基表的 <code>SQL</code> 语句保持不变，<code>StarRocks</code> 会自动选择一个最优的物化视图，从物化视图中读取数据并计算。用户可以通过 <code>EXPLAIN</code> 命令来检查当前查询是否使用了物化视图。而<code>ClickHouse</code>则需要用户自行指定<code>SQL</code>中所需要使用的物化视图。</li>
</ul>
<h4 id="为什么推荐StarRocks"><a href="#为什么推荐StarRocks" class="headerlink" title="为什么推荐StarRocks"></a>为什么推荐StarRocks</h4><ol>
<li>滴滴大数据<code>OLAP</code>团队目前在维护的引擎有<code>StarRocks</code>、<code>ClickHouse</code>、<code>Druid</code>，3个引擎各有各的特点，现有的OLAP引擎(Kylin、Druid、ClickHouse)多表Join时的性能都比较差，甚至不支持多表Join，现有的引擎<code>Druid</code>虽然有<code>lookup</code>表的能力，但经过实际测试后性能不佳。<code>Apache Kylin</code>实际上也不支持<code>Join</code>，多表的<code>Join</code>需要通过在<code>cube</code>构建的时候底层打成宽表来实现。<code>ClickHouse</code>只支持本地<code>Hash join</code>的模式，不支持分布式<code>Shuffle join</code>，多数情况下灵活性受限，性能表现不佳。</li>
<li><code>OLAP</code>引擎需要同时具备明细数据查询和数据聚合的能力。由于<code>Apache Kylin</code>、<code>Druid</code>不能较好支持明细数据查询，我们引入了<code>ClickHouse</code>，通过在明细表基础上创建相应聚合物化视图来处理，但是不够灵活，对于上层应用来说，查明细和查聚合需要切到不同的表去处理。</li>
<li>目前我们团队在有限人员情况下需要维护这3个引擎的稳定性，导致我们对每一个引擎的理解深度都不够，特别像<code>ClickHouse</code>，运维成本非常高，ClickHouse集群的分片、副本信息，都是通过静态的配置文件的方式进行配置。当整个集群需要扩缩容的时候，就必须通过修改配置文件的方式进行刷新，数据的均衡都需要运维人员介入。此外ClickHouse通过zookeeper来做副本管理，当集群规模变大时，副本数过多会导致zookeeper的压力变大，集群的稳定性也就会相应变差。<br>为解决以上问题，滴滴大数据<code>OLAP</code>团队在2022年初开始调研<code>StarRocks</code>，在全面测试过从上面对<code>StarRocks</code>和<code>ClickHouse</code>的对比，我们也可以明显感受到<code>StarRocks</code>在多数场景下都是优于<code>ClickHouse</code>的，我们希望通过<code>StarRocks</code>来实现<code>OLAP</code>平台的多业务场景的查询引擎统一化。<br><img src="/images/starrocks/helloworld/20.png" alt="1"><br>注：这是我们针对<code>Druid</code>、<code>ClickHouse</code>、<code>StarRocks</code>进行的测试对比，<a href="http://wiki.intra.xiaojukeji.com/pages/viewpage.action?pageId=799050659" target="_blank" rel="noopener">链接</a>。</li>
</ol>
<h3 id="StarRocks特性"><a href="#StarRocks特性" class="headerlink" title="StarRocks特性"></a>StarRocks特性</h3><blockquote>
<p><code>StarRocks</code>的架构设计融合了<code>MPP</code>数据库，以及分布式系统的设计思想，其特性如下所示。</p>
<h4 id="架构精简"><a href="#架构精简" class="headerlink" title="架构精简"></a>架构精简</h4><ul>
<li><code>StarRocks</code>内部通过<code>MPP</code>计算框架完成<code>SQL</code>的具体执行工作。<code>MPP</code>框架能够充分的利用多节点的计算能力，整个查询可以并行执行，从而实现良好的交互式分析体验。</li>
<li><code>StarRocks</code>集群不需要依赖任何其他组件，易部署、易维护和极简的架构设计，降低了<code>StarRocks</code>系统的复杂度和维护成本，同时也提升了系统的可靠性和扩展性。管理员只需要专注于<code>StarRocks</code>系统，无需学习和管理任何其他外部系统。</li>
</ul>
</blockquote>
<h4 id="全面向量化引擎"><a href="#全面向量化引擎" class="headerlink" title="全面向量化引擎"></a>全面向量化引擎</h4><p><code>StarRocks</code>的计算层全面采用了向量化技术，将所有算子、函数、扫描过滤和导入导出模块进行了系统性优化。通过列式的内存布局、适配<code>CPU</code>的<code>SIMD</code>指令集等手段，充分发挥了现代<code>CPU</code>的并行计算能力，从而实现亚秒级别的多维分析能力。</p>
<h4 id="智能查询优化"><a href="#智能查询优化" class="headerlink" title="智能查询优化"></a>智能查询优化</h4><p><code>StarRocks</code>通过<code>CBO</code>优化器 <strong>（Cost Based Optimizer）</strong> 可以对复杂查询自动优化。无需人工干预，就可以通过统计信息合理估算执行成本，生成更优的执行计划，大大提高了<code>AdHoc</code>和<code>ETL</code>场景的数据分析效率。</p>
<h4 id="联邦查询"><a href="#联邦查询" class="headerlink" title="联邦查询"></a>联邦查询</h4><p><code>StarRocks</code>支持使用外表的方式进行联邦查询，当前可以支持<code>Hive</code>、<code>MySQL</code>、<code>Elasticsearch</code>、<code>Iceberg</code>和<code>Hudi</code>类型的外表，您无需通过数据导入，可以直接进行数据查询加速。</p>
<h4 id="高效更新"><a href="#高效更新" class="headerlink" title="高效更新"></a>高效更新</h4><p><code>StarRocks</code>支持明细模型(DUPLICATE KEY)、聚合模型(AGGREGATE KEY)、主键模型(PRIMARY KEY)和更新模型(UNIQUE KEY)，其中主键模型可以按照主键进行<code>Upsert</code>或<code>Delete</code>操作，通过存储和索引的优化可以在并发更新的同时实现高效的查询优化，更好的服务实时数仓的场景。</p>
<h4 id="智能物化视图"><a href="#智能物化视图" class="headerlink" title="智能物化视图"></a>智能物化视图</h4><ul>
<li><code>StarRocks</code>支持智能的物化视图。您可以通过创建物化视图，预先计算生成预聚合表用于加速聚合类查询请求。</li>
<li><code>StarRocks</code>的物化视图能够在数据导入时自动完成汇聚，与原始表数据保持一致。</li>
<li>查询的时候，您无需指定物化视图，<code>StarRocks</code>能够自动选择最优的物化视图来满足查询请求。</li>
</ul>
<h4 id="标准SQL"><a href="#标准SQL" class="headerlink" title="标准SQL"></a>标准SQL</h4><ul>
<li><code>StarRocks</code>支持标准的<code>SQL</code>语法，包括聚合、<code>JOIN</code>、排序、窗口函数和自定义函数等功能。</li>
<li><code>StarRocks</code>可以完整支持<code>TPC-H</code>的22个<code>SQL</code>和<code>TPC-DS</code>的99个<code>SQL</code>。</li>
<li><code>StarRocks</code>兼容<code>MySQL</code>协议语法，可以使用现有的各种客户端工具、<code>BI</code>软件访问<code>StarRocks</code>，对<code>StarRocks</code>中的数据进行拖拽式分析。</li>
</ul>
<h4 id="流批一体"><a href="#流批一体" class="headerlink" title="流批一体"></a>流批一体</h4><ul>
<li><code>StarRocks</code>支持实时和批量两种数据导入方式。</li>
<li><code>StarRocks</code>支持的数据源有<code>Kafka</code>、<code>HDFS</code>和本地文件。</li>
<li><code>StarRocks</code>支持的数据格式有<code>ORC</code>、<code>Parquet</code>和<code>CSV</code>等。</li>
<li><code>StarRocks</code>可以实时消费<code>Kafka</code>数据来完成数据导入，保证数据不丢不重 <strong>（exactly once）</strong>。</li>
<li><code>StarRocks</code>也可以从本地或者远程（HDFS）批量导入数据。</li>
</ul>
<h4 id="高可用易扩展"><a href="#高可用易扩展" class="headerlink" title="高可用易扩展"></a>高可用易扩展</h4><ul>
<li><code>StarRocks</code>的元数据和数据都是多副本存储，并且集群中服务有热备，多实例部署，避免了单点故障。</li>
<li>集群具有自愈能力，可弹性恢复，节点的宕机、下线和异常都不会影响<code>StarRocks</code>集群服务的整体稳定性。</li>
<li><code>StarRocks</code>采用分布式架构，存储容量和计算能力可近乎线性水平扩展。<code>StarRocks</code>单集群的节点规模可扩展到数百节点，数据规模可达到10 PB级别。</li>
<li>扩缩容期间无需停服，可以正常提供查询服务。</li>
<li><code>StarRocks</code>中表模式热变更，可通过一条简单<code>SQL</code>命令动态地修改表的定义，例如增加列、减少列和新建物化视图等。同时，处于模式变更中的表也可以正常导入和查询数据。</li>
</ul>
<h3 id="StarRocks应用场景"><a href="#StarRocks应用场景" class="headerlink" title="StarRocks应用场景"></a>StarRocks应用场景</h3><blockquote>
<p>StarRocks可以满足企业级用户的多种分析需求，具体的业务场景如下所示：</p>
<h4 id="OLAP多维分析"><a href="#OLAP多维分析" class="headerlink" title="OLAP多维分析"></a>OLAP多维分析</h4><ul>
<li>用户行为分析</li>
<li>用户画像、标签分析、圈人</li>
<li>高维业务指标报表</li>
<li>自助式报表平台</li>
<li>业务问题探查分析</li>
<li>跨主题业务分析</li>
<li>财务报表</li>
<li>系统监控分析</li>
</ul>
</blockquote>
<h4 id="实时数仓"><a href="#实时数仓" class="headerlink" title="实时数仓"></a>实时数仓</h4><ul>
<li>电商大促数据分析</li>
<li>教育行业的直播质量分析</li>
<li>物流行业的运单分析</li>
<li>金融行业绩效分析、指标计算</li>
<li>广告投放分析</li>
<li>管理驾驶舱</li>
<li>探针分析APM（Application Performance Management）</li>
</ul>
<h4 id="高并发查询"><a href="#高并发查询" class="headerlink" title="高并发查询"></a>高并发查询</h4><ul>
<li>广告主报表分析</li>
<li>零售行业渠道人员分析</li>
<li>saas行业面向用户分析报表</li>
<li>dashboard多页面分析</li>
</ul>
<h4 id="统一分析"><a href="#统一分析" class="headerlink" title="统一分析"></a>统一分析</h4><p>通过使用一套系统解决多维分析、高并发查询、实时分析和Ad-Hoc查询等场景，降低系统复杂度和多技术栈开发与维护成本。</p>
<h3 id="如何使用StarRocks"><a href="#如何使用StarRocks" class="headerlink" title="如何使用StarRocks"></a>如何使用StarRocks</h3><blockquote>
<p>我们olap团队已经将<code>StarRocks</code>接入到滴滴各个平台，下面会介绍如何使用滴滴内部的平台和工具方便快捷的使用<code>StarRocks</code>引擎。<br><img src="/images/starrocks/helloworld/19.png" alt="1"></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/26/replace-trigger/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/26/replace-trigger/" class="post-title-link" itemprop="url">clickhouse离线导入大揭秘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-26 22:57:11" itemprop="dateCreated datePublished" datetime="2022-09-26T22:57:11+08:00">2022-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-28 14:01:34" itemprop="dateModified" datetime="2022-09-28T14:01:34+08:00">2022-09-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="如何按照hive表的表结构创建clickhouse表"><a href="#如何按照hive表的表结构创建clickhouse表" class="headerlink" title="如何按照hive表的表结构创建clickhouse表"></a>如何按照hive表的表结构创建clickhouse表</h3><p>点击<a href="http://studio.data.didichuxing.com/stream/create-table?tableType=clickHouse" target="_blank" rel="noopener">链接</a>通过数梦的实时平台可以快捷创建对应hive表结构的ck表，如下图所示：<br><img src="/images/clickhouse/hive2clickhouse/1.png" alt="clickhouse"></p>
<h3 id="hive表的数据如何导入clickhouse表"><a href="#hive表的数据如何导入clickhouse表" class="headerlink" title="hive表的数据如何导入clickhouse表"></a>hive表的数据如何导入clickhouse表</h3><h4 id="创建同步任务"><a href="#创建同步任务" class="headerlink" title="创建同步任务"></a>创建同步任务</h4><p>点击<a href="http://sync.data-pre.didichuxing.com/job/offline/edit/818177?step=1" target="_blank" rel="noopener">链接</a>通过数梦的同步中心可以快捷创建hive表映射到ck表的同步任务，如下图所示<br><img src="/images/clickhouse/hive2clickhouse/2.png" alt="clickhouse"></p>
<h4 id="同步任务流程"><a href="#同步任务流程" class="headerlink" title="同步任务流程"></a>同步任务流程</h4><p>如下图所示，我们按照这个流程处理hive数据导入到clickhouse<br><img src="/images/clickhouse/hive2clickhouse/3.png" alt="clickhouse"></p>
<ul>
<li><ol>
<li>创建临时表<br>按照ck表的表结构，我们会在集群的所有写节点创建同样表结构的单机表（MergeTree）引擎</li>
</ol>
</li>
<li><ol>
<li>起spark任务，利用clickhouse-local工具将hive表导入到临时表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat data.orc | clickhouse-local \</span><br><span class="line">--format&#x3D;Native \</span><br><span class="line">--query&#x3D;&#39;CREATE TABLE input (col1 String, col2 String, col3 String) ENGINE &#x3D; File(ORC, stdin);CREATE TABLE target_table (col1 String, col2 String, col3 String) ENGINE &#x3D; MergeTree() partition by tuple() order by col1;insert into target_table select *,&quot;$year&quot; as year,&quot;$month&quot; as month, &quot;$day&quot; as day from input;optimize table target_table final&#39; \</span><br><span class="line">--config-file&#x3D;config.xmls</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/05/clickhouse-insert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/05/clickhouse-insert/" class="post-title-link" itemprop="url">clickhouse的写入流程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-05 15:09:55" itemprop="dateCreated datePublished" datetime="2022-04-05T15:09:55+08:00">2022-04-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 18:09:00" itemprop="dateModified" datetime="2022-08-22T18:09:00+08:00">2022-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h3><p>ck请求处理过程<br><img src="/images/clickhouse/insert/1.png" alt="clickhouse"></p>
<h4 id="客户端请求发给ck，ck接收到请求，放入请求队列"><a href="#客户端请求发给ck，ck接收到请求，放入请求队列" class="headerlink" title="客户端请求发给ck，ck接收到请求，放入请求队列"></a>客户端请求发给ck，ck接收到请求，放入请求队列</h4><h4 id="ck将请求队列的请求分发给Request-handler线程处理"><a href="#ck将请求队列的请求分发给Request-handler线程处理" class="headerlink" title="ck将请求队列的请求分发给Request handler线程处理"></a>ck将请求队列的请求分发给Request handler线程处理</h4><h4 id="Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。"><a href="#Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。" class="headerlink" title="Request handler线程处理请求，解析sql语句，执行sql语句逻辑。"></a>Request handler线程处理请求，解析sql语句，执行sql语句逻辑。</h4><h4 id="简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）"><a href="#简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）" class="headerlink" title="简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）"></a>简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）</h4><h4 id="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"><a href="#请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求" class="headerlink" title="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"></a>请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求</h4><h3 id="insert语句解析执行"><a href="#insert语句解析执行" class="headerlink" title="insert语句解析执行"></a>insert语句解析执行</h3><h4 id="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"><a href="#初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息" class="headerlink" title="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"></a>初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息</h4><h4 id="解析sql语句"><a href="#解析sql语句" class="headerlink" title="解析sql语句"></a>解析sql语句</h4><h4 id="检查被写入的表是否存在，是否有写入权限，是否被限流"><a href="#检查被写入的表是否存在，是否有写入权限，是否被限流" class="headerlink" title="检查被写入的表是否存在，是否有写入权限，是否被限流"></a>检查被写入的表是否存在，是否有写入权限，是否被限流</h4><h4 id="对insert数据校验，字段是否存在，是否满足约束"><a href="#对insert数据校验，字段是否存在，是否满足约束" class="headerlink" title="对insert数据校验，字段是否存在，是否满足约束"></a>对insert数据校验，字段是否存在，是否满足约束</h4><h4 id="根据默认值填充空字段和物化字段"><a href="#根据默认值填充空字段和物化字段" class="headerlink" title="根据默认值填充空字段和物化字段"></a>根据默认值填充空字段和物化字段</h4><h4 id="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"><a href="#缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。" class="headerlink" title="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"></a>缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。</h4><h4 id="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"><a href="#将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree" class="headerlink" title="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"></a>将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree</h4><h4 id="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"><a href="#检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表" class="headerlink" title="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"></a>检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表</h4><h3 id="分布式表写入"><a href="#分布式表写入" class="headerlink" title="分布式表写入"></a>分布式表写入</h3><blockquote>
<p>分布式表数据写入一般情况下是异步写入，只有对使用 remote(‘addresses_expr’, db, table[, ‘user’[, ‘password’], sharding_key]) 定义的表的写入是同步的。如果分布式表中含有local表或replical的表local副本，直接写本地表。<br><img src="/images/clickhouse/insert/2.png" alt="clickhouse"></p>
</blockquote>
<h4 id="将写入block数据按sharding逻辑分成多个block"><a href="#将写入block数据按sharding逻辑分成多个block" class="headerlink" title="将写入block数据按sharding逻辑分成多个block"></a>将写入block数据按sharding逻辑分成多个block</h4><h4 id="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"><a href="#将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据" class="headerlink" title="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"></a>将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据</h4><h4 id="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"><a href="#检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎" class="headerlink" title="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"></a>检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎</h4><h4 id="shard在远程，将新insert语句写入远程shard本地缓存文件。"><a href="#shard在远程，将新insert语句写入远程shard本地缓存文件。" class="headerlink" title="shard在远程，将新insert语句写入远程shard本地缓存文件。"></a>shard在远程，将新insert语句写入远程shard本地缓存文件。</h4><h4 id="通知后台线程发送本地缓存中的数据"><a href="#通知后台线程发送本地缓存中的数据" class="headerlink" title="通知后台线程发送本地缓存中的数据"></a>通知后台线程发送本地缓存中的数据</h4><h4 id="后台执行过程"><a href="#后台执行过程" class="headerlink" title="后台执行过程"></a>后台执行过程</h4><h4 id="读远程shard本地缓存目录"><a href="#读远程shard本地缓存目录" class="headerlink" title="读远程shard本地缓存目录"></a>读远程shard本地缓存目录</h4><h4 id="逐个处理每个文件"><a href="#逐个处理每个文件" class="headerlink" title="逐个处理每个文件"></a>逐个处理每个文件</h4><h4 id="根据配置的loadbalance策略，选择合适的机器连接"><a href="#根据配置的loadbalance策略，选择合适的机器连接" class="headerlink" title="根据配置的loadbalance策略，选择合适的机器连接"></a>根据配置的loadbalance策略，选择合适的机器连接</h4><h4 id="将文件的insert语句通过上步连接发送给远程机器执行"><a href="#将文件的insert语句通过上步连接发送给远程机器执行" class="headerlink" title="将文件的insert语句通过上步连接发送给远程机器执行"></a>将文件的insert语句通过上步连接发送给远程机器执行</h4><h4 id="执行成功后删除对应文件"><a href="#执行成功后删除对应文件" class="headerlink" title="执行成功后删除对应文件"></a>执行成功后删除对应文件</h4><h3 id="本地表写入"><a href="#本地表写入" class="headerlink" title="本地表写入"></a>本地表写入</h3><blockquote>
<p>本地表一般是StorageReplicatedXXMergeTree，其写入过程如下：<br><img src="/images/clickhouse/insert/3.png" alt="clickhouse"><br>本地表是以block为最小单元单次写入，一个block中的数据可能是一次insert的全部数据，也可以是部分数据。</p>
</blockquote>
<h4 id="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"><a href="#检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。" class="headerlink" title="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"></a>检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。</h4><h4 id="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"><a href="#检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。" class="headerlink" title="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"></a>检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。</h4><h4 id="将写入block数据按partition-by-逻辑分成多个block"><a href="#将写入block数据按partition-by-逻辑分成多个block" class="headerlink" title="将写入block数据按partition by 逻辑分成多个block"></a>将写入block数据按partition by 逻辑分成多个block</h4><h4 id="依次将每个分区的block数据写入表中"><a href="#依次将每个分区的block数据写入表中" class="headerlink" title="依次将每个分区的block数据写入表中"></a>依次将每个分区的block数据写入表中</h4><h4 id="创建分区的临时part"><a href="#创建分区的临时part" class="headerlink" title="创建分区的临时part"></a>创建分区的临时part</h4><h4 id="计算part数据的sha1生成part对应的blockid"><a href="#计算part数据的sha1生成part对应的blockid" class="headerlink" title="计算part数据的sha1生成part对应的blockid"></a>计算part数据的sha1生成part对应的blockid</h4><h4 id="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"><a href="#检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。" class="headerlink" title="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"></a>检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。</h4><h4 id="将part信息发布到ck，通知其他副本拉去新添加part"><a href="#将part信息发布到ck，通知其他副本拉去新添加part" class="headerlink" title="将part信息发布到ck，通知其他副本拉去新添加part"></a>将part信息发布到ck，通知其他副本拉去新添加part</h4><h4 id="将临时part加入到commit到mergetree表"><a href="#将临时part加入到commit到mergetree表" class="headerlink" title="将临时part加入到commit到mergetree表"></a>将临时part加入到commit到mergetree表</h4><h4 id="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"><a href="#如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件" class="headerlink" title="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"></a>如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件</h4><h4 id="临时part创建过程"><a href="#临时part创建过程" class="headerlink" title="临时part创建过程"></a>临时part创建过程</h4><ul>
<li>创建block数据对应分区的临时part对象</li>
<li>计算分区min_max索引</li>
<li>处理排序和主键索引</li>
<li>ttl处理</li>
<li>其他二级索引处理</li>
<li>将处理过的block数据和索引写入临时part，根据配置的压缩方式压缩</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Blank Lin</p>
  <div class="site-description" itemprop="description">say something about me</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Blank Lin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
