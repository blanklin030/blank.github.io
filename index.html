<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="say something about me">
<meta property="og:type" content="website">
<meta property="og:title" content="BlankLin">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="BlankLin">
<meta property="og:description" content="say something about me">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Blank Lin">
<meta property="article:tag" content="lazy and boring">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>BlankLin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BlankLin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BlankLin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">lazy and boring</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/" class="post-title-link" itemprop="url">常用的linux排查命令</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-04 23:04:11 / 修改时间：23:05:15" itemprop="dateCreated datePublished" datetime="2023-01-04T23:04:11+08:00">2023-01-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tool/" itemprop="url" rel="index"><span itemprop="name">tool</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何看查占用cpu最多的进程？"><a href="#如何看查占用cpu最多的进程？" class="headerlink" title="如何看查占用cpu最多的进程？"></a>如何看查占用cpu最多的进程？</h2><ul>
<li><p>方法一<br>核心指令：ps<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps H -eo pid,pcpu | sort -nk2 | tail</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps H -eo pid,pcpu | sort -nk2 | tail</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">31396  0.6</span><br><span class="line">30904  1.0</span><br><span class="line">30914  1.0</span><br></pre></td></tr></table></figure>
<p>结果：<br>瞧见了吧，最耗cpu的pid=30914。<br>画外音：实际上是31396。</p>
</li>
<li><p>方法二<br>核心指令：top<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">top</span><br><span class="line">Shift + t</span><br></pre></td></tr></table></figure>
<h2 id="找到了最耗CPU的进程ID，对应的服务名是什么呢？"><a href="#找到了最耗CPU的进程ID，对应的服务名是什么呢？" class="headerlink" title="找到了最耗CPU的进程ID，对应的服务名是什么呢？"></a>找到了最耗CPU的进程ID，对应的服务名是什么呢？</h2></li>
<li><p>方法一<br>核心指令：ps<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | fgrep pid</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ps aux | fgrep 30914</span><br><span class="line">work 30914  1.0  0.8 309568 71668 ?  Sl   Feb02 124:44 .&#x2F;router2 –conf&#x3D;rs.conf</span><br></pre></td></tr></table></figure>
<p>结果：<br>瞧见了吧，进程是./router2</p>
</li>
<li><p>方法二<br>直接查proc即可。<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll &#x2F;proc&#x2F;pid</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ ll &#x2F;proc&#x2F;30914</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 cwd -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2</span><br><span class="line">lrwxrwxrwx  1 work work 0 Feb 10 13:27 exe -&gt; &#x2F;home&#x2F;work&#x2F;im-env&#x2F;router2&#x2F;router2</span><br></pre></td></tr></table></figure>
<p>画外音：这个好，全路径都出来了。</p>
</li>
</ul>
<h2 id="如何查看某个端口的连接情况？"><a href="#如何查看某个端口的连接情况？" class="headerlink" title="如何查看某个端口的连接情况？"></a>如何查看某个端口的连接情况？</h2><ul>
<li><p>方法一<br>核心指令：netstat<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lap | fgrep port</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ netstat -lap | fgrep 22022</span><br><span class="line">tcp        0      0 1.2.3.4:22022          *:*                         LISTEN      31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46642          ESTABLISHED 31396&#x2F;imui</span><br><span class="line">tcp        0      0 1.2.3.4:22022          1.2.3.4:46640          ESTABLISHED 31396&#x2F;imui</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法二<br>核心指令：lsof<br>实际命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -i :port</span><br></pre></td></tr></table></figure>
<p>执行效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@test01 ~]$ &#x2F;usr&#x2F;sbin&#x2F;lsof -i :22022</span><br><span class="line">COMMAND   PID USER   FD   TYPE   DEVICE SIZE NODE NAME</span><br><span class="line">router  30904 work   50u  IPv4 69065770       TCP 1.2.3.4:46638-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   51u  IPv4 69065772       TCP 1.2.3.4:46639-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br><span class="line">router  30904 work   52u  IPv4 69065774       TCP 1.2.3.4:46640-&gt;1.2.3.4:22022 (ESTABLISHED)</span><br></pre></td></tr></table></figure>
<p>学废了吗？</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/30/2022/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/30/2022/" class="post-title-link" itemprop="url">2022年终总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-12-30 16:04:11 / 修改时间：15:46:15" itemprop="dateCreated datePublished" datetime="2022-12-30T16:04:11+08:00">2022-12-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blank/" itemprop="url" rel="index"><span itemprop="name">blank</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><p>2022年仓皇而逃，在恍惚间我仍然有一丝怀疑，此时此刻是2019年的冬天，我们在准备着即将到来到2020年，订好了机票，订好了酒店，在跨年夜的云层之上，陌生人在沉睡，而我即将见到相隔数月思念日日夜夜的人儿。</p>
</li>
<li><p>这种怀疑在这3年间时不时的击碎我，重建我，沉溺我，到最后清醒我，时间过得太快了啊，快到迅雷烈风，快到掩耳而逝，我不是还在17/8岁的高中课堂上汗流浃背准备高考吗？怎么这一眨眼之间，我竟已是前额白发丝丝黑眼圈蜡黄脸的中年人了呢？</p>
</li>
<li><p>人世间的痛不知道何时能结束，人世间的离愁不知道何时能消散，虚度30余载光阴，到如今即将到来的第二个本命年，我有何成就吗？我有实现过理想吗？我活着的这些时刻，有给这个世界带来什么美好吗？我不自知只觉惭愧。</p>
</li>
<li><p>麻木的躯壳麻木的灵魂，肤浅的知识肤浅的见识，2022年在核酸、隔离、阳性中结束，这浅浅6个字结束了我的又一个365天，我是谁？我在2022年都做了什么事？有什么可以拿说来细说一二吗？有什么不足和遗憾吗？希望2023年可以改变吗？希望改变什么呢？</p>
</li>
<li><p>没有答案，我甚至连流水账都写不出来，哦，我这个可笑的中年人。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/08/clickhouse%E9%9B%86%E7%BE%A4cpu%E9%A3%99%E9%AB%98%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">clickhouse集群cpu飙高问题排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-08 19:35:11" itemprop="dateCreated datePublished" datetime="2022-12-08T19:35:11+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-15 22:00:44" itemprop="dateModified" datetime="2022-12-15T22:00:44+08:00">2022-12-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse是分布式系统，一条查询sql会经过pipeline处理后通过后台的查询线程池开启多线程查询，而经常会收到cpu飙高告警，则是因为这条sql开启了系统所有的cpu资源（多个线程）进行计算</p>
<h3 id="告警图"><a href="#告警图" class="headerlink" title="告警图"></a>告警图</h3><p>如下图所示，提示cpu飙高<br><img src="/images/clickhouse/cpu_high/1.png" alt="clickhouse"><br>如下图所示，ganglia上展示了当时的cpu顶着线在跑<br><img src="/images/clickhouse/cpu_high/2.png" alt="clickhouse"></p>
<h3 id="system-query-log"><a href="#system-query-log" class="headerlink" title="system.query_log"></a>system.query_log</h3><blockquote>
<p><a href="https://clickhouse.com/docs/en/operations/system-tables/query_log" target="_blank" rel="noopener">地址</a>，该表包含了所有进入到ck的sql语句。<br>该表包含字段如下：</p>
<ul>
<li>type (Enum8) — 执行查询时的事件类型. 值:<ul>
<li>‘QueryStart’ = 1 — 查询成功启动.</li>
<li>‘QueryFinish’ = 2 — 查询成功完成.</li>
</ul>
</li>
<li>‘ExceptionBeforeStart’ = 3 — 查询执行前有异常.</li>
<li>‘ExceptionWhileProcessing’ = 4 — 查询执行期间有异常.</li>
<li>event_date (Date) — 查询开始日期.</li>
<li>event_time (DateTime) — 查询开始时间.</li>
<li>event_time_microseconds (DateTime64) — 查询开始时间（毫秒精度）.</li>
<li>query_start_time (DateTime) — 查询执行的开始时间.</li>
<li>query_start_time_microseconds (DateTime64) — 查询执行的开始时间（毫秒精度）.</li>
<li>query_duration_ms (UInt64) — 查询消耗的时间（毫秒）.</li>
<li>read_rows (UInt64) — 从参与了查询的所有表和表函数读取的总行数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_rows 包括在所有副本上读取的行总数。 每个副本发送它的 read_rows 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li>
<li>read_bytes (UInt64) — 从参与了查询的所有表和表函数读取的总字节数. 包括：普通的子查询, IN 和 JOIN的子查询. 对于分布式查询 read_bytes 包括在所有副本上读取的字节总数。 每个副本发送它的 read_bytes 值，并且查询的服务器-发起方汇总所有接收到的和本地的值。 缓存卷不会影响此值。</li>
<li>written_rows (UInt64) — 对于 INSERT 查询，为写入的行数。 对于其他查询，值为0。</li>
<li>written_bytes (UInt64) — 对于 INSERT 查询时，为写入的字节数。 对于其他查询，值为0。</li>
<li>result_rows (UInt64) — SELECT 查询结果的行数，或INSERT 的行数。</li>
<li>result_bytes (UInt64) — 存储查询结果的RAM量.</li>
<li>memory_usage (UInt64) — 查询使用的内存.</li>
<li>query (String) — 查询语句.</li>
<li>exception (String) — 异常信息.</li>
<li>exception_code (Int32) — 异常码.</li>
<li>stack_trace (String) — Stack Trace. 如果查询成功完成，则为空字符串。</li>
<li>is_initial_query (UInt8) — 查询类型. 可能的值:<ul>
<li>1 — 客户端发起的查询.</li>
<li>0 — 由另一个查询发起的，作为分布式查询的一部分.</li>
</ul>
</li>
<li>user (String) — 发起查询的用户.</li>
<li>query_id (String) — 查询ID.</li>
<li>address (IPv6) — 发起查询的客户端IP地址.</li>
<li>port (UInt16) — 发起查询的客户端端口.</li>
<li>initial_user (String) — 初始查询的用户名（用于分布式查询执行）.</li>
<li>initial_query_id (String) — 运行初始查询的ID（用于分布式查询执行）.</li>
<li>initial_address (IPv6) — 运行父查询的IP地址.</li>
<li>initial_port (UInt16) — 发起父查询的客户端端口.</li>
<li>interface (UInt8) — 发起查询的接口. 可能的值:<ul>
<li>1 — TCP.</li>
<li>2 — HTTP.</li>
</ul>
</li>
<li>os_user (String) — 运行 clickhouse-client的操作系统用户名.</li>
<li>client_hostname (String) — 运行clickhouse-client 或其他TCP客户端的机器的主机名。</li>
<li>client_name (String) — clickhouse-client 或其他TCP客户端的名称。</li>
<li>client_revision (UInt32) — clickhouse-client 或其他TCP客户端的Revision。</li>
<li>client_version_major (UInt32) — clickhouse-client 或其他TCP客户端的Major version。</li>
<li>client_version_minor (UInt32) — clickhouse-client 或其他TCP客户端的Minor version。</li>
<li>client_version_patch (UInt32) — clickhouse-client 或其他TCP客户端的Patch component。</li>
<li>http_method (UInt8) — 发起查询的HTTP方法. 可能值:<ul>
<li>0 — TCP接口的查询.</li>
<li>1 — GET</li>
<li>2 — POST</li>
</ul>
</li>
<li>http_user_agent (String) — http请求的客户端参数</li>
<li>quota_key (String) — 在quotas 配置里设置的“quota key” （见 keyed).</li>
<li>revision (UInt32) — ClickHouse revision.</li>
<li>ProfileEvents (Map(String, UInt64))) — 不同指标的计数器 </li>
<li>Settings (Map(String, String)) — 当前请求里的setting部分参数</li>
<li>thread_ids (Array(UInt64)) — 参与查询的线程数.</li>
<li>Settings.Names (Array（String)) — 客户端运行查询时更改的设置的名称。 要启用对设置的日志记录更改，请将log_query_settings参数设置为1。</li>
<li>Settings.Values (Array（String)) — Settings.Names 列中列出的设置的值。</li>
</ul>
</blockquote>
<h3 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h3><ul>
<li>查询当前时间内耗cpu最高的sql前10条<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select initial_user, event_time, query,</span><br><span class="line">read_rows, read_bytes,</span><br><span class="line">written_rows, written_bytes,</span><br><span class="line">result_rows, result_bytes,</span><br><span class="line">memory_usage, length(thread_ids) as thread_count</span><br><span class="line">from system.query_log</span><br><span class="line">WHERE event_time &gt; &#39;2022-10-27 18:30:00&#39; AND event_time &lt; &#39;2022-10-27 18:35:00&#39; </span><br><span class="line">and initial_user&lt;&gt;&#39;default&#39;</span><br><span class="line">order by thread_count desc</span><br><span class="line">limit 10;</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/08/clickhouse%E5%89%AF%E6%9C%AC%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">clickhouse副本不同步问题排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-08 16:04:11" itemprop="dateCreated datePublished" datetime="2022-12-08T16:04:11+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-22 17:32:56" itemprop="dateModified" datetime="2022-12-22T17:32:56+08:00">2022-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>clickhouse的Replicated**MergeTree表是通过zookeeper来完成同一个shard之间的副本数据的同步，因为clickhouse本身不是master/slave的架构，我们通过proxy的方式，设定了同一个shard里某个副本是读/写节点角色，另一个节点是读角色，当用户写入时，proxy会路由到读/写节点去完成写入操作，然后通过zk发起同步任务，把日志进入到system.replcation_queue。<br>今天早上用户和我们反馈说数据查询时每一次结果都不一样，用户查询也是通过连我们的proxy进行读节点的路由，所以用户反馈的结果不一致，其实是第一次路由到读/写节点查询结果是a，第二次路由到读节点，查询结果是b，由于a和b两个副本数据没有同步，导致了查询结果不一致，下面是排查的过程。</p>
<h3 id="system-replication-queue"><a href="#system-replication-queue" class="headerlink" title="system.replication_queue"></a>system.replication_queue</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/replication_queue/" target="_blank" rel="noopener">官方地址</a>所解释,该表记录了当前的副本任务队列的所有信息，如下图所示，我们看到当前副本同步出现大量异常错误  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where data_files &gt; 0</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/replicas_data_diff/1.png" alt="clickhouse"><br>如下图所示，<strong>type</strong>字段可以查看到当前是什么类型的操作导致的，发现是<strong>MUTATE_PART</strong>操作，<strong>last_exception</strong>字段显示当前我们在操作ttl时创建的元数据目录下columns.txt无法打开，这个是已知bug。<br><img src="/images/clickhouse/replicas_data_diff/2.png" alt="clickhouse"></p>
<h3 id="system-mutations"><a href="#system-mutations" class="headerlink" title="system.mutations"></a>system.mutations</h3><blockquote>
<p>如<a href="https://clickhouse.com/docs/en/operations/system-tables/mutations" target="_blank" rel="noopener">官方地址</a>，该表包含了所有mutation操作的日志信息，<a href="https://clickhouse.com/docs/en/sql-reference/statements/alter/#mutations" target="_blank" rel="noopener">mutation</a>操作包括修改字段类型/修改表ttl操作/按条件删表的数据/按条件更新表的数据等，这些操作都是异步后台线程去处理，都会去回溯该表的所有parts，需要rewrite每个part的信息并且这个操作还不是原子性的，所以如果某个节点操作失败，可能引发该表无法使用。  </p>
</blockquote>
<p>该表包含字段如下：</p>
<ul>
<li><p>database (String) — 数据库名称.</p>
</li>
<li><p>table (String) — 表名称.</p>
</li>
<li><p>data_path (String) — 本地文件的路径.</p>
</li>
<li><p>mutation_id (String) — mutation的唯一标识，可通过该标识直接kill mutation.</p>
</li>
<li><p>command (String) — mutation命令.</p>
</li>
<li><p>create_time (DateTime) —  mutation创建时间.</p>
</li>
<li><p>block_numbers (Map) — partition_id：需要进行mutation操作的分区id，number：需要进行mutation的分区对应的block序号.</p>
</li>
<li><p>parts_to_do_names (Array) — 即将完成的需要进行mutation的数组.</p>
</li>
<li><p>parts_to_do (Int64) — 准备进行mutation操作的part序号.</p>
</li>
<li><p>is_done (UInt8) — 该操作是否已完成.</p>
</li>
<li><p>latest_failed_part (String) — mutation操作最后失败的part名称.</p>
</li>
<li><p>latest_fail_time (DateTime) — 最后失败的时间.</p>
</li>
<li><p>latest_fail_reason (String) — 最后失败的原因.</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.mutations where latest_failed_part !&#x3D; &#39;&#39;</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/replicas_data_diff/3.png" alt="clickhouse"></p>
<blockquote>
<p>根据异常错误的<strong>command</strong>字段，我们看到错误是通过<code>MATERIALIZE TTL FAST 16070400</code>引起的，mutation操作在写节点处理完成后，也会通过zookeeper进行副本数据同步</p>
</blockquote>
<h3 id="解决副本不同步问题"><a href="#解决副本不同步问题" class="headerlink" title="解决副本不同步问题"></a>解决副本不同步问题</h3><ul>
<li>终止该失败的mutation<br>具体操作语句可以查看<a href="https://clickhouse.com/docs/zh/sql-reference/statements/kill/#:~:text=KILL%20MUTATION%E2%80%8B&amp;text=Tries%20to%20cancel%20and%20remove,list%20of%20mutations%20to%20stop." target="_blank" rel="noopener">官方文档</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill mutation where database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39; and mutation_id &#x3D; &#39;zz&#39;;</span><br></pre></td></tr></table></figure></li>
<li>操作之后，可以通过查询shard里每个副本的count是否一致来判断数据是否已经进行同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(dt), dt from xx.yy group by dt;</span><br></pre></td></tr></table></figure></li>
<li>操作之后，可以通过上面的<code>system.replication_queue</code>表来观察是否开始进行副本数据同步<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from system.replication_queue where type&#x3D;&#39;GET_PART&#39; and database &#x3D; &#39;xx&#39; and table &#x3D; &#39;yy&#39;</span><br></pre></td></tr></table></figure></li>
<li>如果还没恢复，则去对应出错的副本节点，将本地表删除后重建（出错节点可以从上一步里看出来）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists xx.yy</span><br><span class="line">create table xx.yy</span><br></pre></td></tr></table></figure></li>
<li>此时再查询replication_queue表出错的队列应该已经被清理掉了<br>可以继续操作元数据修改</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/11/19/starrocks-load/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/starrocks-load/" class="post-title-link" itemprop="url">starrocks离线构建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-19 19:04:11" itemprop="dateCreated datePublished" datetime="2022-11-19T19:04:11+08:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-07 23:54:44" itemprop="dateModified" datetime="2023-01-07T23:54:44+08:00">2023-01-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sre/" itemprop="url" rel="index"><span itemprop="name">sre</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li><p>入口文件-&gt;StarRocksFE.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">feServer.start();</span><br><span class="line">httpServer.start();</span><br><span class="line">qeService.start();</span><br></pre></td></tr></table></figure>
<p>主要是初始化配置和启动服务，分别是mysql server端口、thrift server端口、http端口</p>
</li>
<li><p>mysq服务启动-&gt;QeService.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void start() throws IOException &#123;</span><br><span class="line">        if (!mysqlServer.start()) &#123;</span><br><span class="line">            LOG.error(&quot;mysql server start failed&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(&quot;QE service start.&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>MysqlServer.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; start MySQL protocol service</span><br><span class="line">&#x2F;&#x2F; return true if success, otherwise false</span><br><span class="line">public boolean start() &#123;</span><br><span class="line">    if (scheduler &#x3D;&#x3D; null) &#123;</span><br><span class="line">        LOG.warn(&quot;scheduler is NULL.&quot;);</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; open server socket</span><br><span class="line">    try &#123;</span><br><span class="line">        serverChannel &#x3D; ServerSocketChannel.open();</span><br><span class="line">        serverChannel.socket().bind(new InetSocketAddress(&quot;0.0.0.0&quot;, port), 2048);</span><br><span class="line">        serverChannel.configureBlocking(true);</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">        LOG.warn(&quot;Open MySQL network service failed.&quot;, e);</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; start accept thread</span><br><span class="line">    listener &#x3D; ThreadPoolManager.newDaemonCacheThreadPool(1, &quot;MySQL-Protocol-Listener&quot;, true);</span><br><span class="line">    running &#x3D; true;</span><br><span class="line">    listenerFuture &#x3D; listener.submit(new Listener());</span><br><span class="line"></span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Listener 监听线程，实现Runnable接口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">    while (running &amp;&amp; serverChannel.isOpen()) &#123;</span><br><span class="line">        SocketChannel clientChannel;</span><br><span class="line">        try &#123;</span><br><span class="line">            clientChannel &#x3D; serverChannel.accept();</span><br><span class="line">            if (clientChannel &#x3D;&#x3D; null) &#123;</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F; submit this context to scheduler</span><br><span class="line">            ConnectContext context &#x3D; new ConnectContext(clientChannel);</span><br><span class="line">            &#x2F;&#x2F; Set catalog here.</span><br><span class="line">            context.setCatalog(Catalog.getCurrentCatalog());</span><br><span class="line">            if (!scheduler.submit(context)) &#123;</span><br><span class="line">                LOG.warn(&quot;Submit one connect request failed. Client&#x3D;&quot; + clientChannel.toString());</span><br><span class="line">                &#x2F;&#x2F; clear up context</span><br><span class="line">                context.cleanup();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            &#x2F;&#x2F; ClosedChannelException</span><br><span class="line">            &#x2F;&#x2F; AsynchronousCloseException</span><br><span class="line">            &#x2F;&#x2F; ClosedByInterruptException</span><br><span class="line">            &#x2F;&#x2F; Other IOException, for example &quot;to many open files&quot; ...</span><br><span class="line">            LOG.warn(&quot;Query server encounter exception.&quot;, e);</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(100);</span><br><span class="line">            &#125; catch (InterruptedException e1) &#123;</span><br><span class="line">                &#x2F;&#x2F; Do nothing</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (Throwable e) &#123;</span><br><span class="line">            &#x2F;&#x2F; NotYetBoundException</span><br><span class="line">            &#x2F;&#x2F; SecurityException</span><br><span class="line">            LOG.warn(&quot;Query server failed when calling accept.&quot;, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">+ submit提交连接的上下文给线程池</span><br></pre></td></tr></table></figure>
<p>// submit one MysqlContext to this scheduler.<br>// return true, if this connection has been successfully submitted, otherwise return false.<br>// Caller should close ConnectContext if return false.<br>public boolean submit(ConnectContext context) {<br>  if (context == null) {</p>
<pre><code>  return false;
</code></pre><p>  }</p>
<p>  context.setConnectionId(nextConnectionId.getAndAdd(1));<br>  // no necessary for nio.<br>  if (context instanceof NConnectContext) {</p>
<pre><code>  return true;
</code></pre><p>  }<br>  if (executor.submit(new LoopHandler(context)) == null) {</p>
<pre><code>  LOG.warn(&quot;Submit one thread failed.&quot;);
  return false;
</code></pre><p>  }<br>  return true;<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoopHandler.java (实现Runnable接口)</span><br></pre></td></tr></table></figure>
<p>public void run() {<br>  try {</p>
<pre><code>  // Set thread local info
  context.setThreadLocalInfo();
  context.setConnectScheduler(ConnectScheduler.this);
  // authenticate check failed.
  if (!MysqlProto.negotiate(context)) {
      return;
  }

  if (registerConnection(context)) {
      MysqlProto.sendResponsePacket(context);
  } else {
      context.getState().setError(&quot;Reach limit of connections&quot;);
      MysqlProto.sendResponsePacket(context);
      return;
  }

  context.setStartTime();
  ConnectProcessor processor = new ConnectProcessor(context);
  processor.loop();
</code></pre><p>  } catch (Exception e) {</p>
<pre><code>  // for unauthrorized access such lvs probe request, may cause exception, just log it in debug level
  if (context.getCurrentUserIdentity() != null) {
      LOG.warn(&quot;connect processor exception because &quot;, e);
  } else {
      LOG.debug(&quot;connect processor exception because &quot;, e);
  }
</code></pre><p>  } finally {</p>
<pre><code>  unregisterConnection(context);
  context.cleanup();
</code></pre><p>  }<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; loop</span><br></pre></td></tr></table></figure>
<p>public void loop() {<br>  while (!ctx.isKilled()) {</p>
<pre><code>  try {
      processOnce();
  } catch (Exception e) {
      // TODO(zhaochun): something wrong
      LOG.warn(&quot;Exception happened in one seesion(&quot; + ctx + &quot;).&quot;, e);
      ctx.setKilled();
      break;
  }
</code></pre><p>  }<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; processOnce</span><br></pre></td></tr></table></figure>
<p>// handle one process<br>public void processOnce() throws IOException {<br>  // set status of query to OK.<br>  ctx.getState().reset();<br>  executor = null;</p>
<p>  // reset sequence id of MySQL protocol<br>  final MysqlChannel channel = ctx.getMysqlChannel();<br>  channel.setSequenceId(0);<br>  // read packet from channel<br>  try {</p>
<pre><code>  packetBuf = channel.fetchOnePacket();
  if (packetBuf == null) {
      throw new IOException(&quot;Error happened when receiving packet.&quot;);
  }
</code></pre><p>  } catch (AsynchronousCloseException e) {</p>
<pre><code>  // when this happened, timeout checker close this channel
  // killed flag in ctx has been already set, just return
  return;
</code></pre><p>  }</p>
<p>  // dispatch<br>  dispatch();<br>  // finalize<br>  finalizeCommand();</p>
<p>  ctx.setCommand(MysqlCommand.COM_SLEEP);<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; dispatch</span><br></pre></td></tr></table></figure>
<p>int code = packetBuf.get();<br>MysqlCommand command = MysqlCommand.fromCode(code);<br>….<br>ctx.setCommand(command);<br>….</p>
</li>
</ul>
<p>switch (command) {<br>    case COM_INIT_DB:<br>        handleInitDb();<br>        break;<br>    case COM_QUIT:<br>        handleQuit();<br>        break;<br>    case COM_QUERY:<br>        handleQuery();<br>        ctx.setStartTime();<br>        break;<br>    case COM_FIELD_LIST:<br>        handleFieldList();<br>        break;<br>    case COM_CHANGE_USER:<br>        handleChangeUser();<br>        break;<br>    case COM_RESET_CONNECTION:<br>        handleResetConnnection();<br>        break;<br>    case COM_PING:<br>        handlePing();<br>        break;<br>    default:<br>        ctx.getState().setError(“Unsupported command(“ + command + “)”);<br>        LOG.warn(“Unsupported command(“ + command + “)”);<br>        break;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ ConnectProcessor.java -&gt; handleQuery</span><br></pre></td></tr></table></figure><br>….<br>List<StatementBase> stmts = analyze(originStmt);<br>for (int i = 0; i &lt; stmts.size(); ++i) {<br>    ctx.getState().reset();<br>    if (i &gt; 0) {<br>        ctx.resetRetureRows();<br>        ctx.setQueryId(UUIDUtil.genUUID());<br>    }<br>    parsedStmt = stmts.get(i);<br>    parsedStmt.setOrigStmt(new OriginStatement(originStmt, i));</p>
<pre><code>executor = new StmtExecutor(ctx, parsedStmt);
ctx.setExecutor(executor);

ctx.setIsLastStmt(i == stmts.size() - 1);

executor.execute();

// do not execute following stmt when current stmt failed, this is consistent with mysql server
if (ctx.getState().getStateType() == QueryState.MysqlStateType.ERR) {
    break;
}

if (i != stmts.size() - 1) {
    // NOTE: set serverStatus after executor.execute(),
    //      because when execute() throws exception, the following stmt will not execute
    //      and the serverStatus with MysqlServerStatusFlag.SERVER_MORE_RESULTS_EXISTS will
    //      cause client error: Packet sequence number wrong
    ctx.getState().serverStatus |= MysqlServerStatusFlag.SERVER_MORE_RESULTS_EXISTS;
    finalizeCommand();
}
</code></pre><p>}<br>….</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; analyze</span><br></pre></td></tr></table></figure>
<p>// analyze the origin stmt and return multi-statements<br>private List<StatementBase> analyze(String originStmt) throws AnalysisException {<br>    LOG.debug(“the originStmts are: {}”, originStmt);<br>    // Parse statement with parser generated by CUP&amp;FLEX<br>    SqlScanner input = new SqlScanner(new StringReader(originStmt), ctx.getSessionVariable().getSqlMode());<br>    SqlParser parser = new SqlParser(input);<br>    try {<br>        return SqlParserUtils.getMultiStmts(parser);<br>    } catch (Error e) {<br>        throw new AnalysisException(“Please check your sql, we meet an error when parsing.”, e);<br>    } catch (AnalysisException e) {<br>        LOG.warn(“origin_stmt: “ + originStmt + “; Analyze error message: “ + parser.getErrorMsg(originStmt), e);<br>        String errorMessage = parser.getErrorMsg(originStmt);<br>        if (errorMessage == null) {<br>            throw e;<br>        } else {<br>            throw new AnalysisException(errorMessage, e);<br>        }<br>    } catch (Exception e) {<br>        // TODO(lingbin): we catch ‘Exception’ to prevent unexpected error,<br>        // should be removed this try-catch clause future.<br>        LOG.warn(“origin_stmt: “ + originStmt + “; exception: “, e);<br>        String errorMessage = e.getMessage();<br>        if (errorMessage == null) {<br>            throw new AnalysisException(“Internal Error”);<br>        } else {<br>            throw new AnalysisException(“Internal Error: “ + errorMessage);<br>        }<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; execute</span><br></pre></td></tr></table></figure><br>……..<br>} else if (parsedStmt instanceof DdlStmt) {<br>    handleDdlStmt();<br>}<br>……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br>  try {<br>      DdlExecutor.execute(context.getCatalog(), (DdlStmt) parsedStmt);<br>      context.getState().setOk();<br>  } catch (QueryStateException e) {<br>      if (e.getQueryState().getStateType() != MysqlStateType.OK) {<br>          LOG.warn(“DDL statement(“ + originStmt.originStmt + “) process failed.”, e);<br>      }<br>      context.setState(e.getQueryState());<br>  } catch (UserException e) {<br>      LOG.warn(“DDL statement(“ + originStmt.originStmt + “) process failed.”, e);<br>      // Return message to info client what happened.<br>      context.getState().setError(e.getMessage());<br>  } catch (Exception e) {<br>      // Maybe our bug<br>      LOG.warn(“DDL statement(“ + originStmt.originStmt + “) process failed.”, e);<br>      context.getState().setError(“Unexpected exception: “ + e.getMessage());<br>  }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ DdlExecutor.java -&gt; execute</span><br></pre></td></tr></table></figure><br>……..<br>else if (ddlStmt instanceof LoadStmt) {<br>    LoadStmt loadStmt = (LoadStmt) ddlStmt;<br>    EtlJobType jobType = loadStmt.getEtlJobType();<br>    if (jobType == EtlJobType.UNKNOWN) {<br>        throw new DdlException(“Unknown load job type”);<br>    }<br>    if (jobType == EtlJobType.HADOOP &amp;&amp; Config.disable_hadoop_load) {<br>        throw new DdlException(“Load job by hadoop cluster is disabled.”</p>
<pre><code>            + &quot; Try using broker load. See &#39;help broker load;&#39;&quot;);
}

catalog.getLoadManager().createLoadJobFromStmt(loadStmt);
</code></pre><p>}<br>……..<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadManager.java -&gt; createLoadJobFromStmt</span><br></pre></td></tr></table></figure><br>public void createLoadJobFromStmt(LoadStmt stmt) throws DdlException {<br>  Database database = checkDb(stmt.getLabel().getDbName());<br>  long dbId = database.getId();<br>  LoadJob loadJob = null;<br>  writeLock();<br>  try {<br>      checkLabelUsed(dbId, stmt.getLabel().getLabelName());<br>      if (stmt.getBrokerDesc() == null &amp;&amp; stmt.getResourceDesc() == null) {<br>          throw new DdlException(“LoadManager only support the broker and spark load.”);<br>      }<br>      if (loadJobScheduler.isQueueFull()) {<br>          throw new DdlException(<br>                  “There are more than “ + Config.desired_max_waiting_jobs + “ load jobs in waiting queue, “</p>
<pre><code>                      + &quot;please retry later.&quot;);
  }
  loadJob = BulkLoadJob.fromLoadStmt(stmt);
  createLoadJob(loadJob);
</code></pre><p>  } finally {<br>      writeUnlock();<br>  }<br>  Catalog.getCurrentCatalog().getEditLog().logCreateLoadJob(loadJob);</p>
<p>  // The job must be submitted after edit log.<br>  // It guarantee that load job has not been changed before edit log.<br>  loadJobScheduler.submitJob(loadJob);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过 &#96;createLoadJobFromStmt&#96; 创建load任务</span><br><span class="line">+ LoadJobScheduler.java -&gt; process</span><br><span class="line">注意：LoadJobScheduler 继承自 MasterDaemon，MasterDaemon 继承自 Daemon，</span><br><span class="line">Daemon继承自Thread，重载了run方法，里面有一个loop，主要执行runOneCycle</span><br><span class="line">MasterDaemon 又重写了 runOneCycle，执行 runAfterCatalogReady 函数</span><br><span class="line">LoadJobScheduler 又重写了 runAfterCatalogReady 主要就是干process处理，里面是一个死循环，不断从LinkedBlockingQueue类型的needScheduleJobs里出栈取要珍惜的job</span><br></pre></td></tr></table></figure><br>while (true) {<br>  // take one load job from queue<br>  LoadJob loadJob = needScheduleJobs.poll();<br>  if (loadJob == null) {<br>      return;<br>  }</p>
<p>  // schedule job<br>  try {<br>      loadJob.execute();<br>  }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ LoadJob.java -&gt; execute</span><br></pre></td></tr></table></figure><br>/**</p>
<ul>
<li>create pending task for load job and add pending task into pool</li>
<li>if job has been cancelled, this step will be ignored<br>*</li>
<li>@throws LabelAlreadyUsedException  the job is duplicated</li>
<li>@throws BeginTransactionException  the limit of load job is exceeded</li>
<li>@throws AnalysisException          there are error params in job</li>
<li><p>@throws DuplicatedRequestException<br>*/<br>public void execute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,</p>
<pre><code>DuplicatedRequestException, LoadException {
</code></pre><p>writeLock();<br>try {</p>
<pre><code>unprotectedExecute();
</code></pre><p>} finally {</p>
<pre><code>writeUnlock();
</code></pre><p>}<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ SparkLoadJob.java -&gt; unprotectedExecuteJob</span><br></pre></td></tr></table></figure>
<p>protected void unprotectedExecuteJob() throws LoadException {<br>// create pending task<br>LoadTask task = new SparkLoadPendingTask(this, fileGroupAggInfo.getAggKeyToFileGroups(),</p>
<pre><code>    sparkResource, brokerDesc);
</code></pre><p>task.init();<br>idToTasks.put(task.getSignature(), task);<br>submitTask(Catalog.getCurrentCatalog().getPendingLoadTaskScheduler(), task);<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; init</span><br></pre></td></tr></table></figure>
<p>public void init() throws LoadException {<br>createEtlJobConf();<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ LoadTask -&gt; exec</span><br></pre></td></tr></table></figure>
<p>@Override<br>protected void exec() {<br>boolean isFinished = false;<br>try {</p>
<pre><code>// execute pending task
executeTask();
// callback on pending task finished
callback.onTaskFinished(attachment);
isFinished = true;
</code></pre><p>} catch (UserException e) {</p>
<pre><code>failMsg.setMsg(e.getMessage() == null ? &quot;&quot; : e.getMessage());
LOG.warn(new LogBuilder(LogKey.LOAD_JOB, callback.getCallbackId())
        .add(&quot;error_msg&quot;, &quot;Failed to execute load task&quot;).build(), e);
</code></pre><p>} catch (Exception e) {</p>
<pre><code>failMsg.setMsg(e.getMessage() == null ? &quot;&quot; : e.getMessage());
LOG.warn(new LogBuilder(LogKey.LOAD_JOB, callback.getCallbackId())
        .add(&quot;error_msg&quot;, &quot;Unexpected failed to execute load task&quot;).build(), e);
</code></pre><p>} finally {</p>
<pre><code>if (!isFinished) {
    // callback on pending task failed
    callback.onTaskFailed(signature, failMsg);
}
</code></pre><p>}<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ SparkLoadPendingTask.java -&gt; executeTask</span><br></pre></td></tr></table></figure>
<p>void executeTask() throws LoadException {<br>LOG.info(“begin to execute spark pending task. load job id: {}”, loadJobId);<br>submitEtlJob();<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure>
<p>private void submitEtlJob() throws LoadException {<br>SparkPendingTaskAttachment sparkAttachment = (SparkPendingTaskAttachment) attachment;<br>// retry different output path<br>etlJobConfig.outputPath = EtlJobConfig.getOutputPath(resource.getWorkingDir(), dbId, loadLabel, signature);<br>sparkAttachment.setOutputPath(etlJobConfig.outputPath);</p>
<p>// handler submit etl job<br>SparkEtlJobHandler handler = new SparkEtlJobHandler();<br>handler.submitEtlJob(loadJobId, loadLabel, etlJobConfig, resource, brokerDesc, sparkLoadAppHandle,</p>
<pre><code>    sparkAttachment);
</code></pre><p>LOG.info(“submit spark etl job success. load job id: {}, attachment: {}”, loadJobId, sparkAttachment);<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkEtlJobHandler.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure>
<p>public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,</p>
<pre><code>                     BrokerDesc brokerDesc, SparkLoadAppHandle handle, SparkPendingTaskAttachment attachment)
</code></pre><p>  throws LoadException {<br>// delete outputPath<br>// init local dir<br>// prepare dpp archive<br>SparkLauncher launcher = new SparkLauncher(envs);<br>// master      |  deployMode<br>// ——————|——————-<br>// yarn        |  cluster<br>// spark://xx  |  client<br>launcher.setMaster(resource.getMaster())</p>
<pre><code>  .setDeployMode(resource.getDeployMode().name().toLowerCase())
  .setAppResource(appResourceHdfsPath)
  .setMainClass(SparkEtlJob.class.getCanonicalName())
  .setAppName(String.format(ETL_JOB_NAME, loadLabel))
  .setSparkHome(sparkHome)
  .addAppArgs(jobConfigHdfsPath)
  .redirectError();
</code></pre><p>// spark configs</p>
<p>// start app<br>State state = null;<br>String appId = null;<br>String logPath = null;<br>String errMsg = “start spark app failed. error: “;<br>try {<br>  Process process = launcher.launch();<br>  handle.setProcess(process);<br>  if (!FeConstants.runningUnitTest) {</p>
<pre><code>  SparkLauncherMonitor.LogMonitor logMonitor = SparkLauncherMonitor.createLogMonitor(handle);
  logMonitor.setSubmitTimeoutMs(GET_APPID_TIMEOUT_MS);
  logMonitor.setRedirectLogPath(logFilePath);
  logMonitor.start();
  try {
      logMonitor.join();
  } catch (InterruptedException e) {
      logMonitor.interrupt();
      throw new LoadException(errMsg + e.getMessage());
  }
</code></pre><p>  }<br>  appId = handle.getAppId();<br>  state = handle.getState();<br>  logPath = handle.getLogPath();<br>} catch (IOException e) {<br>  LOG.warn(errMsg, e);<br>  throw new LoadException(errMsg + e.getMessage());<br>}<br>……….<br>// success<br>attachment.setAppId(appId);<br>attachment.setHandle(handle);<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadJob.java -&gt; onTaskFinished</span><br></pre></td></tr></table></figure>
<p>public void onTaskFinished(TaskAttachment attachment) {<br>if (attachment instanceof SparkPendingTaskAttachment) {</p>
<pre><code>onPendingTaskFinished((SparkPendingTaskAttachment) attachment);
</code></pre><p>}<br>}</p>
</li>
</ul>
<p>private void onPendingTaskFinished(SparkPendingTaskAttachment attachment) {<br>    writeLock();<br>    try {<br>        // check if job has been cancelled<br>        if (isTxnDone()) {<br>            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)<br>                    .add(“state”, state)<br>                    .add(“error_msg”, “this task will be ignored when job is: “ + state)<br>                    .build());<br>            return;<br>        }</p>
<pre><code>    if (finishedTaskIds.contains(attachment.getTaskId())) {
        LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
                .add(&quot;task_id&quot;, attachment.getTaskId())
                .add(&quot;error_msg&quot;, &quot;this is a duplicated callback of pending task &quot;
                        + &quot;when broker already has loading task&quot;)
                .build());
        return;
    }

    // add task id into finishedTaskIds
    finishedTaskIds.add(attachment.getTaskId());

    sparkLoadAppHandle = attachment.getHandle();
    appId = attachment.getAppId();
    etlOutputPath = attachment.getOutputPath();

    executeEtl();
    // log etl state
    unprotectedLogUpdateStateInfo();
} finally {
    writeUnlock();
}
</code></pre><p>}</p>
<p>```</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/11/04/clickhouse-parser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/04/clickhouse-parser/" class="post-title-link" itemprop="url">ClickHouse解析器大揭秘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-11-04 14:09:55 / 修改时间：17:01:49" itemprop="dateCreated datePublished" datetime="2022-11-04T14:09:55+08:00">2022-11-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>我们知道语法分析器的作用是根据给定的<a href="https://baike.baidu.com/item/%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95/2447403" target="_blank" rel="noopener">形式文法</a>对由词法单元（Token）序列构成的输入进行语法检查、并构建由输入的词法单元（Token）组成的数据结构（一般是<a href="https://baike.baidu.com/item/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E6%A0%91/20452541" target="_blank" rel="noopener">语法分析树</a>、<a href="https://baike.baidu.com/item/%E6%8A%BD%E8%B1%A1%E8%AF%AD%E6%B3%95%E6%A0%91/6129952" target="_blank" rel="noopener">抽象语法树</a>等层次化的数据结构）。而一提到语法解析目前市面上有很多语法解析器，其中解析sql更是数不胜数，例如最为人所知的antlr和jflex，而本文的主人公ClickHouse却自己去纯手工打造实现了一套sql解析器，本篇文章就来聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制。</p>
<h2 id="简单入门"><a href="#简单入门" class="headerlink" title="简单入门"></a>简单入门</h2><blockquote>
<p>首先来简单入门解决个小问题，那就是我们如何去连接ck，如何将query传递ck呢，如何设置传递给ck的query长度呢？   </p>
</blockquote>
<h3 id="通过TCP方式请求"><a href="#通过TCP方式请求" class="headerlink" title="通过TCP方式请求"></a>通过TCP方式请求</h3><blockquote>
<p>通过tcp方式使用clickhouse自己的客户端，连接clickhouse，在会话session里先使用<strong>set max_query_size=xx</strong>的方式让当前这个会话修改query的长度，如下图：  </p>
</blockquote>
<p><img src="/images/clickhouse/maxquerysize/1.png" alt="clickhouse"></p>
<h3 id="通过HTTP方式请求"><a href="#通过HTTP方式请求" class="headerlink" title="通过HTTP方式请求"></a>通过HTTP方式请求</h3><blockquote>
<p>通过http方式请求，<a href="http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写">http://ip:port/database?user=xx&amp;password=yy&amp;max_query_size=xx，ck会传递这个参数给setting重写</a><br>注意chproxy只允许最大max_query_size为512mb，超过此长度会直接报错  </p>
</blockquote>
<h3 id="通过sql创建setting授权给登陆用户"><a href="#通过sql创建setting授权给登陆用户" class="headerlink" title="通过sql创建setting授权给登陆用户"></a>通过sql创建setting授权给登陆用户</h3><ol>
<li>创建setting profile<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create settings profile if not exists role_max_query_size SETTINGS max_query_size &#x3D; 100000000000;</span><br></pre></td></tr></table></figure></li>
<li>将profile赋值给某个用户<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant role_max_query_size to prod_voyager_stats_events;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="源码解析ck是如何处理max-query-size的"><a href="#源码解析ck是如何处理max-query-size的" class="headerlink" title="源码解析ck是如何处理max_query_size的"></a>源码解析ck是如何处理max_query_size的</h3><blockquote>
<p>由于源码较多，只抽出具体实现函数进行源码讲解，本次讲解基于clickhouse v20.6.3.28-stable（该版本与最新版出入较大）。  </p>
</blockquote>
<p><img src="/images/clickhouse/maxquerysize/2.png" alt="clickhouse"></p>
<ol>
<li>如上图所示，在<code>HTTPHandler.cpp</code>下进行各种http的协议处理时，有一个变量叫<strong>HTMLForm</strong>类型的<code>params</code>，承载的是http请求里的<code>uri</code>，并且在代码的484行进行了此变量的处理，如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">for (const auto &amp; [key, value] : params)</span><br><span class="line">&#123;</span><br><span class="line">    if (key &#x3D;&#x3D; &quot;database&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (database.empty())</span><br><span class="line">            database &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (key &#x3D;&#x3D; &quot;default_format&quot;)</span><br><span class="line">    &#123;</span><br><span class="line">        if (default_format.empty())</span><br><span class="line">            default_format &#x3D; value;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (param_could_be_skipped(key))</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        &#x2F;&#x2F;&#x2F; Other than query parameters are treated as settings.</span><br><span class="line">        if (!customizeQueryParam(context, key, value))</span><br><span class="line">            settings_changes.push_back(&#123;key, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>而<code>customizeQueryParam</code>会判断该参数param是否等于query，如果是则不会进入setting的设置，再判断是否是param_开头的如果是则会传入context（理解为这次session会话中需要设置的各种上下文内容）则也不会进行setting处理，不是前面2个case则进行setting处理，重载系统默认的setting里的参数，如下图<br><img src="/images/clickhouse/maxquerysize/3.png" alt="clickhouse"></li>
<li>虽然第二步已经设置了setting，但注意代码的512行，这行代码会走向<strong>SettingsConstraints.cpp</strong>类的<strong>checkImpl</strong>校验逻辑里，有一些配置是不允许修改的，例如<strong>profile</strong>，例如配置就是如果已经通过grant授权配置了<strong>setting profile</strong>了，会去看这个用户的相关权限，如果不符合则会直接抛出exception，不再进行处理，注意这里还有一个问题，抛了异常后，不再将此次请求写入到system.query_log中，之后我们会修复此问题<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; For external data we also want settings</span><br><span class="line">context.checkSettingsConstraints(settings_changes);</span><br><span class="line">context.applySettingsChanges(settings_changes);</span><br></pre></td></tr></table></figure></li>
<li>做完setting的约束校验后，都符合条件，则我们已经重载了setting里的<strong>max_query_size</strong>，之后就走入了<strong>executeQuery.cpp</strong>执行query逻辑，在它的构造函数里我们就可以看到query是根据<strong>max_query_size</strong>来读取的，如下图<br><img src="/images/clickhouse/maxquerysize/4.png" alt="clickhouse"></li>
</ol>
<h2 id="深入探知"><a href="#深入探知" class="headerlink" title="深入探知"></a>深入探知</h2><p>介绍完了<strong>max_query_size</strong>的处理逻辑后，其实我们已经大致明白ck在query的处理流程是如何流转的，那么现在问题来了，我们知道可以通过<code>select xx from tb SETTINGS max_query_size=12112</code>这种方式传入自定义的setting参数，但是有些参数有生效，而select语法却对max_query_size不生效，原因是什么呢？好了，别着急现在我们就来解答ck是如何处理setting这层逻辑的。</p>
<h3 id="为什么max-query-size的select中不生效？"><a href="#为什么max-query-size的select中不生效？" class="headerlink" title="为什么max_query_size的select中不生效？"></a>为什么max_query_size的select中不生效？</h3><p><img src="/images/clickhouse/maxquerysize/5.png" alt="clickhouse"><br>原因很简单，只要我们读过了上面的流转过程，就知道max_query_size这个参数的处理系统默认是256kb，那么如果未通过uri方式传入<strong>max_query_size</strong>，则在截取query长度前，默认都是256kb，注意截取query时是还未进行ck的parser逻辑处理的，我们可以看到query里的setting是需要经过ck的parser解析后，才会重载进去(如下图6)，所以呢如果你的select query在256kb范围内，则截取完整query后，经过ck的parser解析出ast树，是会带上新的setting，但此时已经没有意义了，而相反的如果你的query超过了256kb，则只截取到256kb前的query，此时setting也不会走到<strong>ParserSelectQuery</strong>里，同时因为你的query被不完整截取后，会直接报ast语法错误<br><img src="/images/clickhouse/maxquerysize/6.png" alt="图6"></p>
<h2 id="源码看解析器"><a href="#源码看解析器" class="headerlink" title="源码看解析器"></a>源码看解析器</h2><h3 id="1-HTTPHandler-cpp-gt-processQuery"><a href="#1-HTTPHandler-cpp-gt-processQuery" class="headerlink" title="1. HTTPHandler.cpp =&gt; processQuery"></a>1. HTTPHandler.cpp =&gt; processQuery</h3><blockquote>
<p>每一个http请求都在clickhouse都会起一个叫<strong>HTTPHandler</strong>的线程去处理，根据http请求header和body，初始化请求上下文环境：包括session、用户信息、当前database、响应信息等，另外还处理限流，用户权限，根据配置取到setting信息进行设置，本文重点是调用<code>executeQuery</code>方法处理<code>query</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">executeQuery(*in, *used_output.out_maybe_delayed_and_compressed, &#x2F;* allow_into_outfile &#x3D; *&#x2F; false, context,</span><br><span class="line">        [&amp;response] (const String &amp; current_query_id, const String &amp; content_type, const String &amp; format, const String &amp; timezone)</span><br><span class="line">        &#123;</span><br><span class="line">            response.setContentType(content_type);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Query-Id&quot;, current_query_id);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Format&quot;, format);</span><br><span class="line">            response.add(&quot;X-ClickHouse-Timezone&quot;, timezone);</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="2-executeQuery-cpp-gt-executeQuery"><a href="#2-executeQuery-cpp-gt-executeQuery" class="headerlink" title="2. executeQuery.cpp =&gt; executeQuery"></a>2. executeQuery.cpp =&gt; executeQuery</h3><p>从流中读出字节到buffer里，根据设置的<code>max_query_size</code>判断buffer是否已满，复制到LimitReadBuffer里，重点是执行<strong>executeQueryImpl</strong>，返回tuple类型的(ast, stream)，从stream里提取出<strong>pipeline(流水线)</strong>，根据ast构造出<code>IBlockInputStream</code>或者<code>IBlockOutputStream</code>，传给pipeline后执行pipeline的execute方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::tie(ast, streams) &#x3D; executeQueryImpl(begin, end, context, false, QueryProcessingStage::Complete, may_have_tail, &amp;istr);</span><br></pre></td></tr></table></figure></p>
<h3 id="2-executeQuery-cpp-gt-executeQueryImpl"><a href="#2-executeQuery-cpp-gt-executeQueryImpl" class="headerlink" title="2. executeQuery.cpp =&gt; executeQueryImpl"></a>2. executeQuery.cpp =&gt; executeQueryImpl</h3><p>按照解析出的ast，构造出Interpreter，调用Interpreter的exec方法去执行后返回pipeline，执行结果记录到query_log里，最后把构造出对应的ast和pipeline返回<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 这里是实现了ParserQuery对象，继承了IParserBase，IParserBase继承自IParser，等下走到6时，才知道虚函数parseImpl会通过ParserQuery对象实现</span><br><span class="line">ParserQuery parser(end, settings.enable_debug_queries);</span><br><span class="line">.........</span><br><span class="line">ast &#x3D; parseQuery(parser, begin, end, &quot;&quot;, max_query_size, settings.max_parser_depth);</span><br><span class="line">.........</span><br><span class="line">auto interpreter &#x3D; InterpreterFactory::get(ast, context, stage);</span><br><span class="line">.........</span><br><span class="line">res &#x3D; interpreter-&gt;execute();</span><br><span class="line">QueryPipeline &amp; pipeline &#x3D; res.pipeline;</span><br><span class="line">.........</span><br><span class="line">return std::make_tuple(ast, std::move(res));</span><br></pre></td></tr></table></figure></p>
<h3 id="3-parseQuery-cpp-gt-parseQueryAndMovePosition"><a href="#3-parseQuery-cpp-gt-parseQueryAndMovePosition" class="headerlink" title="3. parseQuery.cpp =&gt; parseQueryAndMovePosition"></a>3. parseQuery.cpp =&gt; parseQueryAndMovePosition</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr res &#x3D; tryParseQuery(parser, pos, end, error_message, false, query_description, allow_multi_statements, max_query_size, max_parser_depth);</span><br></pre></td></tr></table></figure>
<h3 id="4-parseQuery-cpp-gt-tryParseQuery"><a href="#4-parseQuery-cpp-gt-tryParseQuery" class="headerlink" title="4. parseQuery.cpp =&gt; tryParseQuery"></a>4. parseQuery.cpp =&gt; tryParseQuery</h3><blockquote>
<p>尝试解析SQL，将sql通过语法树规则装入TokenIterator，返回ASTPtr  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ClickHouse词法分析器是由Tokens和Lexer类来实现，token是最基础的元祖，之间是没有任何关联的，只是一堆词组和符号，通过lexer语法进行解析后，把元祖里的token建立起关系。</span><br><span class="line">Tokens tokens(pos, end, max_query_size);</span><br><span class="line">IParser::Pos token_iterator(tokens, max_parser_depth);</span><br><span class="line">&#x2F;&#x2F; 注意这里，TokenIterator对-&gt;使用了重载，在重载函数里去初始化TOKEN，主要是从第一个字符开始使用pos++的方式进行判断，可以进入Token Lexer::nextTokenImpl()进行查看</span><br><span class="line">if (token_iterator-&gt;isEnd() || token_iterator-&gt;type &#x3D;&#x3D; TokenType::Semicolon) &#123;</span><br><span class="line">    out_error_message &#x3D; &quot;Empty query&quot;;</span><br><span class="line">    pos &#x3D; token_iterator-&gt;begin;</span><br><span class="line">    return nullptr;</span><br><span class="line">&#125;</span><br><span class="line">.....</span><br><span class="line">Expected expected;</span><br><span class="line">......</span><br><span class="line">ASTPtr res;</span><br><span class="line">bool parse_res &#x3D; parser.parse(token_iterator, res, expected);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<code>IParser</code>的<code>parse</code>方法是<code>virtual</code>虚函数，<code>IParser</code>作为接口角色，被<code>IParserBase</code>继承，在<code>IParserBase</code>里实现了<code>parse</code>方法。</p>
</blockquote>
<h3 id="5-IParserBase-cpp-gt-parse"><a href="#5-IParserBase-cpp-gt-parse" class="headerlink" title="5. IParserBase.cpp =&gt; parse"></a>5. IParserBase.cpp =&gt; parse</h3><blockquote>
<p>在解每个token时都会根据当前的token进行预判（parseImpl返回的结果），返回true才会进入下一个子token  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bool IParserBase::parse(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    expected.add(pos, getName());</span><br><span class="line"></span><br><span class="line">    return wrapParseImpl(pos, IncreaseDepthTag&#123;&#125;, [&amp;]</span><br><span class="line">    &#123;</span><br><span class="line">        bool res &#x3D; parseImpl(pos, node, expected);</span><br><span class="line">        if (!res)</span><br><span class="line">            node &#x3D; nullptr;</span><br><span class="line">        return res;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意到parseImpl在IParserBase中是一个虚函数，将被继承自IParserBase类的子类实现，而在 <strong><em> 第2步 </em></strong>中我们定义的子类是ParserQuery，所以此时是直接调用到ParserQuery子类的parseImpl方法</p>
</blockquote>
<h3 id="6-ParserQuery-cpp-gt-parseImpl"><a href="#6-ParserQuery-cpp-gt-parseImpl" class="headerlink" title="6. ParserQuery.cpp =&gt; parseImpl"></a>6. ParserQuery.cpp =&gt; parseImpl</h3><blockquote>
<p>Parser的主要类（也都是继承自IParserBase）分别定义出来后，每个去尝试解析，如果都不在这几个主要Parser里，则返回false，否则返回true，clickhouse把query分类成以下14类，但本质上可以归纳为2类，第一类是有结果输出可对应show/select/desc/create等，第二类是无结果输出可对应insert/use/set等  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">bool ParserQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ParserQueryWithOutput query_with_output_p(enable_explain);</span><br><span class="line">    ParserInsertQuery insert_p(end);</span><br><span class="line">    ParserUseQuery use_p;</span><br><span class="line">    ParserSetQuery set_p;</span><br><span class="line">    ParserSystemQuery system_p;</span><br><span class="line">    ParserCreateUserQuery create_user_p;</span><br><span class="line">    ParserCreateRoleQuery create_role_p;</span><br><span class="line">    ParserCreateQuotaQuery create_quota_p;</span><br><span class="line">    ParserCreateRowPolicyQuery create_row_policy_p;</span><br><span class="line">    ParserCreateSettingsProfileQuery create_settings_profile_p;</span><br><span class="line">    ParserDropAccessEntityQuery drop_access_entity_p;</span><br><span class="line">    ParserGrantQuery grant_p;</span><br><span class="line">    ParserSetRoleQuery set_role_p;</span><br><span class="line">    ParserExternalDDLQuery external_ddl_p;</span><br><span class="line"></span><br><span class="line">    bool res &#x3D; query_with_output_p.parse(pos, node, expected)</span><br><span class="line">        || insert_p.parse(pos, node, expected)</span><br><span class="line">        || use_p.parse(pos, node, expected)</span><br><span class="line">        || set_role_p.parse(pos, node, expected)</span><br><span class="line">        || set_p.parse(pos, node, expected)</span><br><span class="line">        || system_p.parse(pos, node, expected)</span><br><span class="line">        || create_user_p.parse(pos, node, expected)</span><br><span class="line">        || create_role_p.parse(pos, node, expected)</span><br><span class="line">        || create_quota_p.parse(pos, node, expected)</span><br><span class="line">        || create_row_policy_p.parse(pos, node, expected)</span><br><span class="line">        || create_settings_profile_p.parse(pos, node, expected)</span><br><span class="line">        || drop_access_entity_p.parse(pos, node, expected)</span><br><span class="line">        || grant_p.parse(pos, node, expected)</span><br><span class="line">        || external_ddl_p.parse(pos, node, expected);</span><br><span class="line"></span><br><span class="line">    return res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意看这个parseImpl方法，进来后都会先去接<code>ParserQueryWithOutput</code>类解析相关ast，这里类涉及到了<code>explain</code>、<code>select</code>、<code>show</code>，<code>create</code>、<code>alter</code>等相关语法的解析，如果解析不过，则直接报错，解析成功后会处理我们这篇文章中提到的<strong>SETTING</strong>，如下图7定义的，将setting传入的变量存入到<code>s_settings</code>指针中。<br><img src="/images/clickhouse/maxquerysize/7.png" alt="图7"></p>
<h3 id="clcikhouse的parser总结："><a href="#clcikhouse的parser总结：" class="headerlink" title="clcikhouse的parser总结："></a>clcikhouse的parser总结：</h3><ul>
<li><ol>
<li>ClickHouse词法分析器<br>词法解析的主要任务是读入源程序的输入字符、将它们组成词素，生成并输出一个词法单元（Token）序列，每个词法单元对应于一个词素。<code>ClickHouse</code>中的每个词法单元（<code>Token</code>）使用一个<code>struct Tocken</code>结构体对象来进行存储，结构体中存储了词法单元的<code>type</code>和<code>value</code>。<br>ClickHouse词法分析器是由<code>Tokens</code>和<code>Lexer</code>类来实现， <strong><em>DB::Lexer::nextTokenImpl()</em></strong>函数用来对<code>SQL</code>语句进行词法分析的具体实现</li>
</ol>
</li>
<li><ol>
<li>ClickHouse语法解析器<br>ClickHouse中定义了不同的Parser用来对不同类型的SQL语句进行语法分析，例如：ParserInsertQuery（Insert语法分析器）、ParserCreateQuery（Create语法分析器）、ParserAlterQuery（Alter语法分析器）等等。<br>Parser首先判断输入的Token序列是否是该类型的SQL，若是该类型的SQL，则继续检查语法的正确性，正确则生成AST返回，语法错误的则抛出语法错误异常，否则直接返回空AST语法树</li>
</ol>
</li>
</ul>
<p><img src="/images/clickhouse/parser/1.png" alt="clickhouse"></p>
<h3 id="解答setting生效问题"><a href="#解答setting生效问题" class="headerlink" title="解答setting生效问题"></a>解答setting生效问题</h3><p><img src="/images/clickhouse/maxquerysize/8.png" alt="图7"><br>如上图所示，当前原生ck只支持InterpreterSelectQuery和InterpreterInsertQuery对query传入setting进行了重载处理。<br>InterpreterSelectQuery是在自己的构造函数里初始化了setting到context里<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">void InterpreterSelectQuery::initSettings()</span><br><span class="line">&#123;</span><br><span class="line">    auto &amp; query &#x3D; getSelectQuery();</span><br><span class="line">    if (query.settings())</span><br><span class="line">        InterpreterSetQuery(query.settings(), *context).executeForCurrentContext();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>InterpreterInsertQuery是在parser解析出ast后，在<code>executeQueryImpl</code>进行的setting重载context。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto * insert_query &#x3D; ast-&gt;as&lt;ASTInsertQuery&gt;();</span><br><span class="line"></span><br><span class="line">if (insert_query &amp;&amp; insert_query-&gt;settings_ast)</span><br><span class="line">    InterpreterSetQuery(insert_query-&gt;settings_ast, context).executeForCurrentContext();</span><br></pre></td></tr></table></figure><br>剩下的setting都已经是通过Interpreter执行结束后再处理的，对于我们需要在前置传入时没有效果了。</p>
<h3 id="解决业务困扰"><a href="#解决业务困扰" class="headerlink" title="解决业务困扰"></a>解决业务困扰</h3><p>当前我们的离线导入hive2ck，其实是通过将数据写入到临时表，这张临时表是按照目标表的表结构重新创建了一个MergeTree表，通过spark任务将hive数据以流式方式写入到临时表分区，生产出分区对应的多个part，生产过程中我们会将part拉回到临时表对应的detach目录，这个过程叫<a href="http://way.xiaojukeji.com/article/29557" target="_blank" rel="noopener">离线构建</a>，再将再将part通过ck的attach命令激活，这时候临时表就对该分区可见了，然后再通过replace partition的方式，将临时表的分区替换到我们的目标表去，这整个过程，就是我们的hive2ck处理流程，如下图所示：<br><img src="/images/clickhouse/maxquerysize/9.png" alt="图10"></p>
<blockquote>
<p>我们将此离线构建继承到数梦的同步中心后，陆续遇到了业务方来咨询相关问题，下面是问题汇总及如何解决的  </p>
</blockquote>
<h4 id="如何在离线导入中将明细数据写入到关联的物化视图"><a href="#如何在离线导入中将明细数据写入到关联的物化视图" class="headerlink" title="如何在离线导入中将明细数据写入到关联的物化视图"></a>如何在离线导入中将明细数据写入到关联的物化视图</h4><p>熟悉clickhouse的同学们都知道，原生ck对于物化视图的写入，唯一的方式是在明细表通过insert写入时，才会将数据经过物化视图的触发器写入关联的物化视图，而在离线构建过程中，ck是不支持的，但很多业务方跟我们提出这个需求，希望离线构建可以支持将分区数据写入到关联物化视图去，于是我们对ck的replace partition 进行了改造。<br>改造前的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221102&#39; FROM test.visits_basic_tmp;</span><br></pre></td></tr></table></figure><br>改造后的语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221101&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW;</span><br></pre></td></tr></table></figure><br>在离线构建走到替换分区这一步时，我们改造了AstAlterQuery，让<code>ParserAlterQuery</code>增加了对<code>and trigger view</code>的语法解析，解析之后进入到<code>InterpreterAlterQuery</code>时，如果ast返回的trigger view是true，则程序流程会流转到取出明细表元数据，查询是否有关联物化视图，重新构造出类似下面的sql，交过pipeline进行执行，由此将该分区数据写入到物化视图去。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into 物化视图 select from 明细表 where 分区&#x3D;xx</span><br></pre></td></tr></table></figure></p>
<h4 id="分区过大导入失败如何解决"><a href="#分区过大导入失败如何解决" class="headerlink" title="分区过大导入失败如何解决"></a>分区过大导入失败如何解决</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx has timed out! (120000ms) Possible deadlock avoided. Client should retry</span><br></pre></td></tr></table></figure>
<p><img src="/images/clickhouse/maxquerysize/10.png" alt="图10"><br>如上图所示，替换分区前，会给该明细表加一把锁，并设定锁时间（lock_acquire_timeout），系统默认时120s，如果该分区过大，替换过程超过120s，则会爆上面错误，而本文最开始已经讲解过如何处理setting，考虑到ck原生只支持insert和select时interpreter对setting重载，由此进行改造让InterpreterAlterQuery也支持通过sql传入锁时间，如下面语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test.visits_basic REPLACE PARTITION &#39;20221108&#39; FROM test.visits_basic_tmp AND TRIGGER VIEW SETTINGS lock_acquire_timeout&#x3D;86400000;</span><br></pre></td></tr></table></figure><br>因为ParserQueryWithOutput已经对setting进行了解析，而AstAlterQuery其实是继承自ASTQueryWithOutput，所以我们已经获得了setting这一块的ast，无需再自己初始化新的ast，只要在InterpreterAlterQuery里把setting重载就行了，如图11<br><img src="/images/clickhouse/maxquerysize/11.png" alt="图10"></p>
<h4 id="替换分区成功，物化视图数据写入报错如何解决"><a href="#替换分区成功，物化视图数据写入报错如何解决" class="headerlink" title="替换分区成功，物化视图数据写入报错如何解决"></a>替换分区成功，物化视图数据写入报错如何解决</h4><ol>
<li>首先我们在数梦平台上控制了相关的ddl语句修改，如果用户要删明细表字段，则必须先去处理关联的物化视图字段，如果用户要删明细表，则必须先删物化视图</li>
<li>遇到替换分区成功，而物化视图写入失败，报错都是锁明细表超时，对于这种case可直接解锁明细表的锁，让物化视图自己去写，不再锁明细表，所以只需要做简单的锁释放便可</li>
</ol>
<p>以上是对ck进行改造过程中遇到的3个问题，此改造过程主要是满足离线导入可写入物化视图，未来我们还将对ck进行更多改造，以满足不同业务需求，各个 业务线大佬们如果在使用ck过程中有遇到任何问题，欢迎加入ck用户群，和我们一起沟通解决。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/17/starrocks%E5%9C%A8%E6%BB%B4%E6%BB%B4%E7%9A%84%E8%90%BD%E5%9C%B0/" class="post-title-link" itemprop="url">starrocks在滴滴的落地</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-17 14:48:24" itemprop="dateCreated datePublished" datetime="2022-10-17T14:48:24+08:00">2022-10-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-07 23:53:33" itemprop="dateModified" datetime="2023-01-07T23:53:33+08:00">2023-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><h4 id="ClickHouse介绍"><a href="#ClickHouse介绍" class="headerlink" title="ClickHouse介绍"></a>ClickHouse介绍</h4><blockquote>
<p><code>ClickHouse</code>是由俄罗斯的第一大搜索引擎<code>Yandex</code>公司开源的列存数据库。令人惊喜的是，<code>ClickHouse</code>相较于很多商业<code>MPP</code>数据库，比如<code>Vertica</code>，<code>InfiniDB</code>有着极大的性能提升。除了<code>Yandex</code>以外，越来越多的公司开始尝试使用<code>ClickHouse</code>等列存数据库。对于一般的分析业务，结构性较强且数据变更不频繁，可以考虑将需要进行关联的表打平成宽表，放入<code>ClickHouse</code>中。</p>
<ul>
<li>配置丰富，只依赖与<code>Zookeeper</code></li>
<li>线性可扩展性，可以通过添加服务器扩展集群</li>
<li>容错性高，不同分片间采用异步多主复制</li>
<li>单表性能极佳，采用向量计算，支持采样和近似计算等优化手段</li>
<li>功能强大支持多种表引擎</li>
</ul>
</blockquote>
<h4 id="StarRocks介绍"><a href="#StarRocks介绍" class="headerlink" title="StarRocks介绍"></a>StarRocks介绍</h4><blockquote>
<p><code>StarRocks</code>是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容<code>MySQL</code>协议和<code>MySQL</code>生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。<code>StarRocks</code>致力于在全场景<code>OLAP</code>业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</p>
<ul>
<li>不依赖于大数据生态，同时外表的联邦查询可以兼容大数据生态</li>
<li>提供多种不同的模型，支持不同维度的数据建模</li>
<li>支持在线弹性扩缩容，可以自动负载均衡</li>
<li>支持高并发分析查询</li>
<li>实时性好，支持数据秒级写入</li>
<li>兼容MySQL 5.7协议和MySQL生态</li>
</ul>
</blockquote>
<h4 id="二者的对比"><a href="#二者的对比" class="headerlink" title="二者的对比"></a>二者的对比</h4><p><strong>相似之处</strong></p>
<ul>
<li>都可以提供极致的性能</li>
<li>都不依赖于<code>Hadoop</code>生态</li>
<li>底层存储分片都提供了主主的复制高可用机制。</li>
<li>都是<code>MPP</code>架构</li>
<li>都是列式存储</li>
<li>都支持表述SQL语法</li>
<li>都提供了MOLAP库的预聚合能力</li>
</ul>
<p><strong>差异性</strong></p>
<ul>
<li><code>ClickHouse</code>在更适用于大宽表的场景，<code>TP</code>的数据通过<code>CDC</code>工具的，可以考虑在<code>Flink</code>中将需要关联的表打平，以大宽表的形式写入<code>ClickHouse</code></li>
<li><code>StarRocks</code>对于<code>join</code>的能力更强，<code>ClickHouse虽</code>然提供了<code>join</code>的语义，但使用上对大表关联的能力支撑较弱，复杂的关联查询经常会引起<code>OOM</code></li>
<li><code>ClickHouse</code>对高并发的业务并不友好，即使一个查询，也会用服务器一半的<code>CPU</code>去查询</li>
<li><code>StarRocks</code>可以支撑数千用户同时进行分析查询，在部分场景下，高并发能力能够达到万级。<code>StarRocks</code>在数据存储层，采用先分区再分桶的策略，增加了数据的指向性，利用前缀索引可以快读对数据进行过滤和查找，减少磁盘的<code>I/O</code>操作，提升查询性能</li>
<li>对于用户的原有的查询基表的 <code>SQL</code> 语句保持不变，<code>StarRocks</code> 会自动选择一个最优的物化视图，从物化视图中读取数据并计算。用户可以通过 <code>EXPLAIN</code> 命令来检查当前查询是否使用了物化视图。而<code>ClickHouse</code>则需要用户自行指定<code>SQL</code>中所需要使用的物化视图。</li>
</ul>
<h4 id="为什么推荐StarRocks"><a href="#为什么推荐StarRocks" class="headerlink" title="为什么推荐StarRocks"></a>为什么推荐StarRocks</h4><ol>
<li>滴滴大数据<code>OLAP</code>团队目前在维护的引擎有<code>StarRocks</code>、<code>ClickHouse</code>、<code>Druid</code>，3个引擎各有各的特点，现有的OLAP引擎(Kylin、Druid、ClickHouse)多表Join时的性能都比较差，甚至不支持多表Join，现有的引擎<code>Druid</code>虽然有<code>lookup</code>表的能力，但经过实际测试后性能不佳。<code>Apache Kylin</code>实际上也不支持<code>Join</code>，多表的<code>Join</code>需要通过在<code>cube</code>构建的时候底层打成宽表来实现。<code>ClickHouse</code>只支持本地<code>Hash join</code>的模式，不支持分布式<code>Shuffle join</code>，多数情况下灵活性受限，性能表现不佳。</li>
<li><code>OLAP</code>引擎需要同时具备明细数据查询和数据聚合的能力。由于<code>Apache Kylin</code>、<code>Druid</code>不能较好支持明细数据查询，我们引入了<code>ClickHouse</code>，通过在明细表基础上创建相应聚合物化视图来处理，但是不够灵活，对于上层应用来说，查明细和查聚合需要切到不同的表去处理。</li>
<li>目前我们团队在有限人员情况下需要维护这3个引擎的稳定性，导致我们对每一个引擎的理解深度都不够，特别像<code>ClickHouse</code>，运维成本非常高，ClickHouse集群的分片、副本信息，都是通过静态的配置文件的方式进行配置。当整个集群需要扩缩容的时候，就必须通过修改配置文件的方式进行刷新，数据的均衡都需要运维人员介入。此外ClickHouse通过zookeeper来做副本管理，当集群规模变大时，副本数过多会导致zookeeper的压力变大，集群的稳定性也就会相应变差。<br>为解决以上问题，滴滴大数据<code>OLAP</code>团队在2022年初开始调研<code>StarRocks</code>，在全面测试过从上面对<code>StarRocks</code>和<code>ClickHouse</code>的对比，我们也可以明显感受到<code>StarRocks</code>在多数场景下都是优于<code>ClickHouse</code>的，我们希望通过<code>StarRocks</code>来实现<code>OLAP</code>平台的多业务场景的查询引擎统一化。<br><img src="/images/starrocks/helloworld/20.png" alt="1"><br>注：这是我们针对<code>Druid</code>、<code>ClickHouse</code>、<code>StarRocks</code>进行的测试对比，<a href="http://wiki.intra.xiaojukeji.com/pages/viewpage.action?pageId=799050659" target="_blank" rel="noopener">链接</a>。</li>
</ol>
<h3 id="StarRocks特性"><a href="#StarRocks特性" class="headerlink" title="StarRocks特性"></a>StarRocks特性</h3><blockquote>
<p><code>StarRocks</code>的架构设计融合了<code>MPP</code>数据库，以及分布式系统的设计思想，其特性如下所示。</p>
<h4 id="架构精简"><a href="#架构精简" class="headerlink" title="架构精简"></a>架构精简</h4><ul>
<li><code>StarRocks</code>内部通过<code>MPP</code>计算框架完成<code>SQL</code>的具体执行工作。<code>MPP</code>框架能够充分的利用多节点的计算能力，整个查询可以并行执行，从而实现良好的交互式分析体验。</li>
<li><code>StarRocks</code>集群不需要依赖任何其他组件，易部署、易维护和极简的架构设计，降低了<code>StarRocks</code>系统的复杂度和维护成本，同时也提升了系统的可靠性和扩展性。管理员只需要专注于<code>StarRocks</code>系统，无需学习和管理任何其他外部系统。</li>
</ul>
</blockquote>
<h4 id="全面向量化引擎"><a href="#全面向量化引擎" class="headerlink" title="全面向量化引擎"></a>全面向量化引擎</h4><p><code>StarRocks</code>的计算层全面采用了向量化技术，将所有算子、函数、扫描过滤和导入导出模块进行了系统性优化。通过列式的内存布局、适配<code>CPU</code>的<code>SIMD</code>指令集等手段，充分发挥了现代<code>CPU</code>的并行计算能力，从而实现亚秒级别的多维分析能力。</p>
<h4 id="智能查询优化"><a href="#智能查询优化" class="headerlink" title="智能查询优化"></a>智能查询优化</h4><p><code>StarRocks</code>通过<code>CBO</code>优化器 <strong>（Cost Based Optimizer）</strong> 可以对复杂查询自动优化。无需人工干预，就可以通过统计信息合理估算执行成本，生成更优的执行计划，大大提高了<code>AdHoc</code>和<code>ETL</code>场景的数据分析效率。</p>
<h4 id="联邦查询"><a href="#联邦查询" class="headerlink" title="联邦查询"></a>联邦查询</h4><p><code>StarRocks</code>支持使用外表的方式进行联邦查询，当前可以支持<code>Hive</code>、<code>MySQL</code>、<code>Elasticsearch</code>、<code>Iceberg</code>和<code>Hudi</code>类型的外表，您无需通过数据导入，可以直接进行数据查询加速。</p>
<h4 id="高效更新"><a href="#高效更新" class="headerlink" title="高效更新"></a>高效更新</h4><p><code>StarRocks</code>支持明细模型(DUPLICATE KEY)、聚合模型(AGGREGATE KEY)、主键模型(PRIMARY KEY)和更新模型(UNIQUE KEY)，其中主键模型可以按照主键进行<code>Upsert</code>或<code>Delete</code>操作，通过存储和索引的优化可以在并发更新的同时实现高效的查询优化，更好的服务实时数仓的场景。</p>
<h4 id="智能物化视图"><a href="#智能物化视图" class="headerlink" title="智能物化视图"></a>智能物化视图</h4><ul>
<li><code>StarRocks</code>支持智能的物化视图。您可以通过创建物化视图，预先计算生成预聚合表用于加速聚合类查询请求。</li>
<li><code>StarRocks</code>的物化视图能够在数据导入时自动完成汇聚，与原始表数据保持一致。</li>
<li>查询的时候，您无需指定物化视图，<code>StarRocks</code>能够自动选择最优的物化视图来满足查询请求。</li>
</ul>
<h4 id="标准SQL"><a href="#标准SQL" class="headerlink" title="标准SQL"></a>标准SQL</h4><ul>
<li><code>StarRocks</code>支持标准的<code>SQL</code>语法，包括聚合、<code>JOIN</code>、排序、窗口函数和自定义函数等功能。</li>
<li><code>StarRocks</code>可以完整支持<code>TPC-H</code>的22个<code>SQL</code>和<code>TPC-DS</code>的99个<code>SQL</code>。</li>
<li><code>StarRocks</code>兼容<code>MySQL</code>协议语法，可以使用现有的各种客户端工具、<code>BI</code>软件访问<code>StarRocks</code>，对<code>StarRocks</code>中的数据进行拖拽式分析。</li>
</ul>
<h4 id="流批一体"><a href="#流批一体" class="headerlink" title="流批一体"></a>流批一体</h4><ul>
<li><code>StarRocks</code>支持实时和批量两种数据导入方式。</li>
<li><code>StarRocks</code>支持的数据源有<code>Kafka</code>、<code>HDFS</code>和本地文件。</li>
<li><code>StarRocks</code>支持的数据格式有<code>ORC</code>、<code>Parquet</code>和<code>CSV</code>等。</li>
<li><code>StarRocks</code>可以实时消费<code>Kafka</code>数据来完成数据导入，保证数据不丢不重 <strong>（exactly once）</strong>。</li>
<li><code>StarRocks</code>也可以从本地或者远程（HDFS）批量导入数据。</li>
</ul>
<h4 id="高可用易扩展"><a href="#高可用易扩展" class="headerlink" title="高可用易扩展"></a>高可用易扩展</h4><ul>
<li><code>StarRocks</code>的元数据和数据都是多副本存储，并且集群中服务有热备，多实例部署，避免了单点故障。</li>
<li>集群具有自愈能力，可弹性恢复，节点的宕机、下线和异常都不会影响<code>StarRocks</code>集群服务的整体稳定性。</li>
<li><code>StarRocks</code>采用分布式架构，存储容量和计算能力可近乎线性水平扩展。<code>StarRocks</code>单集群的节点规模可扩展到数百节点，数据规模可达到10 PB级别。</li>
<li>扩缩容期间无需停服，可以正常提供查询服务。</li>
<li><code>StarRocks</code>中表模式热变更，可通过一条简单<code>SQL</code>命令动态地修改表的定义，例如增加列、减少列和新建物化视图等。同时，处于模式变更中的表也可以正常导入和查询数据。</li>
</ul>
<h3 id="StarRocks应用场景"><a href="#StarRocks应用场景" class="headerlink" title="StarRocks应用场景"></a>StarRocks应用场景</h3><blockquote>
<p>StarRocks可以满足企业级用户的多种分析需求，具体的业务场景如下所示：</p>
<h4 id="OLAP多维分析"><a href="#OLAP多维分析" class="headerlink" title="OLAP多维分析"></a>OLAP多维分析</h4><ul>
<li>用户行为分析</li>
<li>用户画像、标签分析、圈人</li>
<li>高维业务指标报表</li>
<li>自助式报表平台</li>
<li>业务问题探查分析</li>
<li>跨主题业务分析</li>
<li>财务报表</li>
<li>系统监控分析</li>
</ul>
</blockquote>
<h4 id="实时数仓"><a href="#实时数仓" class="headerlink" title="实时数仓"></a>实时数仓</h4><ul>
<li>电商大促数据分析</li>
<li>教育行业的直播质量分析</li>
<li>物流行业的运单分析</li>
<li>金融行业绩效分析、指标计算</li>
<li>广告投放分析</li>
<li>管理驾驶舱</li>
<li>探针分析APM（Application Performance Management）</li>
</ul>
<h4 id="高并发查询"><a href="#高并发查询" class="headerlink" title="高并发查询"></a>高并发查询</h4><ul>
<li>广告主报表分析</li>
<li>零售行业渠道人员分析</li>
<li>saas行业面向用户分析报表</li>
<li>dashboard多页面分析</li>
</ul>
<h4 id="统一分析"><a href="#统一分析" class="headerlink" title="统一分析"></a>统一分析</h4><p>通过使用一套系统解决多维分析、高并发查询、实时分析和Ad-Hoc查询等场景，降低系统复杂度和多技术栈开发与维护成本。</p>
<h3 id="如何使用StarRocks"><a href="#如何使用StarRocks" class="headerlink" title="如何使用StarRocks"></a>如何使用StarRocks</h3><blockquote>
<p>我们olap团队已经将<code>StarRocks</code>接入到滴滴各个平台，下面会介绍如何使用滴滴内部的平台和工具方便快捷的使用<code>StarRocks</code>引擎。<br><img src="/images/starrocks/helloworld/19.png" alt="1"></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/26/replace-trigger/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/26/replace-trigger/" class="post-title-link" itemprop="url">clickhouse离线导入大揭秘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-26 22:57:11" itemprop="dateCreated datePublished" datetime="2022-09-26T22:57:11+08:00">2022-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-28 14:01:34" itemprop="dateModified" datetime="2022-09-28T14:01:34+08:00">2022-09-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="如何按照hive表的表结构创建clickhouse表"><a href="#如何按照hive表的表结构创建clickhouse表" class="headerlink" title="如何按照hive表的表结构创建clickhouse表"></a>如何按照hive表的表结构创建clickhouse表</h3><p>点击<a href="http://studio.data.didichuxing.com/stream/create-table?tableType=clickHouse" target="_blank" rel="noopener">链接</a>通过数梦的实时平台可以快捷创建对应hive表结构的ck表，如下图所示：<br><img src="/images/clickhouse/hive2clickhouse/1.png" alt="clickhouse"></p>
<h3 id="hive表的数据如何导入clickhouse表"><a href="#hive表的数据如何导入clickhouse表" class="headerlink" title="hive表的数据如何导入clickhouse表"></a>hive表的数据如何导入clickhouse表</h3><h4 id="创建同步任务"><a href="#创建同步任务" class="headerlink" title="创建同步任务"></a>创建同步任务</h4><p>点击<a href="http://sync.data-pre.didichuxing.com/job/offline/edit/818177?step=1" target="_blank" rel="noopener">链接</a>通过数梦的同步中心可以快捷创建hive表映射到ck表的同步任务，如下图所示<br><img src="/images/clickhouse/hive2clickhouse/2.png" alt="clickhouse"></p>
<h4 id="同步任务流程"><a href="#同步任务流程" class="headerlink" title="同步任务流程"></a>同步任务流程</h4><p>如下图所示，我们按照这个流程处理hive数据导入到clickhouse<br><img src="/images/clickhouse/hive2clickhouse/3.png" alt="clickhouse"></p>
<ul>
<li><ol>
<li>创建临时表<br>按照ck表的表结构，我们会在集群的所有写节点创建同样表结构的单机表（MergeTree）引擎</li>
</ol>
</li>
<li><ol>
<li>起spark任务，利用clickhouse-local工具将hive表导入到临时表<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat data.orc | clickhouse-local \</span><br><span class="line">--format&#x3D;Native \</span><br><span class="line">--query&#x3D;&#39;CREATE TABLE input (col1 String, col2 String, col3 String) ENGINE &#x3D; File(ORC, stdin);CREATE TABLE target_table (col1 String, col2 String, col3 String) ENGINE &#x3D; MergeTree() partition by tuple() order by col1;insert into target_table select *,&quot;$year&quot; as year,&quot;$month&quot; as month, &quot;$day&quot; as day from input;optimize table target_table final&#39; \</span><br><span class="line">--config-file&#x3D;config.xmls</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/05/clickhouse-insert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/05/clickhouse-insert/" class="post-title-link" itemprop="url">clickhouse的写入流程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-05 15:09:55" itemprop="dateCreated datePublished" datetime="2022-04-05T15:09:55+08:00">2022-04-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 18:09:00" itemprop="dateModified" datetime="2022-08-22T18:09:00+08:00">2022-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="请求处理"><a href="#请求处理" class="headerlink" title="请求处理"></a>请求处理</h3><p>ck请求处理过程<br><img src="/images/clickhouse/insert/1.png" alt="clickhouse"></p>
<h4 id="客户端请求发给ck，ck接收到请求，放入请求队列"><a href="#客户端请求发给ck，ck接收到请求，放入请求队列" class="headerlink" title="客户端请求发给ck，ck接收到请求，放入请求队列"></a>客户端请求发给ck，ck接收到请求，放入请求队列</h4><h4 id="ck将请求队列的请求分发给Request-handler线程处理"><a href="#ck将请求队列的请求分发给Request-handler线程处理" class="headerlink" title="ck将请求队列的请求分发给Request handler线程处理"></a>ck将请求队列的请求分发给Request handler线程处理</h4><h4 id="Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。"><a href="#Request-handler线程处理请求，解析sql语句，执行sql语句逻辑。" class="headerlink" title="Request handler线程处理请求，解析sql语句，执行sql语句逻辑。"></a>Request handler线程处理请求，解析sql语句，执行sql语句逻辑。</h4><h4 id="简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）"><a href="#简单请求处理使用Request-handler线程直接执行（如create，insert，alter等），-复杂请求处理使用Pipeline-executor执行（如select，-insert-select等）" class="headerlink" title="简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）"></a>简单请求处理使用Request handler线程直接执行（如create，insert，alter等）， 复杂请求处理使用Pipeline executor执行（如select， insert select等）</h4><h4 id="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"><a href="#请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求" class="headerlink" title="请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求"></a>请求线程处理完当前请求后，发送请求结果给给客户端。然后接着处理后续请求</h4><h3 id="insert语句解析执行"><a href="#insert语句解析执行" class="headerlink" title="insert语句解析执行"></a>insert语句解析执行</h3><h4 id="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"><a href="#初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息" class="headerlink" title="初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息"></a>初始化请求上下文环境。包括session，用户信息，当前database等，限流，权限，设置等信息</h4><h4 id="解析sql语句"><a href="#解析sql语句" class="headerlink" title="解析sql语句"></a>解析sql语句</h4><h4 id="检查被写入的表是否存在，是否有写入权限，是否被限流"><a href="#检查被写入的表是否存在，是否有写入权限，是否被限流" class="headerlink" title="检查被写入的表是否存在，是否有写入权限，是否被限流"></a>检查被写入的表是否存在，是否有写入权限，是否被限流</h4><h4 id="对insert数据校验，字段是否存在，是否满足约束"><a href="#对insert数据校验，字段是否存在，是否满足约束" class="headerlink" title="对insert数据校验，字段是否存在，是否满足约束"></a>对insert数据校验，字段是否存在，是否满足约束</h4><h4 id="根据默认值填充空字段和物化字段"><a href="#根据默认值填充空字段和物化字段" class="headerlink" title="根据默认值填充空字段和物化字段"></a>根据默认值填充空字段和物化字段</h4><h4 id="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"><a href="#缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。" class="headerlink" title="缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。"></a>缓存单次insert语句中的数据，insert语句全部接收完成或缓存数据超过一定大小后批量写入数据。</h4><h4 id="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"><a href="#将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree" class="headerlink" title="将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree"></a>将insert的数据写入存储引擎，主要包含StorageDistributed和StorageReplicatedXXMergeTree</h4><h4 id="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"><a href="#检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表" class="headerlink" title="检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表"></a>检查是否有物化视图，如果有使用物化视图逻辑处理insert数据，写入物化视图表</h4><h3 id="分布式表写入"><a href="#分布式表写入" class="headerlink" title="分布式表写入"></a>分布式表写入</h3><blockquote>
<p>分布式表数据写入一般情况下是异步写入，只有对使用 remote(‘addresses_expr’, db, table[, ‘user’[, ‘password’], sharding_key]) 定义的表的写入是同步的。如果分布式表中含有local表或replical的表local副本，直接写本地表。<br><img src="/images/clickhouse/insert/2.png" alt="clickhouse"></p>
</blockquote>
<h4 id="将写入block数据按sharding逻辑分成多个block"><a href="#将写入block数据按sharding逻辑分成多个block" class="headerlink" title="将写入block数据按sharding逻辑分成多个block"></a>将写入block数据按sharding逻辑分成多个block</h4><h4 id="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"><a href="#将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据" class="headerlink" title="将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据"></a>将原insert语句改写，表名改成分布式表对应的底表，数据改成分shard后的block数据</h4><h4 id="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"><a href="#检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎" class="headerlink" title="检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎"></a>检查待写入的每个shard，如果shard在本机，则直接写入实际存储引擎</h4><h4 id="shard在远程，将新insert语句写入远程shard本地缓存文件。"><a href="#shard在远程，将新insert语句写入远程shard本地缓存文件。" class="headerlink" title="shard在远程，将新insert语句写入远程shard本地缓存文件。"></a>shard在远程，将新insert语句写入远程shard本地缓存文件。</h4><h4 id="通知后台线程发送本地缓存中的数据"><a href="#通知后台线程发送本地缓存中的数据" class="headerlink" title="通知后台线程发送本地缓存中的数据"></a>通知后台线程发送本地缓存中的数据</h4><h4 id="后台执行过程"><a href="#后台执行过程" class="headerlink" title="后台执行过程"></a>后台执行过程</h4><h4 id="读远程shard本地缓存目录"><a href="#读远程shard本地缓存目录" class="headerlink" title="读远程shard本地缓存目录"></a>读远程shard本地缓存目录</h4><h4 id="逐个处理每个文件"><a href="#逐个处理每个文件" class="headerlink" title="逐个处理每个文件"></a>逐个处理每个文件</h4><h4 id="根据配置的loadbalance策略，选择合适的机器连接"><a href="#根据配置的loadbalance策略，选择合适的机器连接" class="headerlink" title="根据配置的loadbalance策略，选择合适的机器连接"></a>根据配置的loadbalance策略，选择合适的机器连接</h4><h4 id="将文件的insert语句通过上步连接发送给远程机器执行"><a href="#将文件的insert语句通过上步连接发送给远程机器执行" class="headerlink" title="将文件的insert语句通过上步连接发送给远程机器执行"></a>将文件的insert语句通过上步连接发送给远程机器执行</h4><h4 id="执行成功后删除对应文件"><a href="#执行成功后删除对应文件" class="headerlink" title="执行成功后删除对应文件"></a>执行成功后删除对应文件</h4><h3 id="本地表写入"><a href="#本地表写入" class="headerlink" title="本地表写入"></a>本地表写入</h3><blockquote>
<p>本地表一般是StorageReplicatedXXMergeTree，其写入过程如下：<br><img src="/images/clickhouse/insert/3.png" alt="clickhouse"><br>本地表是以block为最小单元单次写入，一个block中的数据可能是一次insert的全部数据，也可以是部分数据。</p>
</blockquote>
<h4 id="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"><a href="#检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。" class="headerlink" title="检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。"></a>检查当前表part数量，如果part数量过多（接近part数限制）延时写入数据，如果part数量过限制则写入失败。</h4><h4 id="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"><a href="#检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。" class="headerlink" title="检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。"></a>检查写入block中parttition总数是否超过限制。如果超过限制，写入失败。</h4><h4 id="将写入block数据按partition-by-逻辑分成多个block"><a href="#将写入block数据按partition-by-逻辑分成多个block" class="headerlink" title="将写入block数据按partition by 逻辑分成多个block"></a>将写入block数据按partition by 逻辑分成多个block</h4><h4 id="依次将每个分区的block数据写入表中"><a href="#依次将每个分区的block数据写入表中" class="headerlink" title="依次将每个分区的block数据写入表中"></a>依次将每个分区的block数据写入表中</h4><h4 id="创建分区的临时part"><a href="#创建分区的临时part" class="headerlink" title="创建分区的临时part"></a>创建分区的临时part</h4><h4 id="计算part数据的sha1生成part对应的blockid"><a href="#计算part数据的sha1生成part对应的blockid" class="headerlink" title="计算part数据的sha1生成part对应的blockid"></a>计算part数据的sha1生成part对应的blockid</h4><h4 id="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"><a href="#检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。" class="headerlink" title="检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。"></a>检查该blockid在是否存在（表的zk中会记录所有已存在的part的blockid）。如果存在表示插入数据重复，忽略后续步骤。</h4><h4 id="将part信息发布到ck，通知其他副本拉去新添加part"><a href="#将part信息发布到ck，通知其他副本拉去新添加part" class="headerlink" title="将part信息发布到ck，通知其他副本拉去新添加part"></a>将part信息发布到ck，通知其他副本拉去新添加part</h4><h4 id="将临时part加入到commit到mergetree表"><a href="#将临时part加入到commit到mergetree表" class="headerlink" title="将临时part加入到commit到mergetree表"></a>将临时part加入到commit到mergetree表</h4><h4 id="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"><a href="#如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件" class="headerlink" title="如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件"></a>如果配置最小同步副本大于1，则等代其他副本数据同步达到满足条件</h4><h4 id="临时part创建过程"><a href="#临时part创建过程" class="headerlink" title="临时part创建过程"></a>临时part创建过程</h4><ul>
<li>创建block数据对应分区的临时part对象</li>
<li>计算分区min_max索引</li>
<li>处理排序和主键索引</li>
<li>ttl处理</li>
<li>其他二级索引处理</li>
<li>将处理过的block数据和索引写入临时part，根据配置的压缩方式压缩</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/01/clickhouse-materialized-view/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/01/clickhouse-materialized-view/" class="post-title-link" itemprop="url">clickhouse物化视图在滴滴的实践</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-01 14:57:11" itemprop="dateCreated datePublished" datetime="2022-04-01T14:57:11+08:00">2022-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 18:09:00" itemprop="dateModified" datetime="2022-08-22T18:09:00+08:00">2022-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是物化视图"><a href="#什么是物化视图" class="headerlink" title="什么是物化视图"></a>什么是物化视图</h3><ul>
<li>普通视图（View）是从一张或者多张数据库表查询导出的虚拟表，可以反映出基础表之间的数据变化，但是本身是不存储数据的，每次的查询都会从基础表重新聚合出查询结果，所以普通视图查询其实等同于创建视图时的查询语句的查询效率。</li>
<li>物化视图（Materialized View）是查询结果集的一份持久化存储，也称为底表的快照（snapshot），查询结果集的范围很宽泛，可以是基础表中部分数据的一份简单拷贝，也可以是多表join之后产生的结果或其子集，或者原始数据的聚合指标等等。物化视图不会随着基础表的变化而变化，如果要更新数据的话，需要用户手动进行，如周期性执行SQL，或利用触发器等机制。</li>
</ul>
<p><img src="/images/clickhouse/crud/5.png" alt="avatar"></p>
<h3 id="如何创建物化视图"><a href="#如何创建物化视图" class="headerlink" title="如何创建物化视图"></a>如何创建物化视图</h3><blockquote>
<h4 id="MergeTree排序引擎"><a href="#MergeTree排序引擎" class="headerlink" title="MergeTree排序引擎"></a>MergeTree排序引擎</h4></blockquote>
<h4 id="ReplacingMergeTree去重引擎"><a href="#ReplacingMergeTree去重引擎" class="headerlink" title="ReplacingMergeTree去重引擎"></a>ReplacingMergeTree去重引擎</h4><h4 id="AggregatingMergeTree聚合引擎"><a href="#AggregatingMergeTree聚合引擎" class="headerlink" title="AggregatingMergeTree聚合引擎"></a>AggregatingMergeTree聚合引擎</h4><h4 id="UniqueMergeTree-实时去重引擎"><a href="#UniqueMergeTree-实时去重引擎" class="headerlink" title="UniqueMergeTree 实时去重引擎"></a>UniqueMergeTree 实时去重引擎</h4><h4 id="SummingMergeTree-求和引擎"><a href="#SummingMergeTree-求和引擎" class="headerlink" title="SummingMergeTree 求和引擎"></a>SummingMergeTree 求和引擎</h4><h4 id="CollapsingMergeTree-折叠引擎"><a href="#CollapsingMergeTree-折叠引擎" class="headerlink" title="CollapsingMergeTree 折叠引擎"></a>CollapsingMergeTree 折叠引擎</h4><h4 id="VersionedCollapsingMergeTree-版本折叠引擎"><a href="#VersionedCollapsingMergeTree-版本折叠引擎" class="headerlink" title="VersionedCollapsingMergeTree 版本折叠引擎"></a>VersionedCollapsingMergeTree 版本折叠引擎</h4><h3 id="如何写入物化视图"><a href="#如何写入物化视图" class="headerlink" title="如何写入物化视图"></a>如何写入物化视图</h3><h4 id="底表触发"><a href="#底表触发" class="headerlink" title="底表触发"></a>底表触发</h4><h4 id="同步任务触发"><a href="#同步任务触发" class="headerlink" title="同步任务触发"></a>同步任务触发</h4><h4 id="直接写入到分布式表"><a href="#直接写入到分布式表" class="headerlink" title="直接写入到分布式表"></a>直接写入到分布式表</h4><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Blank Lin</p>
  <div class="site-description" itemprop="description">say something about me</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">37</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Blank Lin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
