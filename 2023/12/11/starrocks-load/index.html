<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="背景介绍  什么是StarRocksStarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容MySQL协议和MySQL生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。StarRocks致力于在全场景OLAP业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。    滴滴olap现状当前在滴滴内部,主要大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="深入浅出starrocks离线导入">
<meta property="og:url" content="http://yoursite.com/2023/12/11/starrocks-load/index.html">
<meta property="og:site_name" content="BlankLin">
<meta property="og:description" content="背景介绍  什么是StarRocksStarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容MySQL协议和MySQL生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。StarRocks致力于在全场景OLAP业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。    滴滴olap现状当前在滴滴内部,主要大数据">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/images/starrocks/hive2sr/structure.png">
<meta property="article:published_time" content="2023-12-11T11:04:11.000Z">
<meta property="article:modified_time" content="2023-12-10T16:41:11.019Z">
<meta property="article:author" content="Blank Lin">
<meta property="article:tag" content="hive">
<meta property="article:tag" content="starrocks">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/starrocks/hive2sr/structure.png">

<link rel="canonical" href="http://yoursite.com/2023/12/11/starrocks-load/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深入浅出starrocks离线导入 | BlankLin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BlankLin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BlankLin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">lazy and boring</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2023/12/11/starrocks-load/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Blank Lin">
      <meta itemprop="description" content="say something about me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlankLin">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深入浅出starrocks离线导入
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-11 19:04:11 / 修改时间：00:41:11" itemprop="dateCreated datePublished" datetime="2023-12-11T19:04:11+08:00">2023-12-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><ul>
<li><ol>
<li>什么是StarRocks<br>StarRocks是一款极速全场景MPP企业级数据库产品，具备水平在线扩缩容，金融级高可用，兼容MySQL协议和MySQL生态，提供全面向量化引擎与多种数据源联邦查询等重要特性。StarRocks致力于在全场景OLAP业务上为用户提供统一的解决方案，适用于对性能，实时性，并发能力和灵活性有较高要求的各类应用场景。</li>
</ol>
</li>
<li><ol>
<li>滴滴olap现状<br>当前在滴滴内部,主要大数据olap生态包括clickhouse/druid/starrocks等,而写入olap可以划分为两个方向,分布是实时和离线:<ul>
<li>2.1 实时方向,包括网约车/金融等大部分业务侧均是使用kafka/ddmq作为source端,将数据通过flink实时攒批写入到大数据olap存储侧</li>
<li>2.2 离线方向,包括网约车/金融等大部分业务侧则是将数据通过hive/mysql/hdfs这个source端,经过各项加工后流入到olap这个sink存储侧</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>本文主题<br>本篇文章主要是介绍在starrocks这一侧,如果实现将hive数据进行加工后写入到starrock,涉及到starrokc的源码解析、组件介绍、及相关平台架构</li>
</ol>
</li>
</ul>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>如下图,是当前滴滴内部hive2sr导入的实现架构图,用户主要是通过访问同步中心,配置hive表和sr表的字段映射及默认值,同步中心会将映射关系通过srm开放的http接口传给srm侧,srm进行相关处理后提交给sr的各个组件,组件之间通过thrift server进行rpc通信,而srm则通过http方式进行任务最终状态监听,当任务完成后返回给数梦的任务调度系统,最终完成整个全链路流程.<br><img src="/images/starrocks/hive2sr/structure.png" alt="架构图"></p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ol>
<li>创建目标分区<br>同步中心会将hive2sr当前批次需要导入hive的分区列表传入srm,srm会校验当前sr表对应分区是否存在,不存在则按照hive分区值对应创建出sr表分区  </li>
<li>创建目标分区<br>上面这一步创建出目标分区后,srm会对应目标分区range范围,在sr目标表中创建出对应的临时分区,临时分区的作用是这样的,srm可以在一张已经定义分区规则的分区表上，创建临时分区，并为这些临时分区设定单独的数据分布策略。将hive数据写入指定的临时分区后,通过原子覆盖写操作(调整分区分桶策略)，srm可以将临时分区作为临时可用的数据载体覆盖到对应的目标分区,用以实现覆盖写操作</li>
<li>创建load任务<br>3.1 这个load首先需要配置涉及sr访问hadoop环境的相关参数,这个操作相当于在sr中配置hdfs-site.xml和core-site.xml<br>3.2 其实load任务需要组装出提交给spark的driver任务内容,通过cluster模式,以master on yarn的方式在fe节点提交出spark任务</li>
<li>fe进程调度etl阶段<br>在etl阶段,fe会开启spark driver等待yarn队列资源充足时,提交给yarn取执行spark任务,Spark集群执行ETL完成对导入数据的预处理。包括全局字典构建（BITMAP类型）、分区、排序、聚合等。预处理后的数据按parquet数据格式落盘HDFS存储侧。</li>
<li>etl完成后fe会调度load阶段<br>ETL 任务完成后，FE获取预处理过的每个分片的hdfs数据路径，并调度相关的 broker 执行 load 任务</li>
<li>broker加载hdfs文件后通知be进行push<br>BE 通过 Broker 进程读取 HDFS 数据，转化为 StarRocks 存储格式。此时临时分区完成对应hive分区数据的加载过程</li>
<li>将临时分区替换到目标分区<br>临时分区的数据将完成的替换到目标分区去,而目标分区数据将被删除,查询后将是新的hive分区数据</li>
</ol>
<h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><h4 id="fe进程启动及相关调度处理"><a href="#fe进程启动及相关调度处理" class="headerlink" title="fe进程启动及相关调度处理"></a>fe进程启动及相关调度处理</h4><ul>
<li><p>入口文件-&gt;StarRocksFE.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">feServer.start();</span><br><span class="line">httpServer.start();</span><br><span class="line">qeService.start();</span><br></pre></td></tr></table></figure>
<p>主要是初始化配置和启动服务，分别是mysql server端口、thrift server端口、http端口</p>
</li>
<li><p>mysq服务启动-&gt;QeService.java<br>由于我们都是通过tcp协议来连sr,所以主要关注QeService</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void start() throws IOException &#123;</span><br><span class="line">        if (!mysqlServer.start()) &#123;</span><br><span class="line">            LOG.error(&quot;mysql server start failed&quot;);</span><br><span class="line">            System.exit(-1);</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(&quot;QE service start.&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>MysqlServer.java<br>这里主要是开启mysql协议的服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public boolean start() &#123;</span><br><span class="line">    ......</span><br><span class="line">    &#x2F;&#x2F; 打开fe的mysql协议的socket管道</span><br><span class="line">    &#x2F;&#x2F; 开启一个常驻线程用以监听mysql协议</span><br><span class="line">    listener &#x3D; ThreadPoolManager.newDaemonCacheThreadPool(1, &quot;MySQL-Protocol-Listener&quot;, true);</span><br><span class="line">    running &#x3D; true;</span><br><span class="line">    listenerFuture &#x3D; listener.submit(new Listener());</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交本次连接的上下文到连接调度器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">........</span><br><span class="line">clientChannel &#x3D; serverChannel.accept();</span><br><span class="line">if (clientChannel &#x3D;&#x3D; null) &#123;</span><br><span class="line">    continue;</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 初始化本次session的上下文信息到连接调度器</span><br><span class="line">&#x2F;&#x2F; submit this context to scheduler</span><br><span class="line">ConnectContext context &#x3D; new ConnectContext(clientChannel, sslContext);</span><br><span class="line">&#x2F;&#x2F; Set globalStateMgr here.</span><br><span class="line">context.setGlobalStateMgr(GlobalStateMgr.getCurrentState());</span><br><span class="line">if (!scheduler.submit(context)) &#123;</span><br><span class="line">    LOG.warn(&quot;Submit one connect request failed. Client&#x3D;&quot; + clientChannel.toString());</span><br><span class="line">    &#x2F;&#x2F; clear up context</span><br><span class="line">    context.cleanup();</span><br><span class="line">&#125;</span><br><span class="line">............</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#96;&#96;&#96;  </span><br><span class="line"></span><br><span class="line">+ 提交本次连接的上下文给线程池</span><br></pre></td></tr></table></figure>
<p>public boolean submit(ConnectContext context) {<br>  if (context == null) {</p>
<pre><code>  return false;
</code></pre><p>  }</p>
<p>  context.setConnectionId(nextConnectionId.getAndAdd(1));<br>  // no necessary for nio.<br>  if (context instanceof NConnectContext) {</p>
<pre><code>  return true;
</code></pre><p>  }<br>  // 这里是将Runnable提交到connect-scheduler-pool线程池<br>  if (executor.submit(new LoopHandler(context)) == null) {</p>
<pre><code>  LOG.warn(&quot;Submit one thread failed.&quot;);
  return false;
</code></pre><p>  }<br>  return true;<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoopHandler.java (实现Runnable接口)</span><br></pre></td></tr></table></figure>
<p>public void run() {<br>  …..<br>  // 注册本次连接,sr会计算当前fe节点总的连接数,在每次连接超过1024后进行sleep连接的驱逐流程<br>  if (registerConnection(context)) {</p>
<pre><code>  MysqlProto.sendResponsePacket(context);
</code></pre><p>  } else {</p>
<pre><code>  context.getState().setError(&quot;Reach limit of connections&quot;);
  MysqlProto.sendResponsePacket(context);
  return;
</code></pre><p>  }<br>  ………<br>  // 常驻,进行核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan<br>  ConnectProcessor processor = new ConnectProcessor(context);<br>  processor.loop();<br>  ……….<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; loop</span><br></pre></td></tr></table></figure>
<p>public void loop() {<br>  while (!ctx.isKilled()) {</p>
<pre><code>  try {
      processOnce();
  } catch (Exception e) {
      // TODO(zhaochun): something wrong
      LOG.warn(&quot;Exception happened in one seesion(&quot; + ctx + &quot;).&quot;, e);
      ctx.setKilled();
      break;
  }
</code></pre><p>  }<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; processOnce</span><br></pre></td></tr></table></figure>
<p>// handle one process<br>public void processOnce() throws IOException {<br>  // 重置上下文的状态<br>  ctx.getState().reset();<br>  executor = null;</p>
<p>  // 重置mysql协议的顺序标识符<br>  final MysqlChannel channel = ctx.getMysqlChannel();<br>  channel.setSequenceId(0);<br>  // 从通道里获取数据包<br>  try {</p>
<pre><code>  packetBuf = channel.fetchOnePacket();
  if (packetBuf == null) {
      throw new IOException(&quot;Error happened when receiving packet.&quot;);
  }
</code></pre><p>  } catch (AsynchronousCloseException e) {</p>
<pre><code>  // when this happened, timeout checker close this channel
  // killed flag in ctx has been already set, just return
  return;
</code></pre><p>  }</p>
<p>  // 调度,这里主要是上面介绍核心sql的parser-》analyze-》rewrite-》logical plan-》optimizer-》physical plan过程<br>  dispatch();<br>  ………….<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; dispatch</span><br></pre></td></tr></table></figure>
<p>…….<br>// 这里主要是实现mysql协议的几种状态<br>switch (command) {<br>  case COM_INIT_DB:</p>
<pre><code>  handleInitDb();
  break;
</code></pre><p>  case COM_QUIT:</p>
<pre><code>  handleQuit();
  break;
</code></pre><p>  case COM_QUERY:<br>  // 这里是完整的sql处理的总入口</p>
<pre><code>  handleQuery();
  ctx.setStartTime();
  break;
</code></pre><p>  case COM_FIELD_LIST:</p>
<pre><code>  handleFieldList();
  break;
</code></pre><p>  case COM_CHANGE_USER:</p>
<pre><code>  handleChangeUser();
  break;
</code></pre><p>  case COM_RESET_CONNECTION:</p>
<pre><code>  handleResetConnnection();
  break;
</code></pre><p>  case COM_PING:</p>
<pre><code>  handlePing();
  break;
</code></pre><p>  default:</p>
<pre><code>  ctx.getState().setError(&quot;Unsupported command(&quot; + command + &quot;)&quot;);
  LOG.warn(&quot;Unsupported command(&quot; + command + &quot;)&quot;);
  break;
</code></pre><p>}<br>……</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ ConnectProcessor.java -&gt; handleQuery</span><br></pre></td></tr></table></figure>
<p>….<br>StatementBase parsedStmt = null;<br>try {<br>  ctx.setQueryId(UUIDUtil.genUUID());<br>  List<StatementBase> stmts;<br>  try {</p>
<pre><code>  //通过antlr4进行sql的解析,获取sql解析ast树列表
  stmts = com.starrocks.sql.parser.SqlParser.parse(originStmt, ctx.getSessionVariable());
</code></pre><p>  } catch (ParsingException parsingException) {</p>
<pre><code>  throw new AnalysisException(parsingException.getMessage());
</code></pre><p>  }<br>  // 对ast语法树进行analyze分析过程<br>  for (int i = 0; i &lt; stmts.size(); ++i) {</p>
<pre><code>  ..........
  // Only add the last running stmt for multi statement,
  // because the audit log will only show the last stmt.
  if (i == stmts.size() - 1) {
      addRunningQueryDetail(parsedStmt);
  }

  executor = new StmtExecutor(ctx, parsedStmt);
  ctx.setExecutor(executor);

  ctx.setIsLastStmt(i == stmts.size() - 1);

  executor.execute();

  // 如果sql有一条执行失败,后续不再执行
  if (ctx.getState().getStateType() == QueryState.MysqlStateType.ERR) {
      break;
  }
  ........
</code></pre><p>  }<br>}<br>….</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#### 对sql进行Parser语法解析</span><br><span class="line">+ SqlParser.java -&gt; parser</span><br></pre></td></tr></table></figure>
<p>// 首先，我们需要初始化 StarRocksLexer，即词法解析器。在这里，StarRocksLexer 是根据上文介绍的StarRocksLex.g4 词法文件，使用 Antlr4 自动生成的代码类。<br>StarRocksLexer lexer = new StarRocksLexer(new CaseInsensitiveStream(CharStreams.fromString(sql)));<br>lexer.setSqlMode(sessionVariable.getSqlMode());<br>// 然后，代码将词法解析器 StarRocksLexer 作为参数，传入语法解析器中。语法解析器类StarRocksParser，同样是根据上文介绍的 StarRocks.g4 语法文件自动生成的代码类。<br>CommonTokenStream tokenStream = new CommonTokenStream(lexer);<br>StarRocksParser parser = new StarRocksParser(tokenStream);<br>// 到这里，我们就完成了语法解析类的构建。之后再调用 parser.addErrorListener(new ErrorHandler())，将 Antlr4 的默认错误处理规则，替换为自定义的错误处理逻辑即可。<br>parser.removeErrorListeners();<br>parser.addErrorListener(new ErrorHandler());<br>parser.removeParseListeners();<br>parser.addParseListener(new TokenNumberListener(sessionVariable.getParseTokensLimit(),<br>        Math.max(Config.expr_children_limit, sessionVariable.getExprChildrenLimit())));<br>……<br>List<StatementBase> statements = Lists.newArrayList();<br>// 调用 parser.sqlStatements() 返回值 StarRocksParser.SqlStatementsContext，这是一套 antlr 自定义的抽象语法树，根据语法文件生成。<br>List<StarRocksParser.SingleStatementContext> singleStatementContexts = parser.sqlStatements().singleStatement();<br>for (int idx = 0; idx &lt; singleStatementContexts.size(); ++idx) {<br>    // 将 antlr 的语法树转换为 StarRocks 的抽象语法树<br>    StatementBase statement = (StatementBase) new AstBuilder(sessionVariable.getSqlMode())<br>            .visitSingleStatement(singleStatementContexts.get(idx));<br>    statement.setOrigStmt(new OriginStatement(sql, idx));<br>    statements.add(statement);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StarRocks.g4 -&gt; loadStatement</span><br><span class="line">因为本文主要介绍的导入流程,所以以LoadStatement来举例</span><br></pre></td></tr></table></figure><br>loadStatement<br>    : LOAD LABEL label=labelName<br>        data=dataDescList?<br>        broker=brokerDesc?<br>        (BY system=identifierOrString)?<br>        (PROPERTIES props=propertyList)?<br>    | LOAD LABEL label=labelName<br>        data=dataDescList?<br>        resource=resourceDesc<br>        (PROPERTIES props=propertyList)?<br>    ;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">antlr 的语法文件采用 BNF 范式，用&#39;|&#39;表示分支选项，&#39;?&#39;表达0次或一次，其他符号可类比正则表达式。</span><br><span class="line">+ AstBuilder.java -&gt; visitLoadStatement</span><br><span class="line">Antlr4 会根据语法文件生成一份 Visitor 模式的代码，这样就可以做到动作代码与文法产生式解耦，利于文法产生式的重用。而自定义的 AstBuilder 文件则继承了 StarRocksBaseVisitor，用于将 antlr 内部的 AST 翻译成 StarRocks 自定义的 AST。</span><br></pre></td></tr></table></figure><br>public ParseNode visitLoadStatement(StarRocksParser.LoadStatementContext context) {<br>    // 解析出load里写的label名称<br>    LabelName label = getLabelName(context.labelName());<br>    // 解析出load里的字段映射默认值配置等描述<br>    List<DataDescription> dataDescriptions = null;<br>    if (context.data != null) {<br>        dataDescriptions = context.data.dataDesc().stream().map(this::getDataDescription)<br>                .collect(toList());<br>    }<br>    // 解析出load里关于spark任务配置的property属性<br>    Map<String, String> properties = null;<br>    if (context.props != null) {<br>        properties = Maps.newHashMap();<br>        List<Property> propertyList = visit(context.props.property(), Property.class);<br>        for (Property property : propertyList) {<br>            properties.put(property.getKey(), property.getValue());<br>        }<br>    }<br>    // 解析出load里引用的spark外部资源名称<br>    if (context.resource != null) {<br>        ResourceDesc resourceDesc = getResourceDesc(context.resource);<br>        return new LoadStmt(label, dataDescriptions, resourceDesc, properties);<br>    }<br>    // 解析出broker的配置<br>    BrokerDesc brokerDesc = getBrokerDesc(context.broker);<br>    String cluster = null;<br>    if (context.system != null) {<br>        cluster = ((Identifier) visit(context.system)).getValue();<br>    }<br>    //  visit 的返回值返回一个 AST 的基类，在StarRocks 中称为 ParseNode<br>    return new LoadStmt(label, dataDescriptions, brokerDesc, cluster, properties);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们对 Parser 源码进行了重点分析，包括 ANTLR4、SqlParser 和 ASTBuilder。与此同时，我们还通过一个例子，介绍了如何将一条文本类型的 SQL 语句，一步步解析成 StarRocks 内部使用的 AST。</span><br><span class="line">可以看到，Parser 能判断出用户的 SQL 中是否存在明显的语法错误，如 SQL 语句select * from;会在Parser 阶段报错。但如果 SQL 语句select * from foo;没有语法错误，StarRocks 中也没有 foo 这张表，那么 StarRocks 该如何做到错误处理呢？这就需要依赖下一节的 Analyzer 模块去判断了。</span><br><span class="line"></span><br><span class="line">#### 对sql进行analyzer语法解析</span><br><span class="line">+ StmtExecutor.java-&gt; executor</span><br></pre></td></tr></table></figure><br>……..<br>// 本文主要介绍load流程,所以这里值关注ddl的词法分析过程<br>} else if (parsedStmt instanceof DdlStmt) {<br>    handleDdlStmt();<br>}<br>……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br> if (parsedStmt instanceof ShowStmt) {<br>    com.starrocks.sql.analyzer.Analyzer.analyze(parsedStmt, context);<br>    PrivilegeChecker.check(parsedStmt, context);</p>
<pre><code>QueryStatement selectStmt = ((ShowStmt) parsedStmt).toSelectStmt();
if (selectStmt != null) {
    parsedStmt = selectStmt;
    execPlan = StatementPlanner.plan(parsedStmt, context);
}
</code></pre><p> } else {<br>    // 这里是analyze解析的入口函数<br>        execPlan = StatementPlanner.plan(parsedStmt, context);<br> }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ StatementPlanner.java -&gt; plan</span><br></pre></td></tr></table></figure><br>// 注意在analyze阶段,也是会进行锁库操作(db.readLock();)<br>lock(dbs);<br>try (PlannerProfile.ScopedTimer ignored = PlannerProfile.getScopedTimer(“Analyzer”)) {<br>    Analyzer.analyze(stmt, session);<br>}</p>
<p>PrivilegeChecker.check(stmt, session);<br>if (stmt instanceof QueryStatement) {<br>    OptimizerTraceUtil.logQueryStatement(session, “after analyze:\n%s”, (QueryStatement) stmt);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Analyzer 类是所有语句的语义解析的主入口，采用了 Visitor 设计模式，会根据处理的语句类型的不同，调用不同语句的 Analyzer。不同语句的处理逻辑会包含在单独的语句 Analyzer 中，交由不同的 Analyzer 处理</span><br><span class="line"></span><br><span class="line">+ LoadExecutor.java -&gt; accept</span><br></pre></td></tr></table></figure><br>public <R, C> R accept(AstVisitor<R, C> visitor, C context) {<br>    return visitor.visitLoadStatement(this, context);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;real_analyze.png)</span><br><span class="line"></span><br><span class="line">+ LoadStmtAnalyzer.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public Void visitLoadStatement(LoadStmt statement, ConnectContext context) {<br>    analyzeLabel(statement, context);<br>    analyzeDataDescriptions(statement);<br>    analyzeProperties(statement);<br>    return null;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如上所示,LoadStatement交由LoadStmtAnalyzer来专门处理解析,这样做的优点是，可以将 Statement 的定义文件和 Analyze 的处理逻辑分开，并且同一类型的语句也交由特定的 Analyzer 处理，做到不同功能之间代码的解耦合。</span><br><span class="line"></span><br><span class="line">+ StmtExecutor.java -&gt; handleDdlStmt</span><br></pre></td></tr></table></figure><br>private void handleDdlStmt() {<br>  ……..<br>// 这里是ddl 执行分析的总入口<br>ShowResultSet resultSet = DDLStmtExecutor.execute(parsedStmt, context);<br>  ……..<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; execute</span><br></pre></td></tr></table></figure><br>……..<br>// ddl statement analyze 处理逻辑的基类<br>return stmt.accept(StmtExecutorVisitor.getInstance(), context);<br>……..<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![analyze](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;analyze.png)</span><br><span class="line"></span><br><span class="line">+ DDLStmtExecutor.java -&gt; visitLoadStatement</span><br></pre></td></tr></table></figure><br>public ShowResultSet visitLoadStatement(LoadStmt stmt, ConnectContext context) {<br>    ErrorReport.wrapWithRuntimeException(() -&gt; {<br>        EtlJobType jobType = stmt.getEtlJobType();<br>        if (jobType == EtlJobType.UNKNOWN) {<br>            throw new DdlException(“Unknown load job type”);<br>        }<br>        if (jobType == EtlJobType.HADOOP &amp;&amp; Config.disable_hadoop_load) {<br>            throw new DdlException(“Load job by hadoop cluster is disabled.”</p>
<pre><code>                + &quot; Try using broker load. See &#39;help broker load;&#39;&quot;);
    }
    // 这里进行真正的创建load任务的分析过程
    context.getGlobalStateMgr().getLoadManager().createLoadJobFromStmt(stmt, context);
});
return null;
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### 创建load处理pending阶段</span><br><span class="line"></span><br><span class="line">+ LoadManager.java -&gt; createLoadJobFromStmt</span><br></pre></td></tr></table></figure><br>public void createLoadJobFromStmt(LoadStmt stmt) throws DdlException {<br>    // 校验库元数据<br>  Database database = checkDb(stmt.getLabel().getDbName());<br>  long dbId = database.getId();<br>  LoadJob loadJob = null;<br>  // 加写锁,防止并发<br>  writeLock();<br>  try {<br>    // 校验lable是否已使用<br>      checkLabelUsed(dbId, stmt.getLabel().getLabelName());<br>      if (stmt.getBrokerDesc() == null &amp;&amp; stmt.getResourceDesc() == null) {<br>          throw new DdlException(“LoadManager only support the broker and spark load.”);<br>      }<br>      // 判断queue队列长度是否超过1024,超过就报错<br>      if (loadJobScheduler.isQueueFull()) {<br>          throw new DdlException(<br>                  “There are more than “ + Config.desired_max_waiting_jobs + “ load jobs in waiting queue, “</p>
<pre><code>                      + &quot;please retry later.&quot;);
  }
  // 按load类型初始化出LoadJob
  loadJob = BulkLoadJob.fromLoadStmt(stmt);
  // 在内存中记录该load元数据
  createLoadJob(loadJob);
</code></pre><p>  } finally {<br>      writeUnlock();<br>  }<br>  // 通知bdbje加入load元数据同步<br>  Catalog.getCurrentCatalog().getEditLog().logCreateLoadJob(loadJob);</p>
<p>  // loadJob守护进程从load job schedule queue取出任务去执行<br>  loadJobScheduler.submitJob(loadJob);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过 &#96;createLoadJobFromStmt&#96; 创建load任务</span><br><span class="line">+ LoadJobScheduler.java -&gt; process</span><br><span class="line">注意：LoadJobScheduler 继承自 MasterDaemon，MasterDaemon 继承自 Daemon，</span><br><span class="line">Daemon继承自Thread，重载了run方法，里面有一个loop，主要执行runOneCycle</span><br><span class="line">MasterDaemon 又重写了 runOneCycle，执行 runAfterCatalogReady 函数</span><br><span class="line">LoadJobScheduler 又重写了 runAfterCatalogReady 主要就是干process处理，里面是一个死循环，不断从LinkedBlockingQueue类型的needScheduleJobs里出栈取要执行的job</span><br></pre></td></tr></table></figure><br>while (true) {<br>  // take one load job from queue<br>  LoadJob loadJob = needScheduleJobs.poll();<br>  if (loadJob == null) {<br>      return;<br>  }</p>
<p>  // schedule job<br>  try {<br>      loadJob.execute();<br>  }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadJob.java -&gt; execute</span><br></pre></td></tr></table></figure><br>public void execute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>        DuplicatedRequestException, LoadException {<br>            // 加写锁,这里防止多并发提交<br>    writeLock();<br>    try {<br>        unprotectedExecute();<br>    } finally {<br>        writeUnlock();<br>    }<br>}<br>// 这里主要关注beginTxn,做了事务处理,目的是防止fe节点突然挂掉再拉起后其他follower节点切换到leader节点时会重新执行这个任务,而这个时候旧leader节点已经执行过一段这个任务,导致fe节点之间执行无法同步,所以直接抛出label已经存在不再继续执行<br>public void unprotectedExecute() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException,<br>            DuplicatedRequestException, LoadException {<br>    // check if job state is pending<br>    if (state != JobState.PENDING) {<br>        return;<br>    }<br>    // the limit of job will be restrict when begin txn<br>    beginTxn();<br>    unprotectedExecuteJob();<br>    // update spark load job state from PENDING to ETL when pending task is finished<br>    if (jobType != EtlJobType.SPARK) {<br>        unprotectedUpdateState(JobState.LOADING);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadJob.java -&gt; unprotectedExecuteJob</span><br></pre></td></tr></table></figure><br>protected void unprotectedExecuteJob() throws LoadException {<br>    // create pending task<br>    LoadTask task = new SparkLoadPendingTask(this, fileGroupAggInfo.getAggKeyToFileGroups(),<br>            sparkResource, brokerDesc);<br>    task.init();<br>    idToTasks.put(task.getSignature(), task);<br>    // 注意这里的线程池,初始化时只有5个核心线程(最大线程数),而队列长度只有1024,这里会影响任务长期处于pending状态,如果当前5个任务在下面的spark driver阶段一直等不到yarn资源,则一直处于这个线程执行状态,其他提交进来的任务执行在queue队列中等待<br>    submitTask(Catalog.getCurrentCatalog().getPendingLoadTaskScheduler(), task);<br>    ……….<br>    // this.pendingLoadTaskScheduler = new LeaderTaskExecutor(“pending_load_task_scheduler”, Config.max_broker_load_job_concurrency,Config.desired_max_waiting_jobs, !isCheckpointCatalog);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; init</span><br><span class="line">&#x2F;&#x2F; 初始化任务的配置参数</span><br></pre></td></tr></table></figure><br>public void init() throws LoadException {<br>    createEtlJobConf();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ LoadTask -&gt; exec</span><br><span class="line">&#x2F;&#x2F; 由于SparkLoadTask继承自LoadTask,而LoadTask实现了PriorityRunnable接口,所以上面的submitTask实际上就是自动执行这里的exec方法</span><br></pre></td></tr></table></figure><br>@Override<br>protected void exec() {<br>    boolean isFinished = false;<br>    try {<br>        // execute pending task<br>        executeTask();<br>        // callback on pending task finished<br>        callback.onTaskFinished(attachment);<br>        isFinished = true;<br>    ………<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### load任务处理etl阶段</span><br><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; executeTask</span><br></pre></td></tr></table></figure><br>void executeTask() throws LoadException {<br>    LOG.info(“begin to execute spark pending task. load job id: {}”, loadJobId);<br>    submitEtlJob();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkLoadPendingTask.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>private void submitEtlJob() throws LoadException {<br>    SparkPendingTaskAttachment sparkAttachment = (SparkPendingTaskAttachment) attachment;<br>    // 配置etl阶段清洗出的文件在hdfs上的存放路径<br>    etlJobConfig.outputPath = EtlJobConfig.getOutputPath(resource.getWorkingDir(), dbId, loadLabel, signature);<br>    sparkAttachment.setOutputPath(etlJobConfig.outputPath);</p>
<pre><code>// 提交etl任务
SparkEtlJobHandler handler = new SparkEtlJobHandler();
handler.submitEtlJob(loadJobId, loadLabel, etlJobConfig, resource, brokerDesc, sparkLoadAppHandle,
        sparkAttachment);
LOG.info(&quot;submit spark etl job success. load job id: {}, attachment: {}&quot;, loadJobId, sparkAttachment);
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ SparkEtlJobHandler.java -&gt; submitEtlJob</span><br></pre></td></tr></table></figure><br>public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,<br>                             BrokerDesc brokerDesc, SparkLoadAppHandle handle, SparkPendingTaskAttachment attachment)<br>      throws LoadException {<br>  // delete outputPath<br>  // init local dir<br>  // prepare dpp archive<br>  SparkLauncher launcher = new SparkLauncher(envs);<br>  // master      |  deployMode<br>  // ——————|——————-<br>  // yarn        |  cluster<br>  // spark://xx  |  client<br>  launcher.setMaster(resource.getMaster())<br>          .setDeployMode(resource.getDeployMode().name().toLowerCase())<br>          .setAppResource(appResourceHdfsPath)<br>          .setMainClass(SparkEtlJob.class.getCanonicalName())<br>          .setAppName(String.format(ETL_JOB_NAME, loadLabel))<br>          .setSparkHome(sparkHome)<br>          .addAppArgs(jobConfigHdfsPath)<br>          .redirectError();</p>
<p>  // spark configs</p>
<p>  // start app<br>  State state = null;<br>  String appId = null;<br>  String logPath = null;<br>  String errMsg = “start spark app failed. error: “;<br>  try {<br>     // 提交spark任务给yarn<br>      Process process = launcher.launch();<br>      handle.setProcess(process);<br>      if (!FeConstants.runningUnitTest) {<br>        // 监听spark driver输出的日志,直到任务被yarn接收后,退出driver进程<br>          SparkLauncherMonitor.LogMonitor logMonitor = SparkLauncherMonitor.createLogMonitor(handle);<br>          logMonitor.setSubmitTimeoutMs(GET_APPID_TIMEOUT_MS);<br>          logMonitor.setRedirectLogPath(logFilePath);<br>          logMonitor.start();<br>          try {<br>              logMonitor.join();<br>          } catch (InterruptedException e) {<br>              logMonitor.interrupt();<br>              throw new LoadException(errMsg + e.getMessage());<br>          }<br>      }<br>      appId = handle.getAppId();<br>      state = handle.getState();<br>      logPath = handle.getLogPath();<br>  } catch (IOException e) {<br>      LOG.warn(errMsg, e);<br>      throw new LoadException(errMsg + e.getMessage());<br>  }<br>……….<br>  // success<br>  attachment.setAppId(appId);<br>  attachment.setHandle(handle);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### load任务处理loading阶段</span><br><span class="line">+ 1、spark load 有一个 LoadEtlChecker定时调度任务,每5s执行一次</span><br></pre></td></tr></table></figure><br>protected void runAfterCatalogReady() {<br>    try {<br>        loadManager.processEtlStateJobs();<br>    } catch (Throwable e) {<br>        LOG.warn(“Failed to process one round of LoadEtlChecker with error message {}”, e.getMessage(), e);<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 2、这个定时调度就是定时检查load任务状态,有变化就去更新,简单说就是load状态扭转控制器</span><br></pre></td></tr></table></figure><br>// only for those jobs which have etl state, like SparkLoadJob<br>public void processEtlStateJobs() {<br>    idToLoadJob.values().stream().filter(job -&gt; (job.jobType == EtlJobType.SPARK &amp;&amp; job.state == JobState.ETL))<br>            .forEach(job -&gt; {<br>                try {<br>                    ((SparkLoadJob) job).updateEtlStatus();<br>                } catch (DataQualityException e) {<br>                    LOG.info(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(FailMsg.CancelType.ETL_QUALITY_UNSATISFIED,<br>                                    DataQualityException.QUALITY_FAIL_MSG),<br>                            true, true);<br>                } catch (TimeoutException e) {<br>                    // timeout, retry next time<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                } catch (UserException e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                    job.cancelJobWithoutCheck(new FailMsg(CancelType.ETL_RUN_FAIL, e.getMessage()), true, true);<br>                } catch (Exception e) {<br>                    LOG.warn(“update load job etl status failed. job id: {}”, job.getId(), e);<br>                }<br>            });<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 3、当任务状态走到loading时,会提交load任务,函数是submitPushTasks(),这里简单说就是把表的排序健&#x2F;分区&#x2F;be副本等查询出来,后面要把这些元数据通过rpc调用be来处理</span><br></pre></td></tr></table></figure><br>if (tablet instanceof LocalTablet) {<br>    for (Replica replica : ((LocalTablet) tablet).getImmutableReplicas()) {<br>        long replicaId = replica.getId();<br>        tabletAllReplicas.add(replicaId);<br>        long backendId = replica.getBackendId();<br>        Backend backend = GlobalStateMgr.getCurrentState().getCurrentSystemInfo()<br>                .getBackend(backendId);</p>
<pre><code>    pushTask(backendId, tableId, partitionId, indexId, tabletId,
            replicaId, schemaHash, params, batchTask, tabletMetaStr,
            backend, replica, tabletFinishedReplicas, TTabletType.TABLET_TYPE_DISK);
}

if (tabletAllReplicas.size() == 0) {
    LOG.error(&quot;invalid situation. tablet is empty. id: {}&quot;, tabletId);
}

// check tablet push states
if (tabletFinishedReplicas.size() &gt;= quorumReplicaNum) {
    quorumTablets.add(tabletId);
    if (tabletFinishedReplicas.size() == tabletAllReplicas.size()) {
        fullTablets.add(tabletId);
    }
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、遍历生成出的每个tablet,再去遍历每个副本replica,都会给每个replica提交一个load任务,那这里要注意的是会构造</span><br></pre></td></tr></table></figure><br>PushTask pushTask = new PushTask(backendId, dbId, tableId, partitionId,<br>        indexId, tabletId, replicaId, schemaHash,<br>        0, id, TPushType.LOAD_V2,<br>        TPriority.NORMAL, transactionId, taskSignature,<br>        tBrokerScanRange, params.tDescriptorTable,<br>        params.useVectorized, timezone, tabletType);<br>if (AgentTaskQueue.addTask(pushTask)) {<br>    batchTask.addTask(pushTask);<br>    if (!tabletToSentReplicaPushTask.containsKey(tabletId)) {<br>        tabletToSentReplicaPushTask.put(tabletId, Maps.newHashMap());<br>    }<br>    tabletToSentReplicaPushTask.get(tabletId).put(replicaId, pushTask);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、会通过thrift rpc 通知这个副本的be,主要是这个地方client.submit_tasks(agentTaskRequests);</span><br></pre></td></tr></table></figure><br> // create AgentClient<br>address = new TNetworkAddress(backend.getHost(), backend.getBePort());<br>client = ClientPool.backendPool.borrowObject(address);<br>List<TAgentTaskRequest> agentTaskRequests = new LinkedList<TAgentTaskRequest>();<br>for (AgentTask task : tasks) {<br>    agentTaskRequests.add(toAgentTaskRequest(task));<br>}<br>client.submit_tasks(agentTaskRequests);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、这个submit_tasks方法主要是调用BackendService类,这个类实现在be这一侧</span><br></pre></td></tr></table></figure><br>public com.starrocks.thrift.TAgentResult submit_tasks(java.util.List<com.starrocks.thrift.TAgentTaskRequest> tasks) throws org.apache.thrift.TException<br>{<br>    send_submit_tasks(tasks);<br>    return recv_submit_tasks();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个submit_tasks方法是 Thrift 生成的 Java 客户端代码。它和 C++ 服务端的对应关系是:</span><br><span class="line">1) fe发送 Thrift RPC 请求,调用这个 Java submit_tasks方法。</span><br><span class="line">2) Thrift框架会将参数 tasks 序列化成数据包,发送给服务端be。</span><br><span class="line">3) be服务端 Thrift 接收到数据包,反序列化出 tasks 参数。</span><br><span class="line">4) 根据方法名submit_tasks映射到 C++ 的 BackendService::submit_tasks 方法,将 tasks 作为参数调用它。</span><br><span class="line">5) C++ 方法执行完成,将返回值通过 Thrift 序列化后发送给客户端。</span><br><span class="line">6) Java 端 recv_submit_tasks 反序列化出返回结果,赋值给 submit_tasks 方法的返回值。</span><br><span class="line">7) submit_tasks 返回结果,Java 前端获得调用结果。</span><br><span class="line"></span><br><span class="line">#### be部分如何处理loading的代码逻辑</span><br><span class="line">+ 1、BackendService的rpc服务在启动be时已开启</span><br></pre></td></tr></table></figure><br>// Begin to start services<br>// 1. Start thrift server with ‘be_port’.<br>auto thrift_server = BackendService::create<BackendService>(exec_env, starrocks::config::be_port);<br>if (auto status = thrift_server-&gt;start(); !status.ok()) {<br>    LOG(ERROR) &lt;&lt; “Fail to start BackendService thrift server on port “ &lt;&lt; starrocks::config::be_port &lt;&lt; “: “<br>                &lt;&lt; status;<br>    starrocks::shutdown_logging();<br>    exit(1);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 2、客户端BackendServiceClient::submit_tasks()会序列化tasks参数,然后发送RPC请求到服务端,服务端的RPC框架收到请求,根据方法名映射到BackendService::submit_tasks()</span><br></pre></td></tr></table></figure><br>void BackendService::submit_tasks(TAgentResult&amp; return_value, const std::vector<TAgentTaskRequest>&amp; tasks) {<br>    _agent_server-&gt;submit_tasks(return_value, tasks);<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">+ 3、而agent_server里处理submit_tasks的核心逻辑是再次调用PushTaskWorkerPool下的submit_tasks</span><br></pre></td></tr></table></figure><br>// batch submit push tasks<br>if (!push_divider.empty()) {<br>    LOG(INFO) &lt;&lt; “begin batch submit task: “ &lt;&lt; tasks[0].task_type;<br>    for (const auto&amp; push_item : push_divider) {<br>        const auto&amp; push_type = push_item.first;<br>        auto all_push_tasks = push_item.second;<br>        switch (push_type) {<br>        case TPushType::LOAD_V2:<br>            _push_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        case TPushType::DELETE:<br>        case TPushType::CANCEL_DELETE:<br>            _delete_workers-&gt;submit_tasks(all_push_tasks);<br>            break;<br>        default:<br>            ret_st = Status::InvalidArgument(strings::Substitute(“tasks(type=$0, push_type=$1) has wrong task type”,<br>                                                                    TTaskType::PUSH, push_type));<br>            LOG(WARNING) &lt;&lt; “fail to batch submit push task. reason: “ &lt;&lt; ret_st.get_error_msg();<br>        }<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 4、在TaskWorkPool里处理submit_tasks的,主要是实现批量注册task信息,做去重和幂等校验,成功后进行遍历,核心函数是_convert_task</span><br></pre></td></tr></table></figure><br>for (size_t i = 0; i &lt; task_count; i++) {<br>    if (failed_task[i] == 0) {<br>        auto new_task = _convert_task(*tasks[i], recv_time);<br>        _tasks.emplace_back(std::move(new_task));<br>    }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 5、_convert_task它会将TAgentTaskRequest转换成PushReqAgentTaskRequest类型的任务请求,这个是通用线程池,构造函数里回调了_worker_thread_callback,构造出的EngineBatchLoadTask对象后取执行task</span><br></pre></td></tr></table></figure><br>EngineBatchLoadTask engine_task(push_req, &amp;tablet_infos, agent_task_req-&gt;signature, &amp;status,<br>                                ExecEnv::GetInstance()-&gt;load_mem_tracker());<br>StorageEngine::instance()-&gt;execute_task(&amp;engine_task);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 6、在EngineBatchLoadTask::execute()里追踪了内存使用后,会检查tablet是否存在,目录是否达到存储限制等,继续执行load_v2(sparkload&#x2F;brokerload)的_process,而_process也是做了下校验后提交到_push里</span><br></pre></td></tr></table></figure><br>if (status == STARROCKS_SUCCESS) {<br>    uint32_t retry_time = 0;<br>    while (retry_time &lt; PUSH_MAX_RETRY) {<br>        status = _process();</p>
<pre><code>    if (status == STARROCKS_PUSH_HAD_LOADED) {
        LOG(WARNING) &lt;&lt; &quot;transaction exists when realtime push, &quot;
                        &quot;but unfinished, do not report to fe, signature: &quot;
                        &lt;&lt; _signature;
        break; // not retry anymore
    }
    // Internal error, need retry
    if (status == STARROCKS_ERROR) {
        LOG(WARNING) &lt;&lt; &quot;push internal error, need retry.signature: &quot; &lt;&lt; _signature;
        retry_time += 1;
    } else {
        break;
    }
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 7、在_push()里初始化了PushHandler类,现在才开始处理数据的摄入过程</span><br></pre></td></tr></table></figure><br>vectorized::PushHandler push_handler;<br>res = push_handler.process_streaming_ingestion(tablet, request, type, tablet_info_vec);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 8、在核心逻辑_do_streaming_ingestion函数里,开始检查这个tablet是否能迁移,能的话,拿回tablet的写锁后做load转换过程</span><br></pre></td></tr></table></figure><br>Status st = Status::OK();<br>if (push_type == PUSH_NORMAL_V2) {<br>    st = _load_convert(tablet_vars-&gt;at(0).tablet, &amp;(tablet_vars-&gt;at(0).rowset_to_add));<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 9、而在_load_convert里就是初始化出RowsetWriterContext这个上下文对象和RowsetWriter,RowsetWriter对象主要是需要去读出tablet的每一行,用这个wirter写回</span><br></pre></td></tr></table></figure><br>st = reader-&gt;init(t_scan_range, _request);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “fail to init reader. res=” &lt;&lt; st.to_string() &lt;&lt; “, tablet=” &lt;&lt; cur_tablet-&gt;full_name();<br>    return st;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 10、而reader的初始化就是构造PushBrokerReader对象,在PushBrokerReader里会按照文件类型构造出扫描对象,们目前通过sparkload处理的数据,都是使用parquet格式,所以这个初始化就是初始化出ParquetScanner</span><br></pre></td></tr></table></figure><br>// init scanner<br>FileScanner* scanner = nullptr;<br>switch (t_scan_range.ranges[0].format_type) {<br>case TFileFormatType::FORMAT_PARQUET: {<br>    scanner = new ParquetScanner(_runtime_state.get(), _runtime_profile, t_scan_range, _counter.get());<br>    if (scanner == nullptr) {<br>        return Status::InternalError(“Failed to create scanner”);<br>    }<br>    break;<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 11、再回到PushHandler里的_load_conver函数的读取tablet过程,也是按行来去读一个个chunk,而这个next_chunk在PushBrokerReader里又是如何处理的呢?</span><br></pre></td></tr></table></figure><br>ChunkPtr chunk = ChunkHelper::new_chunk(schema, 0);<br>while (!reader-&gt;eof()) {<br>    st = reader-&gt;next_chunk(&amp;chunk);<br>    if (!st.ok()) {<br>        LOG(WARNING) &lt;&lt; “fail to read next chunk. err=” &lt;&lt; st.to_string() &lt;&lt; “, read_rows=” &lt;&lt; num_rows;<br>        return st;<br>    } else {<br>        if (reader-&gt;eof()) {<br>            break;<br>        }</p>
<pre><code>    st = rowset_writer-&gt;add_chunk(*chunk);
    if (!st.ok()) {
        LOG(WARNING) &lt;&lt; &quot;fail to add chunk to rowset writer&quot;
                        &lt;&lt; &quot;. res=&quot; &lt;&lt; st &lt;&lt; &quot;, tablet=&quot; &lt;&lt; cur_tablet-&gt;full_name()
                        &lt;&lt; &quot;, read_rows=&quot; &lt;&lt; num_rows;
        return st;
    }

    num_rows += chunk-&gt;num_rows();
    chunk-&gt;reset();
}
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 12、next_chunk函数会通过scanner对象,去读每行,直到读完为止,再把读出来的结果转换成chunk</span><br></pre></td></tr></table></figure><br>auto res = _scanner-&gt;get_next();<br>if (res.status().is_end_of_file()) {<br>    _eof = true;<br>    return Status::OK();<br>} else if (!res.ok()) {<br>    return res.status();<br>}</p>
<p>return _convert_chunk(res.value(), chunk);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 13、在parquet_scanner里是如何读每个行的呢?当然是按批来读,把读出的批追加到chunk去,chunk满了就返回,这种方式就是通用的流式数据处理,避免了一次新把整个parquet读到内存里,在这里主要关注到next_batch函数,就是Parquet文件的读取流程</span><br></pre></td></tr></table></figure><br>const TBrokerRangeDesc&amp; range_desc = _scan_range.ranges[_next_file];<br>Status st = create_random_access_file(range_desc, _scan_range.broker_addresses[0], _scan_range.params,<br>                                        CompressionTypePB::NO_COMPRESSION, &amp;file);</p>
<p>14、而在create_random_access_file函数由于parquet_scanner是继承自file_scanner,统一回来按文件类型去创建RandomAccessFile的逻辑<br>int64_t timeout_ms = _state-&gt;query_options().query_timeout * 1000 / 4;<br>timeout_ms = std::max(timeout_ms, static_cast<int64_t>(DEFAULT_TIMEOUT_MS));<br>BrokerFileSystem fs_broker(address, params.properties, timeout_ms);<br>if (config::use_local_filecache_for_broker_random_access_file) {<br>    ASSIGN_OR_RETURN(auto broker_file, fs_broker.new_sequential_file(range_desc.path));</p>
<pre><code>std::string dest_path = create_tmp_file_path();
LOG(INFO) &lt;&lt; &quot;broker load cache file: &quot; &lt;&lt; dest_path;
ASSIGN_OR_RETURN(auto dest_file, FileSystem::Default()-&gt;new_writable_file(dest_path));

auto res = fs::copy(broker_file.get(), dest_file.get(), 10 * 1024 * 1024);
std::shared_ptr&lt;RandomAccessFile&gt; local_file;
ASSIGN_OR_RETURN(local_file, FileSystem::Default()-&gt;new_random_access_file(dest_path));
src_file = std::make_shared&lt;TempRandomAccessFile&gt;(dest_path, local_file);
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+ 15、使用BrokerFileSystem通过broker来打开文件(这里的address就是我们在fe节点里创建tBrokerScanRange传入的),用broker把hdfs上的文件下载到本地临时目录下,再打开本地临时目录</span><br></pre></td></tr></table></figure><br>Status st = call_method(_broker_addr, &amp;BrokerServiceClient::openReader, request, &amp;response);<br>if (!st.ok()) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; st;<br>    return st;<br>}<br>if (response.opStatus.statusCode != TBrokerOperationStatusCode::OK) {<br>    LOG(WARNING) &lt;&lt; “Fail to open “ &lt;&lt; path &lt;&lt; “: “ &lt;&lt; response.opStatus.message;<br>    return to_status(response.opStatus);<br>}</p>
<p>// Get file size.<br>ASSIGN_OR_RETURN(const uint64_t file_size, get_file_size(path));<br>auto stream = std::make_shared<BrokerInputStream>(_broker_addr, response.fd, file_size);<br>return std::make_unique<SequentialFile>(std::move(stream), path);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">注意,这里又是通过rpc来调用broker了</span><br><span class="line"></span><br><span class="line">+ 16、再回来看parquet_scanner里的get_next()函数,就是整个parquet文件的核心逻辑,而核心里的调用就是初始化chunk的逻辑</span><br></pre></td></tr></table></figure><br>Status ParquetScanner::initialize_src_chunk(ChunkPtr<em> chunk) {<br>    SCOPED_RAW_TIMER(&amp;_counter-&gt;init_chunk_ns);<br>    _pool.clear();<br>    (</em>chunk) = std::make_shared<Chunk>();<br>    size_t column_pos = 0;<br>    _chunk_filter.clear();<br>    for (auto i = 0; i &lt; _num_of_columns_from_file; ++i) {<br>        SlotDescriptor<em> slot_desc = _src_slot_descriptors[i];<br>        if (slot_desc == nullptr) {<br>            continue;<br>        }<br>        auto</em> array = _batch-&gt;column(column_pos++).get();<br>        ColumnPtr column;<br>        RETURN_IF_ERROR(new_column(array-&gt;type().get(), slot_desc, &amp;column, &amp;_conv_funcs[i], &amp;_cast_exprs[i]));<br>        column-&gt;reserve(_max_chunk_size);<br>        (*chunk)-&gt;append_column(column, slot_desc-&gt;id());<br>    }<br>    return Status::OK();<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1) 清空内存池</span><br><span class="line">2) 创建新的空chunk</span><br><span class="line">3) 遍历所有columns:</span><br><span class="line">  3.1、如果column在schema中,获取对应的SlotDescriptor</span><br><span class="line">  3.2、根据数组类型和SlotDescriptor创建新的Column</span><br><span class="line">  3.3、预留chunk大小内存</span><br><span class="line">  3.4、将Column添加到chunk</span><br><span class="line">4) 返回OK状态</span><br><span class="line">这样就构造了一个空的、符合schema的chunk。</span><br><span class="line">主要逻辑是:</span><br><span class="line">- 清理内存池重用</span><br><span class="line">- 创建空chunk</span><br><span class="line">- 为每个column创建对应的Column对象</span><br><span class="line">- 预留内存</span><br><span class="line">- 添加到chunk</span><br><span class="line">这是非常标准的按schema构建chunk的过程。</span><br><span class="line">这种实现可以高效构建chunk,同时预留内存减少后续内存分配,并可以重用内存池避免频繁new&#x2F;delete。</span><br><span class="line">构建一个符合schema并预分配内存的空chunk是vectorized执行的基础,这个初始化实现是非常重要的一步</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 演进之路</span><br><span class="line">&gt; 上面主要解读了整个离线导入过程的核心代码,在离线导入模块集成到同步中心后,也遇到了一些问题,进行了3次升级来支持不同业务侧对离线导入的需求,下面主要介绍下几个大feature的升级</span><br><span class="line"></span><br><span class="line">#### 国际化业务需要字符串分区</span><br><span class="line">滴滴内部的国际化业务,由于不同国家有时区问题,所以业务侧在hive建表时总会创建出dt&#x3D;yyyy-mm-dd&#x2F;country_code&#x3D;xx的表结构,但如果要将这种hive表结构导入到sr,在当前官方版本都是不支持的,一直到3.0+版本我们和社区一起共建了string类型分区这个feature,并且在离线导入这一块进行了深度迭代升级,用以支撑国际化业务的快速落地sr,这个feature我们已经和入到官方版本中,当前社区版本需要在3.0+才可以支持string类型分区,而滴滴内部在2.3和2.5都已经支持了string类型分区功能,如下表结构</span><br></pre></td></tr></table></figure><br>CREATE TABLE <code>db</code>.<code>tb</code> (<br>  <code>is_passenger</code> varchar(65533),<br>  <code>country_code</code> varchar(65533),<br>  <code>dt</code> varchar(65533),<br>  <code>pid</code> varchar(65533),<br>  <code>resource_stage_1_show_cnt</code> bigint(20)<br>) ENGINE=OLAP<br>DUPLICATE KEY(<code>is_passenger</code>, <code>country_code</code>, <code>dt</code>)<br>COMMENT “xxxx”<br>PARTITION BY LIST(<code>country_code</code>,<code>dt</code>)(<br>  PARTITION pMX_20231208 VALUES IN ((‘MX’, ‘2023-12-08’))<br>)<br>DISTRIBUTED BY HASH(<code>pid</code>) BUCKETS 48<br>PROPERTIES (<br>“replication_num” = “3”,<br>“in_memory” = “false”,<br>“storage_format” = “DEFAULT”,<br>“enable_persistent_index” = “false”<br>);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### spark任务提交由于yarn资源缺乏只能等待5分钟便终止等待load退出</span><br><span class="line">滴滴内部yarn资源队列core和memory均做了一定限制,在降本增效的前提下,很多业务侧无法再去扩容yarn队列,而sr官方社区版本在spark任务提交给yarn之后,默认只给等待5分钟,如果始终等不到yarn接收任务,则自动退出,针对这种特定场景,我们优化了load任务的执行流程,引入了&#96;&quot;spark_load_submit_timeout&quot; &#x3D; &quot;7200&quot;&#96;参数,在每个load任务提交后都可以自动化配置这个任务需要等待yarn多少时间,并且如果spark任务确实等待超时后无法提交,也加入了通知yarn去杀死这个等待的spark任务,防止后续等待到yarn资源后又再次提交执行,而sr这一侧load已经退出,白白浪费资源,如下load结构</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.lable_name_xxx (<br>  DATA INFILE(<br>    “hdfs://DClusterUS1/xxxxx”<br>  )<br>  INTO TABLE tb<br>  TEMPORARY PARTITION(temp<strong>partition)<br>  FORMAT AS “ORC”<br>  (<code>column_1</code>, <code>column_2</code>)<br>  SET (<br>    <code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br>    <code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>    <code>dt</code> = “2023-12-09”,<br>    <code>country_code</code> = “MX”<br>)<br>) WITH RESOURCE ‘external_spark_resource’ (<br>  “spark.yarn.tags” = “xxxxxxx”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 由于spark driver进程在fe节点中占用过多内存,会导致cgroup自动杀死fe进程</span><br><span class="line">目前我们的k8s集群对fe节点的配置只有12g,所以大批量spark driver进程在fe节点中,按照之前统计每个dirver进程大约会占用200-300mb,而fe进程的元数据占用内存大约在5g-10g之间,所以留给spark driverfe进程的可用内存其实非常少,但是目前大量业务侧使用离线导入功能,这样就会造成fe进程不定时被kill的困局,针对这个场景,我们开发了load限流功能,将并发提交的任务转入限流队列中,由队列资源来控制当前任务的并行度,在fe节点内存资源达到高峰期时,进行相应的限流策略,用以保障fe节点的稳定性,如下所示</span><br><span class="line">![架构图](&#x2F;images&#x2F;starrocks&#x2F;hive2sr&#x2F;structure.png)</span><br><span class="line"></span><br><span class="line">#### 由于hivesql写入的hive表产出的hdsf文件自动生成的格式是_col1..._coln,在通过字段导入时会造成hive表字段和sr表字段映射失败</span><br><span class="line">在离线导入功能演进过程中,这个场景在大量业务侧被迫切提出需求,业务侧由于业务口径的变更,需要对hive表进行修改,而一修改hive表结构,历史数据回溯时就会出现分区映射的hdfs文件无法正确映射成功,这个场景和通过hive sql导入hive表时一样的,hive和sr之间字段无法映射成功,就造成导入失败,针对这一场景我们升级了离线导入模块,引入了spark sql模式来读取hive表,而不是之前的spark 文件方式读取,这种方式成功解决了用户变更表字段回溯场景和hive sql导入场景,如下load结构:</span><br></pre></td></tr></table></figure><br>LOAD LABEL db.label_name (<br>DATA FROM TABLE hive_tb<br>INTO TABLE sr_tb<br>TEMPORARY PARTITION(temp</strong>partition)<br>SET (<br><code>column_1</code> = if(<code>column_1</code> is null or <code>column_1</code> = “null”, null, <code>column_1</code>),<br><code>column_2</code> = if(<code>column_2</code> is null or <code>column_2</code> = “null”, null, <code>column_2</code>),<br>…….<br>)<br>WHERE (<code>dt</code> = ‘2023-12-09’)<br>)WITH RESOURCE ‘spark_external_resource’ (<br>  “spark.yarn.tags” = “h2s_foit_150748320231209914ff11b87c94c85947ab13f84ff4622”,<br>  “spark.dynamicAllocation.enabled” = “true”,<br>  “spark.executor.memory” = “3g”,<br>  “spark.executor.memoryOverhead” = “2g”,<br>  “spark.streaming.batchDuration” = “5”,<br>  “spark.executor.cores” = “1”,<br>  “spark.yarn.executor.memoryOverhead” = “2g”,<br>  “spark.speculation” = “false”,<br>  “spark.dynamicAllocation.minExecutors” = “2”,<br>  “spark.dynamicAllocation.maxExecutors” = “100”<br>) PROPERTIES (<br>  “timeout” = “36000”,<br>  “spark_load_submit_timeout” = “7200”<br>)<br>```<br>注意:目前spark sql导入只在明细模型(duplicate key)和聚合模型(aggregate key)上支持</p>
<h3 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h3><p>目前starrocks离线导入功能已经集成在滴滴的同步中心,用户可以通过同步中心按hive表结构自动创建sr表结构,并且配置出hive表字段和sr表字段的映射关系后,构造出导入配置相应参数,调用sr的离线导入模块进行数据导入,而这个导入功能我们在2024年也会有以下规划:</p>
<h4 id="spark-sql支持主键模型-primary-key"><a href="#spark-sql支持主键模型-primary-key" class="headerlink" title="spark sql支持主键模型(primary key)"></a>spark sql支持主键模型(primary key)</h4><p>目前有一部分业务侧会通过主键模型将hive分区数据定时导回给sr,当前还不支持spark sql方式导入,2024年我们会进行这个feature开发</p>
<h4 id="独立sr构建工具"><a href="#独立sr构建工具" class="headerlink" title="独立sr构建工具"></a>独立sr构建工具</h4><p>当前的etl构建过程仍然需要在fe进程中进行交互处理,但fe节点我们在k8s初始化时只有12g,虽然在演进之路里我们加入了限流功能来保障fe稳定性,但是这个方式也会将hive导入耗时加大,对于一些需要高优保障的任务仍然需要按时产出,所以我们规划在2024年可以将这个构建工具从sr的fe进程中独立出来,单独部署在构建集群中,在完成etl阶段后,通过rpc方式通知给sr的fe进程来拉取</p>
<h4 id="hive外表联邦查询"><a href="#hive外表联邦查询" class="headerlink" title="hive外表联邦查询"></a>hive外表联邦查询</h4><p>当前很多业务侧的hive表分区压缩后也会在100g+,这种case导入到sr后查询性能也不一定会很好,但是导入过程却很浪费资源,我们计划在2024年完成hive外表的集成功能,让用户可以直接通过sr来查询hive表,而不需要再进行分区导入这一部分操作</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hive/" rel="tag"># hive</a>
              <a href="/tags/starrocks/" rel="tag"># starrocks</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/04/linux%E5%B8%B8%E7%94%A8%E6%8E%92%E6%9F%A5%E5%91%BD%E4%BB%A4/" rel="prev" title="常用的linux排查命令">
      <i class="fa fa-chevron-left"></i> 常用的linux排查命令
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#背景介绍"><span class="nav-number">1.</span> <span class="nav-text">背景介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#架构设计"><span class="nav-number">2.</span> <span class="nav-text">架构设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本原理"><span class="nav-number">3.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#源码解析"><span class="nav-number">4.</span> <span class="nav-text">源码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#fe进程启动及相关调度处理"><span class="nav-number">4.1.</span> <span class="nav-text">fe进程启动及相关调度处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#未来规划"><span class="nav-number">5.</span> <span class="nav-text">未来规划</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#spark-sql支持主键模型-primary-key"><span class="nav-number">5.1.</span> <span class="nav-text">spark sql支持主键模型(primary key)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#独立sr构建工具"><span class="nav-number">5.2.</span> <span class="nav-text">独立sr构建工具</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hive外表联邦查询"><span class="nav-number">5.3.</span> <span class="nav-text">hive外表联邦查询</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Blank Lin</p>
  <div class="site-description" itemprop="description">say something about me</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Blank Lin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
